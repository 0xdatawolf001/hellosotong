Data decoding | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Generating utility modules Data decoding Direct RPC queries Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools Type-safe decoding Data decoding Data decoding squid-evm-typegen squid-substrate-typegen squid-ink-typegen Decoding events: import * as depositsAbi from './abi/deposits' processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let c of ctx . blocks ) { for ( let log of c . logs ) { if ( log . address === CONTRACT_ADDRESS && log . topics [ 0 ] == depositsAbi . events . Deposit . topic ) { // type-safe decoding of the Deposit event data const amt = depositsAbi . events . Deposit . decode ( log ) . wad } } } } ) Similar to events , transaction access is provided by the functions object for each contract method defined in the ABI. To decode events and calls, first determine the appropriate runtime version with .is() , then decode them with .decode() : import { events , calls } from './types' processor . run ( new TypeormDatabase ( ) , async ctx => { for ( let block of ctx . blocks ) { for ( let event of block . events ) { if ( event . name == events . balances . transfer . name ) { let rec : { from : Bytes , to : Bytes , amount : bigint } if ( events . balances . transfer . v1020 . is ( event ) ) { let [ from , to , amount , fee ] = events . balances . transfer . v1020 . decode ( event ) rec = { from , to , amount } } // ... decode all runtime versions similarly // with events.balances.transfer.${ver}.is/.decode } } for ( let call of block . calls ) { if ( call . name == calls . balances . forceTransfer . name ) { let rec : { source : Bytes , dest : Bytes , value : bigint } | undefined if ( calls . balances . forceTransfer . v1020 . is ( call ) ) { let res = calls . balances . forceTransfer . v1020 . decode ( call ) assert ( res . source . __kind === 'AccountId' ) assert ( res . dest . __kind === 'AccountId' ) rec = { source : res . source . value , dest : res . dest . value , value : res . value } } // ... decode all runtime versions similarly // with calls.balances.forceTransfer.${ver}.is/.decode } } } ) Here is an example of a utility module generated by the ink! typegen: src/abi/erc20.ts const _abi = new Abi ( metadata ) export function decodeEvent ( hex : string ) : Event { return _abi . decodeEvent ( hex ) } export function decodeMessage ( hex : string ) : Message { return _abi . decodeMessage ( hex ) } export function decodeConstructor ( hex : string ) : Constructor { return _abi . decodeConstructor ( hex ) } The usage in a batch handler is straightforward: processor . run ( new TypeormDatabase ( ) , async ctx => { for ( let block of ctx . blocks ) { for ( let event of block . events ) { if ( event . name === 'Contracts.ContractEmitted' && event . args . contract === CONTRACT_ADDRESS ) { let event = erc20 . decodeEvent ( event . args . data ) if ( event . __kind === 'Transfer' ) { // event is of type `Event_Transfer` } } } } } ) Edit this page Previous Generating utility modules Next Direct RPC queries
Instructions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Instructions On this page Instructions addInstruction(options) â€‹ This allows for tracking program instructions. options has the following structure: { // data requests where ? : { programId ? : string [ ] d1 ? : string [ ] d2 ? : string [ ] d3 ? : string [ ] d4 ? : string [ ] d8 ? : string [ ] a0 ? : string [ ] a1 ? : string [ ] a2 ? : string [ ] a3 ? : string [ ] a4 ? : string [ ] a5 ? : string [ ] a6 ? : string [ ] a7 ? : string [ ] a8 ? : string [ ] a9 ? : string [ ] isCommitted ? : boolean } // related data retrieval include ? : { transaction ? : boolean transactionTokenBalances ? : boolean logs ? : boolean innerInstructions ? : boolean } range ? : { from : number to ? : number } } The data requests here are: programId : the set of program addresses to track. Leave undefined to subscribe to instructions data of all programs from the whole network. d1 through d8 : sets of 1, 2, 3, 4 and 8-byte instruction discriminators, correspondingly. a0 through a9 : sets of base58-encoded account inputs to the instruction, at positions 0-9 correspondingly. isCommitted : true to request only instructions that did not revert. Related data retrieval flags: transaction = true : retrieve the transaction that gave rise to instruction transactionTokenBalances = true : retrieve all token balance records in the parent instruction logs = true : retrieve all logs emitted due to the instruction innerInstructions = true : retrieve the data for all instruction calls due to the matching instruction The related data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/solana-objects on the block data to populate the convenience reference fields like instruction.inner . Selection of the exact fields to be retrieved for each instruction item and the related data is done with the setFields() method documented on the Field selection page. Edit this page Previous General settings Next Transactions
JSON support | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions CSV support Parquet support JSON support S3 support bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks file-store extensions JSON support On this page JSON format support Table Implementation ​ The @subsquid/file-store-json package provides a Table implementation for writing to JSON and JSONL files. Use it by supplying one or more of its instances via the tables field of the Database constructor argument . The Table uses a constructor with the following signature: Table < S extends Record < string , any >> ( fileName : string , options ? : { lines ? : boolean } ) Here, S is a Typescript type describing the schema of the table data. fileName: string is the name of the output file in every dataset partition folder. options?: {lines?: boolean} are table options. At the moment the only available setting is whether to use JSONL instead of a plain JSON array (default: false). Example ​ This saves ERC20 Transfer events captured by the processor to a JSONL file where each line is a JSON serialization of a {from: string, to: string, value: number} object. Full squid code is available in this repo . import { Database } from '@subsquid/file-store' import { Table } from '@subsquid/file-store-json' ... const dbOptions = { tables : { TransfersTable : new Table < { from : string , to : string , value : bigint } > ( 'transfers.jsonl' , { lines : true } ) } , dest : new LocalDest ( './data' ) , chunkSizeMb : 10 } processor . run ( new Database ( dbOptions ) , async ( ctx ) => { ... let from : string = ... let to : string = ... let value : bigint = ... ctx . store . TransfersTable . write ( { from , to , value } ) ... } ) Edit this page Previous Parquet support Next S3 support Table Implementation Example
Logger | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Logger On this page Logger A Logger interface is injected into the handler context with ctx.log . It is bound to the namespace sqd:processor:mapping . The context logger is a recommended way of logging for squid processors. Logger exposes the following logging levels, in order of increasing severity: TRACE DEBUG INFO WARN ERROR FATAL By default, the logging level is set to INFO . And here is an example: processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { ctx . log . trace ( "Trace Log example" ) ; ctx . log . debug ( "Debug Log example" ) ; ctx . log . info ( "Info Log example" ) ; ctx . log . warn ( "Warn Log example" ) ; ctx . log . error ( "Error Log example" ) ; ctx . log . fatal ( "Fatal Log example" ) } ) ; Overriding the log level ​ The log level can be overridden by setting a matching namespace selector to one of the SQD_TRACE , ..., SQD_FATAL env variables. In particular, to set the handler logs level to DEBUG set the environment variable SQD_DEBUG to sqd:processor:mapping : SQD_DEBUG = sqd:processor:mapping The namespace selector supports wildcards, so one can also enable internal debug logs of @subsquid/substrate-processor with SQD_DEBUG=sqd:processor* since all processor context loggers inherit the processor-level namespace sqd:processor . Accessing logs of a deployed Squid ​ Processor logs can be inspected once the squid is deployed to Cloud: sqd logs -n < name > -s < slot > -f --level = < level > or sqd logs -n < name > -t < tag > -f --level = < level > For older version-based deployments... ...the slot string is v${version} , so use sqd logs -n < name > -s v < version > -f --level = < level > Check out the Slots and tags guide to learn more. The available levels are: info warning debug error - will fetch messages emitted by ctx.log.error ctx.log.fatal ctx.log.trace See CLI Reference or sqd logs --help for a full list of log options supported by SQD Cloud. Edit this page Previous bigquery-store Next Schema file Overriding the log level Accessing logs of a deployed Squid
Parquet support | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions CSV support Parquet support JSON support S3 support bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks file-store extensions Parquet support On this page Parquet format support info Support for the Parquet format is currently experimental. Contact us at the SquidDevs Telegram channel for support. Table Implementation ​ Apache Parquet is an advanced format for storing tabular data in files. It divides table columns into column chunks . Each column chunk is stored contiguously, allowing efficient partial reads of column subsets. Column chunks can also be compressed with row-specific compression algorithms, further enhancing the performance. Retrieval relies on metadata appended to the end of a Parquet file. Metadata standard of Apache Parquet is extremely powerful, enabling all sorts of extensions . Among other things, metadata contains the schema of the data, making the format self-describing. The @subsquid/file-store-parquet package provides a Table implementation for writing to Parquet files. Use it by supplying one or more of its instances via the tables field of the Database constructor argument . Constructor of the Table implementation accepts the following arguments: fileName: string : the name of the output file in every dataset partition folder. schema: {[column: string]: ColumnData} : a mapping from Parquet column names to ColumnData objects . A mapping of the same keys to data values is the row type used by the table writer . options?: TableOptions : see Table Options . Columns ​ ColumnData objects define storage options for each table column. They are made with the Column factory function that accepts a column data type and an optional options: ColumnOptions object. Column types can be obtained by making the function calls listed below from the Types submodule. They determine the Parquet type that will be used to store the data and the type that the table writer will expect to find at the corresponding field of data row objects. Column type Logical type Primitive type Valid data row object field contents Types.String() variable length string BYTE_ARRAY string of any length Types.Binary (length?) variable or fixed length byte array BYTE_ARRAY or FIXED_LEN_ BYTE_ARRAY Uint8Array of length equal to length if it is set or of any length otherwise Types.Int8() 8-bit signed integer INT32 number from -128 to 127 Types.Int16() 16-bit signed integer INT32 number from -32768 to 32767 Types.Int32() 32-bit signed integer INT32 number from -2147483648 to 2147483647 Types.Int64() 64-bit signed integer INT64 bigint or number from -9223372036854775808 to 9223372036854775807 Types.Uint8() 8-bit unsigned integer INT32 number from 0 to 255 Types.Uint16() 16-bit unsigned integer INT32 number from 0 to 65535 Types.Uint32() 32-bit unsigned integer INT32 number from 0 to 4294967295 Types.Uint64() 64-bit unsigned integer INT64 bigint or number from 0 to 18446744073709551615 Types.Float() 32-bit floating point number FLOAT non- Nan number Types.Double() 64-bit floating point number DOUBLE non- Nan number Types.Boolean() boolean value BOOLEAN boolean Types.Timestamp() UNIX timestamp in milliseconds INT64 Date Types.Decimal (precision, scale=0) decimal with precision digits and scale digits to the right of the decimal point INT32 or INT64 or FIXED_LEN_ BYTE_ARRAY number or bigint or BigDecimal Types.List (itemType, {nullable=false}) a list filled with optionally nullable items of itemType column type - Array of items satisfying itemType Types.JSON<T extends {[k: string]: any}>() JSON object of type T BYTE_ARRAY Object of type T Types.BSON<T extends {[k: string]: any}>() BSON object of type T BYTE_ARRAY Object of type T tip The widest decimals that PyArrow can read are Types.Decimal(76) . The following column options are available: ColumnOptions { nullable ? : boolean compression ? : Compression encoding ? : Encoding } See the Encoding and Compression section for details. Table Options ​ As its optional final argument, the constructor of Table accepts an object that defines table options: TableOptions { compression ? : Compression rowGroupSize ? : number pageSize ? : number } Here, compression determines the file-wide compression algorithm. Per-column settings override this. See Encoding and Compression for the list of available algorithms. Default: Compression.UNCOMPRESSED . rowGroupSize determines the approximate uncompressed size of the row group in bytes. Default: 32 * 1024 * 1024 . pageSize determines the approximate uncompressed page size in bytes. Default: 8 * 1024 . When pageSize is less than rowGroupSize times the number of columns, the latter setting will be ignored. In this case each row group will contain exactly one roughly pageSize d page for each column. Encoding and Compression ​ Encodings are set at a per-column basis. At the moment the default and the only supported value is 'PLAIN' . Compression can be set at a per-file or a per-column basis. Available values are 'UNCOMPRESSED' (default) 'GZIP' 'LZO' 'BROTLI' 'LZ4' Example ​ This saves ERC20 Transfer events captured by the processor to a Parquet file. All columns except for from are GZIP ped. Row groups are set to be roughly 30000 bytes in size each. Each row group contains roughly ten ~1000 bytes-long pages per column. Full squid code is available in this repo . import { Database , LocalDest } from '@subsquid/file-store' import { Column , Table , Types } from '@subsquid/file-store-parquet' ... const dbOptions = { tables : { TransfersTable : new Table ( 'transfers.parquet' , { from : Column ( Types . String ( ) , { compression : 'UNCOMPRESSED' } ) , to : Column ( Types . String ( ) ) , value : Column ( Types . Uint64 ( ) ) } , { compression : 'GZIP' , rowGroupSize : 300000 , pageSize : 1000 } ) } , dest : new LocalDest ( './data' ) , chunkSizeMb : 10 } processor . run ( new Database ( dbOptions ) , async ( ctx ) => { ... let from : string = ... let to : string = ... let value : bigint = ... ctx . store . TransfersTable . write ( { from , to , value } ) ... } ) Edit this page Previous CSV support Next JSON support Table Implementation Columns Table Options Encoding and Compression Example
Rules | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Rules Rules SQD Testnet App Rules & Motivation Introduction: Welcome to the initial phase of the SQD testnet. As we embark on this journey, it's essential to understand that this is a testing phase. There might be occasional downtimes or outages. These challenges are crucial for us, as they will guide our efforts to strengthen the decentralized network, ensuring its robustness and resilience. Our Motivation: Reliability Assessment: We aim to evaluate the reliability and resilience of the SQD network, especially under significant load. Cybersecurity Evaluation: It's crucial to test the SQD network's resilience against potential cybersecurity threats. Community Engagement: We want to recognize our most active and loyal testnet participants. Developer Collaboration: This testnet will help us identify highly engaged developers, opening doors for potential future collaborations. Participation Rules: To be a part of this testnet, participants need to link their Metamask wallet. This step is crucial as the goal is to accumulate the highest amount of tSQD. Please note, once a wallet is linked, it's final; switching to another wallet won't be possible. As participants complete quests, tSQD will be credited to their accounts. Types of Quests: Social Quests: These quests revolve around social media activities. Examples include linking accounts from platforms like Twitter, Discord, and GitHub. Stay alert! New quests will be introduced throughout the test period. Regularly visit the quest page to stay updated. Tech Quests: Tech quests involve deploying squids on local machines and generating a specific number of queries directed at the SQD testnet. Follow the provided instructions and monitor your query progress directly on the quest page. Special Quests: These quests are for those with a deep understanding of blockchain indexing. To attempt these quests, select the corresponding cards on the quest page and follow the instructions. Completing special quests will grant participants a higher amount of tSQD. Join us in this exciting phase, contribute to the network's growth, and earn rewards for your efforts! Edit this page
hasura-configuration tool v2 | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Migrate from The Graph ArrowSquid for EVM ArrowSquid for Substrate hasura-configuration tool v2 Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Migration guides hasura-configuration tool v2 Migrating to Hasura configuration tool v2 Pre-2.0.0 @subsquid/hasura-configuration used a fixed naming schema for relation fields : Forward relation fields were always named after the type they referred to. Type names were snake_case d. For example, any field referring to an entity called BurnTest was called burn_test . Inverse relation fields names were also determined by the relation type: In one-to-one relation their names were to_snake_case(typeName) , where typeName is the name of the entity that the inverse field is typed with. In one-to-many inverse relations they were named ${to_snake_case(typeName)}s . Any names given to the fields in the schema were ignored. @subsquid/ [email protected] introduces full support for in-schema field names. Now the fields will be called exactly as they are in the schema file. If these field names are different from what's described above, your API will have a breaking change. If you'd like to avoid it, please make sure that the fields in your schema.graphql are named exactly as described above. To update the tool and your Hasura config: npm i @subsquid/hasura-configuration@latest npx squid-hasura-configuration regenerate If you deployed to the Cloud you can safely (barring the field names change) redeploy your squid in-place. If you opted to not change your schema to accommodate the old relation field names, please follow this up by revising GraphQL queries in any of your client apps. Edit this page Previous ArrowSquid for Substrate Next Tutorials
Core queries exposed by the OpenReader API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Core queries exposed by the OpenReader API 📄️ Intro GraphQL and its support in SQD 📄️ Entity queries Basic data retrieval 📄️ AND/OR filters Basic logic operators for use in filters 📄️ Nested field queries Query entities related to other entities 📄️ Cross-relation queries Filtering by fields of nested entities 📄️ JSON queries Query entities with Object-typed fields 📄️ Pagination Dealing with large query outputs 📄️ Sorting The orderBy argument 📄️ Union type resolution Use the __typename field to resolve types Previous Access control Next Intro
Best practices | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Best practices Best practices Here is a list of items to check out before you deploy your squid for use in production: Make sure that you use batch processing throughout your code. Consider using @belopash/typeorm-store for large projects with extensive entity relations and frequent database reads . Filter your data in the batch handler. E.g. if you request event logs from a particular contract, do check that the address field of the returned data items matches the contract address before processing the data. This will make sure that any future changes in your processor configuration will not cause the newly added data to be routed to your old processing code by mistake. info Batch handler data filtering used to be compulsory before the release of @subsquid/ [email protected] . Now it is optional but highly recommended. If your squid saves its data to a database , make sure your schema has @index decorators for all entities that will be looked up frequently. Follow the queries optimization procedure for best results. If your squid serves a GraphQL API Do not use OpenReader if your application uses subscriptions. Instead, use PostGraphile or Hasura . If you do use OpenReader: configure the built-in DoS protection against heavy queries; configure caching . If you use PostGraphile or Hasura, follow their docs to harden your service in a similar way. If you deploy your squid to SQD Cloud: Deploy your squid to a Professional organization . Do not use dedicated: false in the scale: section of the manifest. Make sure that your scale: section requests a sufficient but not excessive amount of resources. Set your deployment up for zero downtime updates . Use a tag-based URL and and not slot-based URLs to access your API e.g. from your frontend app. Make sure to use secrets for all sensitive data you might've used in your code. The most common type of such data is API keys and URLs containing them. Follow the recommendations from the Cloud logging page . Edit this page Previous Resources Next Environment variables
SQD vs The Graph | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK SQD vs The Graph On this page SQD vs The Graph The Graph is an indexing protocol and framework for EVM chains. In The Graph, indexing projects are called subgraphs. A Graph indexing node is a black-box that executes subgraphs compiled into WASM. The data is sourced directly from an archival blockchain node, and the processed data is stored in a built-in Postgres database. On the contrary, SQD employs a radically open modular architecture with: a separated layer for efficient data extraction and batch queries ( SQD Network ) a client-side libraries for data transformation and presentation. The data transformation and presentation is enabled by a growing list of tools and SDKs that consume the raw data from SQD Network: Typescript based Squid SDK offers pluggable data targets for both online and offline use-cases Python-based DipDup SDK Subgraphs (via SQD Firehose adapter ) ApeWorx (via ape-subsquid plugin ) Feature matrix ​ SQD Network + Squid SDK The Graph Programming language Typescript AssemblyScript (compiled to WASM) Indexing speed ~1k-50k bps ~100-150 bps ABI-based generator Yes Yes Real-time indexing (unfinalized blocks) Yes No Off-chain data Yes No Data targets Customizable Postgres-only Customizable DB migrations Yes No Factory contract indexing Yes, via wildcards Yes Multi-contract indexing Yes Limited Analytic data targets BigQuery, Parquet, CSV No Local setup Easy Requires an archival node GraphQL API Generated from schema.graphql Generated from schema.graphql Custom resolvers and mutations Yes No Subscriptions Yes Via middleware Hosted service Yes Yes (to be sunset) Secret env variables Yes No Payment Fiat, subscription-based GRT, pay-per-query Decentralization Decentralized data sourcing via SQD Network , with opt-in decentralized data targets (Kwil DB, Ceramic) and processing (via Lava, in development) The Graph network Architecture difference ​ By design, The Graph indexing node is a black-box that executes subgraphs compiled into WASM. The data is sourced directly from the archival node and local IPFS, and the processed data is stored in a built-in Postgres database. The data stored in the database is considered to be "sealed", so that no external process can modify or read the data except through the GraphQL interface. Edit this page Previous FAQ Next SQD Cloud Feature matrix Architecture difference
Whitepaper | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Whitepaper On this page Whitepaper warning The SQD whitepaper and any other documents attached or linked to this statement are intended only to provide a broad overview of the general direction of SQD and the SQD Network. The whitepaper and these documents are intended for information purposes only and may not be incorporated into any contract. These materials are not and should not be considered to offer a commitment to deliver any material, code, functionality, token, and should not be relied upon in making token purchase decisions or any kind of investment. All details related to the development, timing, or issue of any SQD feature or token remain at the sole discretion of Subsquid Labs GmbH. Intro ​ Blockchain, while initially designed solely for peer-to-peer transfers (Bitcoin), has since evolved into a general
purpose global execution runtime. The introduction of smart contract blockchains (Ethereum being the first) has enabled the creation of decentralized Apps (dApps) that are now catering to users in countless verticals, including: Decentralized Finance (DeFi) and tokenized real-world assets (RWA) Decentralized Social application (DeSoc) Decentralized Gaming with peer-to-peer game economies (GameFi) Decentralized physical infrastructure (DePIN) Decentralized creator economy (Music, Videos, NFTs) One of the key fundamentals that has allowed countless dApps to thrive is the emergent infrastructure addressing the challenges of scaling, storage, and retrieval of data. We also refer to this as the "Web3 tech stack". The below all form part of the stack: Scalable execution (L2 chains, Rollups, Bridges, Solana) Bringing off-chain and real-world data on-chain (Oracles like Chainlink) Permanent and long-term storage for large data (Arweave, Filecoin) Application-specific databases and APIs (TheGraph, POKT, Kwil, WeaveDB) However, there is still one part missing. One of the biggest challenges dApp developers face is accessing data at scale. It's currently highly complex to query and aggregate
the exponentially growing amount of data produced by blockchains (transactions and state data), applications
(decoded smart contract states), and any relevant off-chain data (prices, data kept in IPFS and Arweave, pruned EIP4844 data). In Web2, data produced by applications (such as logs or client data) is
stored in centralized data lakes like BigQuery, Snowflake, Apache, and Iceberg to facilitate access. Storing Web3 data in similar centralized data lakes would defeat the purpose of open, resilient access. Still, Web3 app data, if aggregated, filtered, and easily extractable, will serve as a catalyst to unlock the potential of the industry in a multi-chain paradigm. Instead of replicating Web2 approaches the permissionless and decentralized nature of Web3 requires a fundamentally different solution with the following properties: Infinite horizontal scalability: the capacity of the lake should grow indefinitely as new nodes join Permissionless data access: the data can be uploaded and queried without any gatekeeping or centralized control Credible neutrality: no barrier for new nodes to join the network Trust-minimized queries: the data can be audited, and all clients can verify the query result Low maintenance cost: the price per query should be negligible The SQD Network is built to satisfy all the above properties, thanks to its architecture where: The raw data is uploaded into permanent storage by the data providers, which act as oracles but for "big data" The data is compressed and distributed among the network nodes Node operators have to bond a security deposit, which can be slashed for byzantine behavior Each node efficiently queries the local data with DuckDB Any query can be verified by submitting a signed response to an on-chain smart contract With the recent advances in in-process databases like DuckDB and an ever-decreasing price of quality SSD storage, the operational cost of a single node handling 1Tb of data is as low as 35/mo [1] and will likely decline further in the future. In the first stage, the SQD Network only supports simple queries that can be resolved by querying the local data of a suitable node. This already provides a fundamentally new primitive for on-chain data availability, fully eliminating the need to maintain expensive archival blockchain nodes to access historical blockchain data. After the bootstrapping period, the network will be expanded with an additional query planning and execution layer supporting general-purpose SQL queries inspired by Apache Calcite and DataFusion. Design Overview ​ The below actors all participate in the SQD Network: Data Providers Workers Scheduler Logs Collector Rewards manager Data Consumers Data Providers ​ Data Providers produce data to be served by SQD Network. Currently, the focus is on on-chain data, and data providers are blockchains and L2s. At the moment, SQD only supports EVM- and Substrate chains, but we plan on adding support for Cosmos and Solana in the near future. Data providers are responsible for ensuring the quality and timely provision of data. During the bootstrapping phase, Subsquid Labs GmbH acts as the sole data provider for the SQD Network, serving as a proxy for chains from which the data is ingested block-by-block. The ingested data is validated by comparing hashes. It's then split into small compressed chunks and saved into persistent storage, from which the chunks are randomly distributed between the workers. The metadata is saved on-chain and is updated by the data provider each time a new piece of the data is uploaded. The metadata describes the schema, the data layout, the reserved storage, and the desired replication factor to be accommodated by the network. You can find the metadata structure in the Appendix. The data provider is incentivized to provide consistent and valid data, and it is their responsibility to make the data available in persistent storage. During the bootstrap phase, the persistent storage used by Subsquid Labs is an S3-compatible service with backups pinned to IPFS. As the network matures, more data providers and storage options will be added, with data providers being vetted by on-chain governance and known trusted parties. Data providers pay on-chain subscription fees to the network to make data available and to have it served by workers. These fees are sent to the SQD Network treasury. Scheduler ​ The scheduler is responsible for distributing the data chunks submitted by the data providers among the workers. The scheduler listens to updates of the data sets, as well as updates of the worker sets, and sends requests to the workers to download new chunks and/or redistribute the existing data chunks based on the capacity and the target redundancy for each dataset. Once a worker receives an update request , it downloads the missing data chunks from the corresponding persistent storage. Workers ​ Workers contribute the storage and compute resources to the network. They serve the data in a peer-to-peer manner for consumption and receive $SQD tokens as compensation. Each worker has to be registered on-chain by bonding 100_000 SQD tokens, which can be slashed if the worker provably violates the protocol rule. $SQD holders can also delegate to a specific worker to signal the reliability of the worker and earn a portion of the rewards. The rewards are distributed each epoch and depend on: The previous number of epochs the worker stayed online The amount of data served to clients The number of delegated tokens Fairness Liveness during the epoch Workers submit ping information along with their signed logs of the executed query requests to the Validator, thus committing to the results returned to the clients. Logs collector ​ The Logs Collector's sole responsibility is to collect the liveness pings and the query execution logs from the workers, batch them, and save them into public persistent storage. The logs are signed by the workers' P2P identities and pinned to IPFS. The data is stored for at least 6 months and used by other network participants. Reward Manager ​ The Reward Manager is responsible for calculating and submitting worker rewards on-chain for each epoch. The rewards depend on: Worker liveness during the epoch Delegated tokens Served queries (in bytes; both scanned and returned sizes are accounted for) Liveness since the registration The Reward Manager accesses the logs, calculates the rewards, and submits a claimable commitment on-chain for each epoch. Each worker then claims their reward individually. The rewards may expire after an extended period of time Data Consumers ​ To query the network, data consumers have to operate a gateway or use an externally provided service (public or private). Each gateway is bound to an on-chain address. The number of requests a gateway can submit to the network is capped by a number calculated based on the amount of locked $SQD tokens. Consequently, the more tokens are locked by the gateway operator, the more bandwidth it is allowed to consume. One can think of this mechanism as if the locked SQD yields virtual "compute units" (CU) based on the period the tokens are locked. All queries cost the same price of 1 CU (until complex SQL queries are implemented). The query cap is calculated by: Calculating the virtual yield in SQD on the locked tokens (in SQD) Multiplying by the current CU price (in CU/SQD) Boosters ​ The locking mechanism has an additional booster design to incentivize gateway operators to lock their tokens for longer periods of time for an increase in CU. The longer the lock period, the more CUs are allocated per SQD/yr. The APY is calculated as BASE_VIRTUAL_APY * BOOSTER . At the launch of the network, the parameters are set to be BASE_VIRTUAL_APY = 12% and 1SQD = 4000 CU . For example, if a gateway operator locks 100 SQD for 2 months, the virtual yield is 2SQD, which means it can perform 8000 queries (8000 CU).
If 100 SQD are locked for 3 years, the virtual yield is 12% * 3 APY, so the operator gets CUs worth 108 of SQD, that is it can submit up to 432000 queries to the network within the period of 3 years. Query Validation ​ The SQD Network provides economic guarantees for the validity of the queried data, with the added possibility of validating specific queries on-chain. All query responses are signed by the worker who  executed the query, acting as a commitment to the query response. Anyone can submit such a response on-chain, and if it is deemed incorrect, the worker bond is slashed. The smart contract validation logic may be dataset-specific depending on the nature of the data being queried, with the following options: Proof by Authority: a white-listed set of on-chain identities decides on the validity of the response. Optimistic on-chain: after the validation request is submitted, anyone can submit a claim proving the query response is incorrect. For example, assuming the original query was "Return transactions matching the filter X in the block range [Y, Z] " and the response is some set of transactions T. During the validation window, anyone can submit a Merkle proof for some transaction t matching the filter X yet not in T. If no such proofs are submitted during the decision window, the response is considered valid. Zero-Knowledge: a zero-knowledge proof that the response exactly matches the requests. The zero-knowledge proof is generated off-chain by a prover and is validated on-chain by the smart contract. Since submitting each query for on-chain validation on-chain is costly and not feasible, clients opt-in to submit query responses in an off-chain queue, together with the metadata such as response latency. Then, independent searchers scan the queue and submit suspicious queries for on-chain validation. If the query is deemed invalid, the submitter gets a portion of the slashed bond as a reward, thus incentivizing searchers to efficiently scan the queue for malicious responses. SQD Token ​ SQD is the ERC-20 protocol token that is native to the SQD Network ecosystem. The token smart contract is to be deployed on the Ethereum mainnet and bridged to Arbitrum One. This strategy seeks to ensure the blockchain serves as a reliable, censorship-resistant, and verifiably impartial ledger, facilitating reward settlements and managing access to network resources. The SQD token is a critical component of the SQD ecosystem. Use cases for the SQD token are focused on streamlining and securing network operations in a permissionless manner: Alignment of incentives for infrastructure providers: SQD is used to reward node operators that contribute computation and storage resources to the network. Curation of network participants: Via delegation, the SQD token design includes built-in curation of nodes, facilitating permissionless selection of trustworthy operators for rewards. Fair resource consumption: By locking SQD tokens, consumers of data from the decentralized data lake may increase rate limits. Network decision making: SQD tokenholders can participate in governance, and are enabled to vote on protocol changes and other proposals. The SQD token’s innovative curation component allows the SQD community to delegate SQD to Node Operators of their choice, ensuring trustlessness. SQD’s utility as a tool for adjusting rate limits is unique in increasing trustless performance, by locking SQD tokens, without having to pay a centralized provider for quicker or more efficient data access. Appendix I -- Metadata ​ The metadata has the following structure: interface DataSet { dataset_id : string // how many times the dataset should be replicated replication_factor : int // a unique URL for permanent storage. Can be s3:// bucket permanent_storage : string // how much space should for a single replica. // the total space required by a dataset is replication_factor * reserved_space reserved_space : int // dataset-specific additional metadata extra_meta ? : string // dataset-specific state description state ? : string // reserved for future use, must be false premium : boolean } The on-chain state of the dataset should be updated by the data provider periodically, and the total space must not exceed reserved_space . The dataset can be in the following states: enum DatasetState { // submitted by the data provider SUBMITTED , // the data being distributed by the network IN_PREPARATION , // served by the network ACTIVE , // if the subscription fee // has not been provided for the duration of an epoch COOLDOWN , // if the dataset is no longer served by the network // and will be deleted DISABLED } The Scheduler changes the state to IN_PREPARATION and ACTIVE from SUBMITTED . The COOLDOWN and DISABLED states are activated automatically if subscription payments aren't made. At the initial stage of the network, the switch to disabling datasets is granted to Subsquid Labs GmbH, which is going to be replaced by auto payments at a later stage. Appendix II -- Rewards ​ The network rewards are paid out to workers and delegators for each epoch. The Reward Manager submits an on-chain claim commitment, from which each participant can claim. The rewards are allocated from the rewards pool. Each epoch, the rewards pool unlocks APY_D * S * EPOCH_LENGTH in rewards, where EPOCH_LENGTH is the length of the epoch in days, S is the total (bonded + delegated) amount of staked SQD during the epoch and APY_D is the (variable) base reward rate calculated for the epoch. Rewards pool ​ The SQD supply is fixed for the initial pool, and the rewards are distributed from a pool, to which 10% of the supply is allocated at TGE. The reward pool has a protective mechanism which caps the amount of rewards distributed per epoch, and it is subject to change via governance. During the initial 3-year bootstrapping phase, the reward cap and the total supply of SQD is fixed. Afterwards, the reward cap drops significantly until the governance motion concludes on the
inflation schedule and new tokens are minted to replenish the reward pool. Unlike most projects who fix the inflation schedule before the launch, postponing this decision leaves a lot more flexibility and allows the community to analyze the historical 3-year data to make
and informed decision on the future issuace of SQD . Reward rate ​ The reward rate depends on two factors: utilization of the network and staked supply. The network utilization rate is defined as u_rate = (target_capacity - actual_capacity)/target_capacity The target capacity is calculated as target_capacity = sum([d.reserved_space * d.replication_factor]) where the sum is over the non-disabled datasets d . The actual capacity is calculated as actual_capacity = num_of_active_workers() * WORKER_CAPCITY * CHURN The WORKER_CAPACITY is a fixed storage per worker, set to 1Tb . CHURN is a discounting factor to account for the churn, set to 0.9 . The target APR (365-day-based rate) is then calculated as: rAPR = MIN(base_apr(u_rate), APR_CAP(total_staked)) The base_apr is projected to be around 20% in the equilibrium state, when the actual worker capacity matches the desired network capacity, set externally. It is increased up to 70% to incentivize more workers to join the network until the target capcity is reached: The APR_CAP cut-off is added to cap the total rewards per epoch.
One defines first total_staked = bonded + delegated to be the total amount of reward-earning SQD locked by workers ( bonded ) and delegators ( delegated ) respescitvely. We then define APR_CAP(total_staked) = 0.3 * INITIAL_POOL_SIZE / total_staked It is set so that after the 3 year boostrapping period at most 90% of the initial reward pool is spent. Worker reward rate ​ For each epoch, rAPR is calculated, and the total of R = rAPR/365 * total_staked * EPOCH_LENGTH is unlocked from the rewards pool. The rewards are then distributed between the workers and delegators, and the leftovers are split between the burn and the treasury. For a single worker and stakers for this token, the maximal reward is rAPR/365 * (bond + staked) * EPOCH_LENGTH . It is split into the worker liveness reward and the worker traffic reward. Let S[i] be the stake for i -th worker and T[i] be the traffic units (defined below) processed by the worker. We define the relative weights as s[i] = S[i]/sum(S[i]) t_scanned[i] = T_scanned[i]/sum(T_scanned[i]) t_e[i] = T_e[i]/sum(T_e[i]) t[i] = sqrt(t_scanned[i] * t_e[i]) In other words, s[i] and t[i] correspond to the contribution of the i -th worker to the total stake and to total traffic, respectively. The traffic weight t[i] is a geometric average of the normalized scanned ( t_scanned[i] )  and the egress ( t_e[i] ) traffic processed by the worker. It is calculated by aggregating the logs of the queries processed by the worker during the epoch, and for each processed query, the worker reports the response size (egress) and the number of scanned data chunks. The max potential yield for the epoch is given by rAPR described above: r_max = rAPR/365 * EPOCH_LENGTH The actuall yield r[i] for the i -th worker is discounted: r[i] = r_max * D_liveness * D_traffic(t_i, s_i) * D_tenure D_traffic is a Cobb-Douglas type discount factor defined as D_traffic(t_i, s_i) = min( (t_i/s_i)^alpha, 1 ) with the elasticity parameter alpha set to 0.1 . It has the following properties: Always in the interval [0, 1] Goes to zero as t_i goes to zero Neutral (i.e., close to 1) when s_i ~ t_i , that is, the stake contribution is fair (proportional to the traffic contribution) Excess traffic contributes only sub-linearly to the reward D_liveness is a liveness factor calculated as the percentage of the time the worker is self-reported as online. A worker sends a ping message every 10 seconds, and if there are no pings within a minute, the worker is deemed offline for this period of time. The liveness factor is the percentage of the time (with minute-based granularity) the network is live. We suggest a piecewise linear function with the following properties: It is 0 below a reasonably high threshold (set to 0.8 ) Sharply increases to near 1 in the "intermediary" regime 0.8-0.9 The penalty around 1 is diminishing Finally, D_tenure is a long-range liveness factor incentivizing consistent liveness across the epochs. The rationale is that The probability of a worker failure decreases with the time the worker is live thus freshly spawned workers are rewarded less The discount for freshly spawned workers discourages the churn among workers and incentivizes longer-term commitments Distribution between the worker and delegators ​ The total claimable reward for the i -th worker and the stakers is calculated simply as r[i] * s[i] . Clearly, s[i] is the sum of the (fixed) bond b[i] and the (variable) delegated stake d[i] . Thus, the delegator rewards account for r[i] * d[i] .  This extra reward part is split between the worker and the delegators: The worker gets: r[i] * b[i] + 0.5 * r[i] * s[i] The delegators get 0.5 * r[i] * s[i] , effectively attining 0.5 * r[i] as the effectual reward rate. The rationale for this split is: Make the worker accountable for r[i] Incentivize the worker to attract stakers (the additional reward part) Incentivize the stakers to stake for a worker with high liveness (and, in general, high r[i] ) At an equilibrium, the stakers will get a 10% annual yield, while workers get anything between 20-30% depending on the staked funds. Note that the maximal stake is limited by the bond size. References ​ [1] SuperDAO Growth Trends report [2] Based on the estimate that read-only RPC queries constitute roughly 90% of the RPC provider traffic Edit this page Previous Overview Next FAQ Intro Design Overview Data Providers Scheduler Workers Logs collector Reward Manager Data Consumers Boosters Query Validation SQD Token Appendix I -- Metadata Appendix II -- Rewards Rewards pool Reward rate Worker reward rate Distribution between the worker and delegators References
Development flow | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Environment set up Indexer from scratch Development flow Project structure sqd CLI cheatsheet Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Getting started Development flow On this page Development flow This page is a definitive end-to-end guide into practical squid development. It uses templates to simplify the process. Check out Squid from scratch for a more educational barebones approach. info Feel free to also use the template-specific sqd scripts defined in commands.json to simplify your workflow. See sqd CLI cheatsheet for a short intro. Prepare the environment ​ Node v16.x or newer Git Squid CLI Docker (if your squid will store its data to PostgreSQL) See also the Environment set up page. Understand your technical requirements ​ Consider your business requirements and find out How the data should be delivered. Options: PostgreSQL with an optional GraphQL API - can be real-time file-based dataset - local or on S3 Google BigQuery What data should be delivered What are the technologies powering the blockchain(s) in question. Supported options: Ethereum Virtual Machine (EVM) chains like Ethereum - supported networks Substrate -powered chains like Polkadot and Kusama - supported networks Note that you can use SQD via RPC ingestion even if your network is not listed. What exact data should be retrieved from blockchain(s) Whether you need to mix in any off-chain data Example requirements ​ DEX analytics on Polygon Suppose you want to train a prototype ML model on all trades done on Uniswap Polygon since the v3 upgrade. A delay of a few hours typically won't matter for training, so you may want to deliver the data as files for easier handling. The output could be a simple list of swaps, listing pair, direction and token amounts for each. Polygon is an EVM chain. All the required data is contained within Swap events emitted by the pair pool contracts. Uniswap deploys these dynamically , so you will also have to capture PoolCreated events from the factory contract to know which Swap events are coming from Uniswap and map them to pairs. No off-chain data will be necessary for this task. NFT ownership on Ethereum Suppose you want to make a website that shows the image and ownership history for ERC721 NFTs from a certain Ethereum contract. For this application it makes sense to deliver a GraphQL API. Output data might have Token , Owner and Transfer database tables / entities , with e.g. Token supplying all the fields necessary to show ownership history and the image. Ethereum is an EVM chain. Data on token mints and ownership history can be derived from Transfer(address,address,uint256) EVM event logs emitted by the contract. To render images, you will also need token metadata URLs that are only available by querying the contract state with the tokenURI(uint256) function. You'll need to retrieve the off-chain token metadata (usually from IPFS). Kusama transfers BigQuery dataset Suppose you want to create a BigQuery dataset with Kusama native tokens transfers. The delivery format is BigQuery. A single table with from , to and amount columns may suffice. Kusama is a Substrate chain. The required data is available from Transfer events emitted by the Balances pallet. Take a look at our Substrate data sourcing miniguide for more info on how to figure out which pallets, events and calls are necessary for your task. No off-chain data will be necessary for this task. Start from a template ​ Although it is possible to compose a squid from individual packages , in practice it is usually easier to start from a template. EVM Substrate Templates for the PostgreSQL+GraphQL data destination A minimal template intended for developing EVM squids. Indexes ETH burns. sqd init my-squid-name -t evm A starter squid for indexing ERC20 transfers. sqd init my-squid-name -t https://github.com/subsquid-labs/squid-erc20-template Classic example Subgraph after a migration to SQD. sqd init my-squid-name -t gravatar A template showing how to combine data from multiple chains . Indexes USDC transfers on Ethereum and Binance. sqd init my-squid-name -t multichain Templates for storing data in files USDC transfers -> local CSV sqd init my-squid-name -t https://github.com/subsquid-labs/file-store-csv-example USDC transfers -> local Parquet sqd init my-squid-name -t https://github.com/subsquid-labs/file-store-parquet-example USDC transfers -> CSV on S3 sqd init my-squid-name -t https://github.com/subsquid-labs/file-store-s3-example Templates for the Google BigQuery data destination USDC transfers -> BigQuery dataset sqd init my-squid-name -t https://github.com/subsquid-labs/squid-bigquery-example Templates for the PostgreSQL+GraphQL data destination Native events emitted by Substrate-based chains sqd init my-squid-name -t substrate ink! smart contracts sqd init my-squid-name -t ink Frontier EVM contracts on Astar and Moonbeam sqd init my-squid-name -t frontier-evm After retrieving the template of choice install its dependencies: cd my-squid-name npm i Test the template locally. The procedure varies depending on the data sink: PostgreSQL+GraphQL filesystem dataset BigQuery Launch a PostgreSQL container with docker compose up -d Build the squid with npm run build Apply the DB migrations with npx squid-typeorm-migration apply Start the squid processor with node -r dotenv/config lib/main.js You should see output that contains lines like these ones: 04:11:24 INFO  sqd:processor processing blocks from 6000000 04:11:24 INFO  sqd:processor using archive data source 04:11:24 INFO  sqd:processor prometheus metrics are served at port 45829 04:11:27 INFO  sqd:processor 6051219 / 18079056 , rate: 16781 blocks/sec, mapping: 770 blocks/sec, 544 items/sec, eta: 12m Start the GraphQL server by running npx squid-graphql-server in a separate terminal, then visit the GraphiQL console to verify that the GraphQL API is up. When done, shut down and erase your database with docker compose down . (for the S3 template only) Set the credentials and prepare a bucket for your data as described in the template README . Build the squid with npm run build Start the squid processor with node -r dotenv/config lib/main.js The output should contain lines like these ones: 04:11:24 INFO  sqd:processor processing blocks from 6000000 04:11:24 INFO  sqd:processor using archive data source 04:11:24 INFO  sqd:processor prometheus metrics are served at port 45829 04:11:27 INFO  sqd:processor 6051219 / 18079056 , rate: 16781 blocks/sec, mapping: 770 blocks/sec, 544 items/sec, eta: 12m You should see a ./data folder populated with indexer data appear in a bit. A local folder looks like this: $ tree ./data/ ./data/ ├── 0000000000-0007242369 │   └── transfers.tsv ├── 0007242370-0007638609 │   └── transfers.tsv .. . └── status.txt Create a dataset with your BigQuery account, then follow the template README . The bottom-up development cycle ​ The advantage of this approach is that the code remains buildable at all times, making it easier to catch issues early. I. Regenerate the task-specific utilities ​ EVM Substrate Retrieve JSON ABIs for all contracts of interest (e.g. from Etherscan), taking care to get ABIs for implementation contracts and not proxies where appropriate. Assuming that you saved the ABI files to ./abi , you can then regenerate the utilities with npx squid-evm-typegen ./src/abi ./abi/*.json --multicall Or if you would like the tool to retrieve the ABI from Etherscan in your stead, you can run e.g. npx squid-evm-typegen \ src/abi \ 0xdAC17F958D2ee523a2206206994597C13D831ec7 #usdt The utility classes will become available at src/abi . See also EVM typegen code generation . Follow the respective reference configuration pages of each typegen tool: Substrate typegen configuration ink! typegen configuration A tip for frontier-evm squids These squids use both Substrate typegen and EVM typegen. To generate all the required utilities, configure the Substrate part , then save all relevant JSON ABIs to ./abi , then run npx squid-evm-typegen ./src/abi ./abi/*.json --multicall followed by npx squid-substrate-typegen ./typegen.json II. Configure the data requests ​ Data requests are customarily defined at src/processor.ts . The details depend on the network type: EVM Substrate Edit the definition of const processor to Use a data source appropriate for your chain and task. It is possible to use RPC as the only data source, but adding a SQD Network data source will make your squid sync much faster. RPC is a hard requirement if you're building a real-time API. If you're using RPC as one of your data sources, make sure to set the number of finality confirmations so that hot blocks ingestion works properly. Request all event logs , transactions , execution traces and state diffs that your task requires, with any necessary related data (e.g. parent transactions for event logs). Select all data fields necessary for your task (e.g. gasUsed for transactions). See reference documentation for more info and processor configuration showcase for a representative set of examples. Edit the definition of const processor to Use a data source appropriate for your chain and task Use a SQD Network gateway whenever it is available. RPC is still required in this case. For networks without a gateway use just the RPC. Request all events and calls that your task requires, with any necessary related data (e.g. parent extrinsics). If your squid indexes any of the following: an ink! contract an EVM contract running on the Frontier EVM pallet Gear messages then you can use some of the specialized data requesting methods to retrieve data more selectively. Select all data fields necessary for your task (e.g. fee for extrinsics). See reference documentation for more info. Processor config examples can be found in the tutorials: general Substrate ink! Frontier EVM III. Decode and normalize the data ​ Next, change the batch handler to decode and normalize your data. In templates, the batch handler is defined at the processor.run() call in src/main.ts as an inline function. Its sole argument ctx contains: at ctx.blocks : all the requested data for a batch of blocks at ctx.store : the means to save the processed data at ctx.log : a Logger at ctx.isHead : a boolean indicating whether the batch is at the current chain head at ctx._chain : the means to access RPC for state calls This structure ( reference ) is common for all processors; the structure of ctx.blocks items varies. EVM Substrate Each item in ctx.blocks contains the data for the requested logs, transactions, traces and state diffs for a particular block, plus some info on the block itself. See EVM batch context reference . Use the .decode methods from the contract ABI utilities to decode events and transactions, e.g. import * as erc20abi from './abi/erc20' processor . run ( db , async ctx => { for ( let block of ctx . blocks ) { for ( let log of block . logs ) { if ( log . topics [ 0 ] === erc20abi . events . Transfer . topic ) { let { from , to , value } = erc20 . events . Transfer . decode ( log ) } } } } ) See also the EVM data decoding . Each item in ctx.blocks contains the data for the requested events, calls and, if requested, any related extrinsics; it also has some info on the block itself. See Substrate batch context reference . Use the .is() and .decode() functions to decode the data for each runtime version, e.g. like this: import { events } from './types' processor . run ( db , async ctx => { for ( let block of ctx . blocks ) { for ( let event of block . events ) { if ( event . name == events . balances . transfer . name ) { let rec : { from : string ; to : string ; amount : bigint } if ( events . balances . transfer . v1020 . is ( event ) ) { let [ from , to , amount ] = events . balances . transfer . v1020 . decode ( event ) rec = { from , to , amount } } else if ( events . balances . transfer . v1050 . is ( event ) ) { let [ from , to , amount ] = events . balances . transfer . v1050 . decode ( event ) rec = { from , to , amount } } else if ( events . balances . transfer . v9130 . is ( event ) ) { rec = events . balances . transfer . v9130 . decode ( event ) } else { throw new Error ( 'Unsupported spec' ) } } } } } ) See also the Substrate data decoding . You can also decode the data of certain pallet-specific events and transactions with specialized tools: use the utility classes made with @substrate/squid-ink-typegen to decode events emitted by ink! contracts use the @subsquid/frontier utils and the EVM typegen to decode event logs and transactions of EVM contracts (Optional) IV. Mix in external data and chain state calls output ​ If you need external (i.e. non-blockchain) data in your transformation, take a look at the External APIs and IPFS page. If any of the on-chain data you need is unavalable from the processor or incovenient to retrieve with it, you have an option to get it via direct chain queries . V. Prepare the store ​ At src/main.ts , change the Database object definition to accept your output data. The methods for saving data will be exposed by ctx.store within the batch handler . PostgreSQL+GraphQL filesystem dataset BigQuery Define the schema of the database (and the core schema of the OpenReader GraphQL API if it is used) at schema.graphql . Regenerate the TypeORM model classes with npx squid-typeorm-codegen The classes will become available at src/model . Compile the models code with npm run build Ensure that the squid has access to a blank database. The easiest way to do so is to start PostgreSQL in a Docker container with docker compose up -d If the container is running, stop it and erase the database with docker compose down before issuing an docker compose up -d . The alternative is to connect to an external database. See this section to learn how to specify the connection parameters. Regenerate a migration with rm -r db/migrations npx squid-typeorm-migration generate You can now use the async functions ctx.store.upsert() and ctx.store.insert() , as well as various TypeORM lookup methods to access the database. See the typeorm-store guide and reference for more info. Filesystem dataset writing, as performed by the @subsquid/file-store package and its extensions, stores the data into one or more flat tables. The exact table definition format depends on the output file format. Decide on the file format you're going to use: Parquet CSV JSON/JSONL If your template does not have any of the necessary packages, install them. Define any tables you need at the tables field of the Database constructor argument: import { Database } from '@subsquid/file-store' const dbOptions = { tables : { FirstTable : new Table ( /* ... */ ) , SecondTable : new Table ( /* ... */ ) , // ... } , // ... } processor . run ( new Database ( dbOptions ) , async ctx => { // ... Define the destination filesystem via the dest field of the Database constructor argument. Options: local folder - use LocalDest from @subsquid/file-store S3-compatible file storage service - install @subsquid/file-store-s3 and use S3Dest Once you're done you'll be able to enqueue data rows for saving using the write() and writeMany() methods of the context store-provided table objects: ctx . store . FirstTable . writeMany ( /* ... */ ) ctx . store . SecondTable . write ( /* ... */ ) The store will write the files automatically as soon as the buffer reaches the size set by the chunkSizeMb field of the Database constructor argument, or at the end of the batch if a call to setForceFlush() was made anywhere in the batch handler. See the file-store guide and the reference pages of its extensions . Follow the guide . VI. Persist the transformed data to your data sink ​ Once your data is decoded , optionally enriched with external data and transformed the way you need it to be, it is time to save it. PostgreSQL+GraphQL filesystem dataset BigQuery For each batch, create all the instances of all TypeORM model classes at once, then save them with the minimal number of calls to upsert() or insert() , e.g.: import { EntityA , EntityB } from './model' processor . run ( new TypeormDatabase ( ) , async ctx => { const aEntities : Map < string , EntityA > = new Map ( ) // id -> entity instance const bEntities : EntityB = [ ] for ( let block of ctx . blocks ) { // fill the containets aEntities and bEntities } await ctx . store . upsert ( [ ... aEntities . values ( ) ] ) await ctx . store . insert ( bEntities ) } ) It will often make sense to keep the entity instances in maps rather than arrays to make it easier to reuse them when defining instances of other entities with relations to the previous ones. The process is described in more detail in the step 2 of the BAYC tutorial . If you perform any database lookups , try to do so in batches and make sure that the entity fields that you're searching over are indexed . See also the patterns and anti-pattens sections of the Batch processing guide. You can enqueue the transformed data for writing whenever convenient without any sizeable impact on performance. At low output data rates (e.g. if your entire dataset is in tens of Mbytes or under) take care to call ctx.store.setForceFlush() when appropriate to make sure your data actually gets written. You can enqueue the transformed data for writing whenever convenient without any sizeable impact on performance. The actual data writing will happen automatically at the end of each batch. The top-down development cycle ​ The bottom-up development cycle described above is convenient for inital squid development and for trying out new things, but it has the disadvantage of not having the means of saving the data ready at hand when initially writing the data decoding/transformation code. That makes it necessary to come back to that code later, which is somewhat inconvenient e.g. when adding new squid features incrementally. The alternative is to do the same steps in a different order: Update the store If necessary, regenerate the utility classes Update the processor configuration Decode and normalize the added data Retrieve any external data if necessary Add the persistence code for the transformed data GraphQL options ​ Store your data to PostgreSQL , then consult Serving GraphQL for options. Scaling up ​ If you're developing a large squid, make sure to use batch processing throughout your code. A common mistake is to make handlers for individual event logs or transactions; for updates that require data retrieval that results in lots of small database lookups and ultimately in poor syncing performance. Collect all the relevant data and process it at once. A simple architecture of that type is discussed in the BAYC tutorial . You should also check the Cloud best practices page even if you're not planning to deploy to SQD Cloud - it contains valuable performance-related tips. Many issues commonly arising when developing larger squids are addressed by the third party @belopash/typeorm-store package . Consider using it. For complete examples of complex squids take a look at the Giant Squid Explorer and Thena Squid repos. Next steps ​ Learn about batch processing . Learn how squid deal with unfinalized blocks . Use external APIs and IPFS in your squid. See how squid should be set up for the multichain setting . Deploy your squid on own infrastructure or to SQD Cloud . Edit this page Previous Indexer from scratch Next Project structure Prepare the environment Understand your technical requirements Start from a template The bottom-up development cycle I. Regenerate the task-specific utilities II. Configure the data requests III. Decode and normalize the data (Optional) IV. Mix in external data and chain state calls output V. Prepare the store VI. Persist the transformed data to your data sink The top-down development cycle GraphQL options Scaling up Next steps
Overview | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Overview On this page Overview The SQD ecosystem is built around SQD Network - a decentralized query engine and a horizontally-scalable data lake for batch queries. Currently, SQD Network serves raw historical on-chain data from various blockchains using custom REST-like APIs . In the future, it will additionally support general-purpose SQL queries and an ever-growing collection of structured data sets derived from on- and off- chain data. SQD Network ​ SQD Network API currently provides: raw event logs transaction data (with receipts) execution traces (for selected networks) state diffs (for selected networks) It supports all major EVM and Substrate networks, with more in the works. Let us know if you'd like us to support some other network. Currently there are two deployments of SQD Network: Production-ready private cluster running on SQD infrastructure (formerly known as Subsquid Archives). Access to the cluster is free and public. A decentralized permissionless network , currently a testnet. User-facing API between these two is identical. Use it directly whenever you need historical blockchain data older than about 6-24 hours in either (1) high volume or (2) filtered form. Example use cases for the SQD Network direct API boost the performance of existing pipelines by replacing per-block RPC requests with batch-requests to SQD Network non-Typescript indexers, mobile SDKs high-performance data pipelines for raw historical data a high-performance data source for tools like ApeWorkX, Cryo ad-hoc queries over historical on-chain data, such as "all transactions from a given address in a range of blocks" "all events with a given topic in a range of blocks" For real-time use cases such as app-specific APIs use Squid SDK : it'll use SQD Network to sync quickly, then seamlessly switch to real-time data. If you already have a subgraph indexer, you also have an option to run it against the network . Squid SDK ​ Squid SDK is a powerful Typescript toolkit for building indexers on top of SQD Network, featuring High-level libraries for extracting and filtering the SQD Network data in what can be though of as Extract-Transform-Load (ETL) pipelines Ergonomic tools for decoding and normalizing raw data and efficiently accessing network state Pluggable data sinks to save data into Postgres, files (local or s3) or BigQuery An expressive GraphQL server with a schema-based config Seamless handling of unfinalized blocks and chain reorganizations for real-time data ingestion rapid data extraction and decoding for local analytics The SDK is a go-to choice for production solutions and prototypes of custom APIs based on data from sets of smart contracts, and low-cost, highly performant in-house data pipelines that can work in real time setting (<1s chain latency). Take a look at the list of concrete, real-world applications for which Squid SDK was a good fit. SQD Cloud ​ A Platform-as-a-Service for deploying Squid SDK indexers, featuring provisioning of Postgres and other compute resources for indexers zero downtime migrations between indexer versions provisioning of high-performance RPC endpoints for real-time applications intuitive deployment management through a Web application or CLI Google Cloud-level SLA What's next? ​ SDK SQD Network SQD Cloud Other Learn about squid components , combining them or follow the end-to-end development guide Explore tutorials or examples Learn how to migrate from The Graph Explore the GraphQL server options See the list of supported networks Learn how to use APIs of the open private network Check out the public network FAQ Learn how to deploy your squid Learn about organizations and how they are billed Use your free playground to develop a prototype of your indexer View our pricing schedule Run subgraphs using SQD Network data Learn about third party tools for using with SDK or for building standalone indexers Edit this page Previous Source data from a portal Next SQD Network SQD Network Squid SDK SQD Cloud What's next?
bigquery-store | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks bigquery-store On this page @subsquid/bigquery-store See also the BigQuery guide . Column types â€‹ Column type Value type Dataset column type String() string STRING Numeric(precision, scale) number &#124 bigint NUMERIC(P[, S]) BigNumeric(precision, scale) number &#124 bigint BIGNUMERIC(P[, S]) Bool() boolean BOOL Timestamp() Date TIMESTAMP Float64() number FLOAT64 Int64() number &#124 bigint INT64 Edit this page Previous S3 support Next Logger Column types
Resources for squid developers | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Resources for squid developers 📄️ Batch processing Batch-based data transformation model 📄️ RPC ingestion and reorgs Data from chains and network consensus 📄️ External APIs and IPFS Use external APIs and IPFS from a squid 📄️ Multichain Combine data from multiple chains 📄️ Serving GraphQL GraphQL servers commonly used in squids 📄️ Self-hosting Deploy squid locally or on-premises 🗃️ Persisting data 4 items 🗃️ EVM-specific 2 items 🗃️ Substrate-specific 5 items 🗃️ Tools 5 items 🗃️ Migration guides 4 items Previous sqd CLI cheatsheet Next Batch processing
Pricing | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Pricing On this page SQD Cloud Pricing tip Use our cost calculator if you need a quick estimate Free squids ​ Each account gets a playground organization where a single free squid can be deployed. Certain limitations apply; check the playground organization section for details. Billing ​ SQD bills organizations for the resources used by their squids. To deploy production-ready squids and configure their resources, the organization must have a Professional status . All such organizations receive the following perks: unmetered GraphQL API requests deploying any number of pay-as-you-go dedicated squids deploying any number of pay-as-you-go collocated squids free 2M RPC addon requests monthly + just $2/1M afterwards priority support Other resources used by squids are priced as described below. Billing typically occurs on the first day of each month. We do provide enterprise discounts. If your use case requires a lot of resources or an individual SLA, book a demo to get a personal quote. Pay-as-you-go prices ​ The pricing is based solely on the resources consumed by the squid, as the SQD Network data is provided free of charge.
The total squid price is the sum of compute and database storage prices. Data egress is not billed. Storage ​ Database storage is billed at 0.5$/mo per Gigabyte, both for dedicated and collocated squids. Compute for dedicated squids ​ Dedicated squids have separate compute profiles for the Postgres database and API/processor services: API/Processor ​ small : 0.04$/hr medium : 0.08$/hr large : 0.15$/hr xlarge : 0.30$/hr 2xlarge : 0.60$/hr API and processor are configured and billed separately. Also, if you use API replicas your cost is multiplied by the number of replicas. For example, running a small processor plus two replicas of medium API costs 0.04+2*0.08=0.20$/hr . Full profile descriptions are available here for the API and here for the processor. Database ​ small : 0.08$/hr medium : 0.16$/hr large : 0.33$/hr xlarge : 0.66$/hr 2xlarge : 1.32$/hr Profiles are described here . Combine with the cost of storage to get the full price of maintaining the database. Hibernated squids ​ For hibernated squids only the storage costs are charged. Multi-processor squids ​ For multi-processor squids each processor is charged separately according to the compute profile. Collocated (development) squids ​ Collocated squids share compute resources and thus their performance is NOT guaranteed. We do not recommend using
collocated squids in production, but it's a good fit for development and testing.
Compute is billed at a flat rate of $0.02/hr ( ~$15/mo ) per each collocated squid (API + processor + DB).
It is not possible to additionally configure a collocated squid except for the storage. RPC requests ​ The premium plan already includes a package of 2M requests per month. Above that, it is $2/1M requests. SLAs ​ Our SLA only applies to dedicated squids deployed to Professional organizations . We align with the SLAs of the Google Cloud Platform and will reduce the bill based on the following schedule: Uptime Rebate 95.00% - < 99.50% 10% 90.00% - < 95.00% 25% < 90.00% 100% Note that the SLA applies only to the provisioning of the SQD Cloud services (API availability, provisioning of compute resources) and DOES NOT apply to client code. In particular, if a squid is stuck due to a bug or an upstream SQD Network/RPC issue, the SLA discounts don't apply. Similarly, the client is responsible for provisioning compute profiles adequate to the expected traffic. For Enterprise plan customers, individual SLA terms may be negotiated. Examples ​ Small dedicated squid ​ 1x API service (small) 1x processor (small) 1x DB (small) 50GB storage Runs 24/7 (720 hours a month). Service $/hr $/mo API (small) $0.04 Processor (small) $0.04 Database (small) $0.08 Compute total $0.16 $115.2 Storage $25 Total $140.2 Processor-only squid ​ A squid writing parquet files to an external s3 bucket. No database or API is provisioned. 1x processor (small) Service $/hr $/mo Processor (small) $0.04 Compute total $0.04 $28.8 Total $28.8 Large production squid ​ 2x API service (medium) 1x processor (small) 1x DB (large) 500GB storage Service $/hr $/mo 2xAPI (medium) $0.16 Processor (small) $0.4 Database (large) $0.33 Compute total $0.53 $381.6 Storage $250 Total $631.6 Test collocated squid: ​ API + processor + DB (collocated) 50GB storage Service $/hr $/mo Compute total 0.02 $15 Storage $25 Total $40 Transition to the Paid Plans ​ The transition to the paid plans will take place in September 2023 to enable smooth onboarding.
The existing Cloud users will be able to see the usage and the provisional billing for September, with a full rebate of the total invoice.
The grace period ends on October 1st, 2023 with the payment rails enabled in the SQD Cloud app. FAQ ​ Q : What is the difference between collocated and dedicated squids? Why are collocated squids much cheaper? A : Collocated squids share compute resources which means that it may be unresponsive if the neighbour squid is under high load.
While we do our best to ensure that the resources are shared fairly, we strongly recommend dedicated services for production use. Q : I deployed my squid yesterday, will it be billed for the whole month? A : No, only the actual hours the squid has been running will be billed. Q : How do I configure the profiles for my squid? A : Use the manifest-based deployments and the profile reference . Q : How can I use the provided RPC package? How do I set the rate limits for it? A : Please look at the RPC addon docs . Edit this page Previous Deployment workflow Next Troubleshooting Free squids Billing Pay-as-you-go prices Storage Compute for dedicated squids Hibernated squids Multi-processor squids Collocated (development) squids RPC requests SLAs Examples Small dedicated squid Processor-only squid Large production squid Test collocated squid: Transition to the Paid Plans FAQ
Indexer from scratch | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Environment set up Indexer from scratch Development flow Project structure sqd CLI cheatsheet Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Getting started Indexer from scratch On this page Indexer from scratch Here's an example of how SDK packages can be combined into a working indexer (called squid ). This page goes through all the technical details to make the squid architecture easier to understand. If you would like to get to a working indexer ASAP, bootstrap from a template . USDT transfers API â€‹ Pre-requisites : NodeJS 20.x or newer, Docker. Suppose the task is to track transfers of USDT on Ethereum, then save the resulting data to PostgreSQL and serve it as a GraphQL API. From this description we can immediately put together a list of packages : @subsquid/evm-processor - for retrieving Ethereum data the triad of @subsquid/typeorm-store , @subsquid/typeorm-codegen and @subsquid/typeorm-migration - for saving data to PostgreSQL We also assume the following choice of optional packages: @subsquid/evm-typegen - for decoding Ethereum data and useful constants such as event topic0 values @subsquid/evm-abi - as a peer dependency for the code generated by @subsquid/evm-typegen @subsquid/graphql-server / OpenReader To make the indexer, follow these steps: Create a new folder and initialise a new project create package.json npm init add .gitignore .gitignore node_modules lib Install the packages: npm i dotenv typeorm @subsquid/evm-processor @subsquid/typeorm-store @subsquid/typeorm-migration @subsquid/graphql-server @subsquid/evm-abi npm i typescript @subsquid/typeorm-codegen @subsquid/evm-typegen --save-dev Add a minimal tsconfig.json : tsconfig.json { "compilerOptions" : { "rootDir" : "src" , "outDir" : "lib" , "module" : "commonjs" , "target" : "es2020" , "esModuleInterop" : true , "skipLibCheck" : true , "experimentalDecorators" : true , "emitDecoratorMetadata" : true } } Define the schema for both the database and the core GraphQL API in schema.graphql : schema.graphql type Transfer @entity { id : ID ! from : String ! @index to : String ! @index value : BigInt ! } Generate TypeORM classes based on the schema: npx squid-typeorm-codegen The TypeORM classes are now available at src/model/index.ts . Prepare the database: create .env and docker-compose.yaml files .env DB_NAME = squid DB_PORT = 23798 RPC_ETH_HTTP = https://rpc.ankr.com/eth docker-compose.yaml services : db : image : postgres : 15 environment : POSTGRES_DB : "${DB_NAME}" POSTGRES_PASSWORD : postgres ports : - "${DB_PORT}:5432" start the database container docker compose up -d compile the TypeORM classes npx tsc generate the migration file npx squid-typeorm-migration generate apply the migration with npx squid-typeorm-migration apply Generate utility classes for decoding USDT contract data based on its ABI. Create an ./abi folder: mkdir abi Find the ABI at the "Contract" tab of the contract page on Etherscan . Scroll down a bit: Copy the ABI, then paste to a new file at ./abi/usdt.json . Run the utility classes generator: npx squid-evm-typegen src/abi ./abi/* The utility classes are now available at src/abi/usdt.ts Tie all the generated code together with a src/main.ts executable with the following code blocks: Imports import { EvmBatchProcessor } from '@subsquid/evm-processor' import { TypeormDatabase } from '@subsquid/typeorm-store' import * as usdtAbi from './abi/usdt' import { Transfer } from './model' EvmBatchProcessor object definition const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( { url : process . env . RPC_ETH_HTTP , rateLimit : 10 } ) . setFinalityConfirmation ( 75 ) // 15 mins to finality . addLog ( { address : [ '0xdAC17F958D2ee523a2206206994597C13D831ec7' ] , topic0 : [ usdtAbi . events . Transfer . topic ] } ) TypeormDatabase object definition const db = new TypeormDatabase ( ) A call to processor.run() with an inline definition of the batch handler processor . run ( db , async ctx => { const transfers : Transfer [ ] = [ ] for ( let block of ctx . blocks ) { for ( let log of block . logs ) { let { from , to , value } = usdtAbi . events . Transfer . decode ( log ) transfers . push ( new Transfer ( { id : log . id , from , to , value } ) ) } } await ctx . store . insert ( transfers ) } ) Note how supplying a TypeormDatabase to the function caused ctx.store to be a PostgreSQL-compatible Store object . Compile the project and start the processor process npx tsc node -r dotenv/config lib/main.js In a separate terminal, configure the GraphQL port and start the GraphQL server: .env DB_NAME=squid DB_PORT=23798 RPC_ETH_HTTP=https://rpc.ankr.com/eth + GRAPHQL_SERVER_PORT=4350 npx squid-graphql-server The finished GraphQL API with GraphiQL is available at localhost:4350/graphql . Final code for this mini-tutorial is available in this repo . tip The commands listed here are often abbreviated as custom sqd commands in squids. If you'd like to do that too you can use the commands.json file of the EVM template as a starter. Edit this page Previous Environment set up Next Development flow USDT transfers API
Transactions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM Transactions On this page Transactions addTransaction(options) ​ Get some or all transactions on the network. options has the following structure: { // data requests from ? : string [ ] to ? : string [ ] sighash ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval logs ? : boolean stateDiffs ? : boolean traces ? : boolean } Data requests: from and to : the sets of addresses of tx senders and receivers. Omit to subscribe to transactions from/to any address. sighash : first four bytes of the Keccak hash (SHA3) of the canonical representation of the function signature. Omit to subscribe to any transaction. range : the range of blocks to consider. Enabling the stateDiffs , traces and/or logs flags will cause the processor to retrieve state diffs , traces and/or event logs that occured as a result of each selected transaction. The data will be added to the appropriate iterables within the block data . Note that transactions can also be requested by addLog() , addStateDiff() and addTrace() as related data. Selection of the exact data to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Some examples are available below. tip Typescript ABI modules generated by squid-evm-typegen provide function sighashes as constants, e.g. import * as erc20abi from './abi/erc20' // ... sighash : [ erc20abi . functions . transfer . sighash ] , // ... Examples ​ Request all EVM calls to the contract 0x6a2d262D56735DbA19Dd70682B39F6bE9a931D98 : processor . addTransaction ( { to : [ '0x6a2d262d56735dba19dd70682b39f6be9a931d98' ] } ) Request all transactions matching sighash of transfer(address,uint256) : processor . addTransaction ( { sighash : [ '0xa9059cbb' ] } ) Request all transfer(address,uint256) calls to the specified addresses, from block 6_000_000 onwards and fetch their inputs. Also retrieve all logs emitted by these calls. processor . addTransaction ( { to : [ '0x6a2d262d56735dba19dd70682b39f6be9a931d98' , '0x3795c36e7d12a8c252a20c5a7b455f7c57b60283' ] , sighash : [ '0xa9059cbb' ] , range : { from : 6_000_000 } , logs : true } ) . setFields ( { transaction : { input : true } } ) Mine all transactions to and from Vitalik Buterin's address vitalik.eth . Fetch the involved addresses, ETH value and hash for each transaction. Get execution traces with the default fields for outgoing transactions. const VITALIK_ETH = '0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045' . toLowerCase ( ) const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setFinalityConfirmation ( 75 ) . addTransaction ( { to : [ VITALIK_ETH ] } ) . addTransaction ( { from : [ VITALIK_ETH ] , traces : true } ) . setFields ( { transaction : { from : true , to : true , value : true , hash : true } } ) processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let c of ctx . blocks ) { for ( let txn of c . transactions ) { if ( txn . to === VITALIK_ETH || txn . from === VITALIK_ETH ) { // just output the tx data to console ctx . log . info ( txn , 'Tx:' ) } } } } ) Edit this page Previous Event logs Next Storage state diffs Examples
Use with Ganache or Hardhat | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Use with Ganache or Hardhat On this page Run squids with Ganache or Hardhat Objective ​ Here we show how it is possible to index an Ethereum development node running locally. This is useful for those who want to start developing a squid ETL or API in a local environment without waiting for the contract to be deployed to a testnet. Pre-requisites ​ Either one of Ganache or Hardhat Squid CLI Docker Setup ​ In this tutorial we will be using a layout in which the squid and the Hardhat/Ganache setup share the same folder. Create a new squid by running: sqd init ethereum-local-indexing -t evm Here, ethereum-local-indexing is the project name and evm refers to the evm template . Install the dependencies and update evm-processor and typeorm-store to their ArrowSquid versions: cd ethereum-local-indexing npm i npm i @subsquid/evm-processor@next @subsquid/typeorm-store@next Next, we set up a local EVM development environment. We consider two common options: Hardhat and Ganache . If you chose Hardhat ​ Install the package . 1. Create project ​ In a terminal, navigate to the squid project root folder. Prepare it by running rm README.md mv tsconfig.json tsconfig.json.save This will prevent collisions between SQD and Hardhat files. Next, run npx hardhat to initialize the Hardhat project. When prompted, choose TypeScript project and keep all other options at defaults. Finally, overwrite tsconfig.json provided by Hardhat: mv tsconfig.json.save tsconfig.json 2. Configure Hardhat automining ​ Then, open the hardhat.config.ts and replace the HardhatUserConfig object with this one: const config : HardhatUserConfig = { defaultNetwork : "hardhat" , networks : { hardhat : { chainId : 1337 , mining : { auto : false , interval : [ 4800 , 5200 ] } } , } , solidity : "0.8.17" , } ; The mining configuration will continuously mine blocks, even if no events or transactions are executed. This can be useful for debugging. You can read more about it on hardhat official documentation . 3. Sample contract ​ There should be a contracts subfolder in the project folder now, with a sample contract named Lock.sol . To compile this contract and verify its correctness, run npx hardhat compile You should find the contract's ABI at artifacts/contracts/Lock.sol/Lock.json . We will use this file in squid development. 4. Launch hardhat node ​ From the project root folder, run npx hardhat node 5. Deploy the contract ​ The node will keep the console window busy. In a different terminal, run this command: npx hardhat run scripts/deploy.ts --network localhost If the contract deployed successfully, the output should look like Lock with 1 ETH and unlock timestamp 1704810454 deployed to 0x5FbDB2315678afecb367f032d93F642f64180aa3 Take note of the contract address, you'll need it later . If you chose Ganache ​ Install Truffle and Ganache packages. 1. Truffle project, sample contract ​ Let's create a new truffle project with a sample contract. In the project main folder run truffle unbox metacoin Compile the contracts with truffle compile You should find the contract's ABI at build/contracts/MetaCoin.json . It will be useful for indexing . 2. Create a workspace ​ Launch the Ganache tool and select the New Workspace (Ethereum) option. Next, provide a name for the workspace and link the Truffle project we just created to it. To do that, click Add project and select the truffle-config.js file in the project root folder. Finally, select the Server tab at the top. In this window, change the server configuration to the exact values reported in this image. The AUTOMINE option is disabled to ease the debugging. Finally, click "Start" to launch the blockchain emulator. 3. Deploy a smart contract ​ Configure Truffle by uncommenting the development section in truffle-config.js and setting its properties as follows: development : { host : "127.0.0.1" , // Localhost (default: none) port : 8545 , // Standard Ethereum port (default: none) network_id : "1337" , // Any network (default: none) } , Deploy the sample smart contract by running truffle migrate Log information similar to this should be displayed: Deploying 'MetaCoin' -------------------- > transaction hash:    0xcdb820827306ebad7c6905d750d07536c3db93f4ef76fd777180bdac16eaa2ca > Blocks: 1 Seconds: 4 > contract address:    0xd095211a90268241D75919f12397b389b1062f6F > block number: 329 > block timestamp: 1673277461 > account:             0x131D37F433BAf649111278c1d6E59843fFB26D28 > balance: 99.98855876 > gas used: 414494 ( 0x6531e ) > gas price: 20 gwei > value sent: 0 ETH > total cost: 0.00828988 ETH > Saving artifacts ------------------------------------- > Total cost: 0.01144124 ETH Take note of the contract address, you'll need it later . Squid development ​ The indexing setup is ready to use. To test it, replace the contents of src/processor.ts with the following. import { TypeormDatabase } from '@subsquid/typeorm-store' import { EvmBatchProcessor } from '@subsquid/evm-processor' const processor = new EvmBatchProcessor ( ) . setRpcEndpoint ( 'http://localhost:8545' ) . setFinalityConfirmation ( 5 ) . addTransaction ( { } ) . setFields ( { transaction : { contractAddress : true } } ) processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let blk of ctx . blocks ) { for ( let txn of blk . transactions ) { console . log ( txn ) } } } ) This defines a squid that retrieves all chain transactions from the local node RPC endpoint without filtering, making sure to retrieve the addresses of the deployed contracts for deployment transactions. Run the squid with docker compose up -d npm run build npx squid-typeorm-migration apply node -r dotenv/config lib/main.js You should see the data of one (for Hardhat) or two (for Truffle+Ganache) contract deployment transactions printed to your terminal. Now you can develop a SQD-based indexer alongside your contracts. Head over to the dedicated tutorial for guidance on squid development. Use the contract's ABI ( here or here ) and contract address ( here and here ) from previous steps and be mindful that the data source of the processor class needs to be set to the local node RPC endpoint, as in the example above: // ... . setRpcEndpoint ( 'http://localhost:8545' ) // ... You can also set the data source through an environment variable like it is done in this complete end-to-end project example (outdated, but still useful). This will make sure the project code stays the same and only the environment variables change depending on where the project is deployed. Edit this page Previous Index to Parquet files Next Simple Substrate squid Objective Pre-requisites Setup If you chose Hardhat If you chose Ganache Squid development
Source data from a portal | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Source data from a portal On this page Source data from a portal info SQD Network portals are currently in closed beta. Please report any bugs or suggestions to the SQD Portal chat or to Squid Devs . This guide lets you replace a gateway of the open private version of SQD Network with a portal of the permissionless SQD Network as your primary source of data. Benefits include: Reduced reliance on centralized services: The permissionless SQD Network consists of over 1200 nodes ran by independent operators , with a total capacity of roughly 1.1Pb. This allows for a great deal of redundancy. You can query the network without relying on any centralized services . You can also use the public portal ran by the SQD team to just try it out. Improved speed: The permissionless version of SQD Network has a lot more bandwidth than the open private network; moreover, portals use the available bandwidth more effectively than gateways. Data fetching was 5-10 times faster in some of our tests. For squids that are not bottlenecked by write operations this will translate into better sync performance. Simplified API: compared to the request-response approach used by gateways, portals offer a simpler stream-based API. Being future-proof: All future development will be focused on portals and the permissionless SQD Network. Open private network will be sunset around May 2025. Here are the steps to migrate: Step 1 ​ Navigate to your squid's folder and install the portal-api version of @subsquid/evm-processor : npm i @subsquid/evm-processor@portal-api Step 2 ​ Obtain an URL of an SQD portal for your dataset. If you are an existing user of the SQD Cloud : enable the preview mode navigate to the portals page click on the tile of your network and follow the instructions Once you're done you should be able to run your (now portal-powered) squid both locally and in the Cloud. You can skip step 3 - it should be already done. If you want to use a private portal: set it up get your portal's dataset-specific API URL for the network you're interested in If you don't have an SQD Cloud account and just want to see a portal in action, you can use the following URL template: https://portal.sqd.dev/datasets/<dataset-slug> where <dataset-slug> is the last path segment of the gateway URL for your network found on this page . For example, the URL for the Ethereum dataset is https://portal.sqd.dev/datasets/ethereum-mainnet Step 3 ​ Configure your squid to ingest data from the portal by replacing the call to .setGateway in the processor configuration (conventionally at src/processor.ts ) with a .setPortal call. If you're using a private portal or a public portal on a local machine, the change will look like this export const processor = new EvmBatchProcessor() - .setGateway('<gateway URL for your dataset>') + .setPortal('<portal URL for your dataset>') If you intend to run your squid in the Cloud , you should source the URL from the environment variable you configured in the manifest, e.g. + import { assertNotNull } from '@subsquid/util-internal'; export const processor = new EvmBatchProcessor() - .setGateway('<gateway URL for your dataset>') + .setPortal(assertNotNull( + process.env.PORTAL_URL, + 'Required env variable PORTAL_URL is missing' + )) If your squid used an RPC endpoint, keep the call for now: portals do not have the ability to ingest data in real time. Yet. Your squid is now ready to source its data from an SQD Network portal. Give it a try! Edit this page Step 1 Step 2 Step 3
Typegen | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK Typegen On this page Typegen A typegen is a tool for generating utility code for technology-specific operations such as decoding. Solana typegen: decodes instruction and log message data based on the IDL exposes useful constants such as program IDs and instruction discriminators provides functions that simplify selecting data items based on accounts Install with npm install @subsquid/solana-typegen The squid-solana-typegen tool generates TypeScript facades for Solana instructions and logs. It takes JSON IDLs as inputs. The IDLs  can be specified in three ways: as a plain JSON file(s): npx squid-solana-typegen src/abi whirlpool.json If you use this option, you can also place your JSON IDLs to the idl folder and run npx squid-solana-typegen src/abi ./idl/* load IDL from a Solana node and generate types: npx squid-solana-typegen src/abi whirLbMiicVdio4qvUfM5KAg6Ct8VwpYzGff3uctyCc #whirlpool Usage â€‹ The generated utility modules have three intended uses: Constants: Solana instruction discriminators: // generated by evm-typegen import * as whirlpool from "./abi/whirlpool" ; const dataSource = new DataSourceBuilder ( ) . setGateway ( "https://v2.archive.subsquid.io/network/solana-mainnet" ) . addInstruction ( { where : { programId : [ whirlpool . programId ] , d8 : [ whirlpool . instructions . swap . d8 ] } } ) . build ( ) ; Decoding of Solana instructions // generated by evm-typegen import * as whirlpool from "./abi/whirlpool" ; // ... for ( let block of blocks ) { for ( let ins of block . instructions ) { if ( ins . programId === whirlpool . programId && ins . d8 === whirlpool . instructions . swap . d8 ) { let decodedSwap = whirlpool . instructions . swap . decode ( ins ) let { accounts , data } = decodedSwap } } } Simplifying the account-specific data requests . addInstruction ( { where : { programId : [ whirlpool . programId ] , d8 : [ whirlpool . instructions . swap . d8 ] , // select instructions for the USDC-SOL pair only ... whirlpool . instructions . swap . accountSelection ( { whirlpool : [ '7qbRF6YsyGuLUVs6Y1q64bdVrfe4ZcUUz1JRdoVNUJnm' ] } ) } } ) Edit this page Previous Field selection Next Network API Usage
sqd secrets | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd secrets On this page sqd secrets Manage account secrets The secrets are exposed as a context , and are accessible to all the squids deployed by the current SQD Cloud organization . sqd secrets list sqd secrets remove NAME sqd secrets set NAME VALUE sqd secrets list ​ List organization secrets in the Cloud USAGE $ sqd secrets list [--interactive] [-o <value>] FLAGS -o, --org=<value>       Organization --[no-]interactive  Disable interactive mode ALIASES $ sqd secrets ls See code: src/commands/secrets/ls.ts sqd secrets remove NAME ​ Delete an organization secret in the Cloud USAGE $ sqd secrets remove NAME [--interactive] [-o <value>] ARGUMENTS NAME  The secret name FLAGS -o, --org=<value>       Organization --[no-]interactive  Disable interactive mode ALIASES $ sqd secrets rm See code: src/commands/secrets/rm.ts sqd secrets set NAME VALUE ​ Add or update an organization secret in the Cloud. If value is not specified, it is read from standard input. The secret will be exposed as an environment variable with the given name to all the squids in the organization. NOTE: The changes take affect only after a squid is restarted or updated. USAGE $ sqd secrets set NAME VALUE [--interactive] [-o <value>] ARGUMENTS NAME   The secret name VALUE  The secret value FLAGS -o, --org=<value>       Organization --[no-]interactive  Disable interactive mode See code: src/commands/secrets/set.ts Edit this page Previous sqd run Next sqd tags sqd secrets list sqd secrets remove NAME sqd secrets set NAME VALUE
Generating utility modules | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Generating utility modules Data decoding Direct RPC queries Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools Type-safe decoding Generating utility modules On this page Generating utility modules squid-evm-typegen squid-substrate-typegen squid-ink-typegen The squid-evm-typegen(1) tool generates TypeScript facades for EVM transactions, logs and eth_call queries. The generated facades are assumed to be used by squids indexing EVM data. The tool takes a JSON ABIs as an input. Those can be specified in three ways: as a plain JSON file(s): npx squid-evm-typegen src/abi abi/erc20.json To include all files in ./abi and add an interface for the Multicall contract, run npx squid-evm-typegen ./src/abi ./abi/*.json --multicall You can get JSON ABIs for explorer (Etherscan, Bscscan etc) verified contract by visiting the contract page, going to the "Contract" tab and scrolling down to the "Contract ABI" section. Do not use the "Export ABI" function! Copy the contents to the clipboard and paste them to a new JSON file. (requires an Etherscan API key) as a contract address. One can pass multiple addresses at once. npx squid-evm-typegen --etherscan-api-key < your_key > src/abi 0xBB9bc244D798123fDe783fCc1C72d3Bb8C189413 info Please check if your contract is a proxy when using this method. If it is, consult this page for guidance. as an arbitrary URL: npx squid-evm-typegen src/abi https://example.com/erc721.json In all cases typegen will use basename of the ABI as the root name for the generated files. You can change the basename of generated files using the fragment (#) suffix. squid-evm-typegen src/abi 0xBB9bc244D798123fDe783fCc1C72d3Bb8C189413 #my-contract-name Arguments: output-dir output directory for generated definitions abi A contract address, an URL or a local path to the ABI file. Accepts multiple contracts. Options: --multicall generate a facade for the MakerDAO multicall contract. May significantly improve the performance of contract state calls by batching RPC requests --etherscan-api <url> etherscan API to fetch contract ABI by a known address. By default, https://api.etherscan.io/ --clean delete output directory before the run -h, --help display help for command warning The generated modules depend on @subsquid/evm-abi . Please add it to your package.json as a peer dependency if it's not there already: npm i @subsquid/evm-abi Usage ​ The generated utility modules have three intended uses: Constants: EVM log topic0 values and sighashes for transactions. Example: // generated by evm-typegen import * as weth from './abi/weth' const CONTRACT_ADDRESS = '0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2' . toLowerCase ( ) const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . addLog ( { address : [ CONTRACT_ADDRESS ] , topic0 : [ weth . events . Deposit . topic , weth . events . Withdrawal . topic ] } ) Decoding of EVM logs and transactions Direct chain state queries , including queries to multicall . The substrate typegen tool is a part of Squid SDK. It generates TypeScript wrappers for interfacing Substrate events and calls. Usage: npx squid-substrate-typegen typegen.json If necessary, multiple config files can be supplied: npx squid-substrate-typegen typegen0.json typegen1.json .. . The structure of the typegen.json config file is best illustrated with an example: { "outDir" : "src/types" , "specVersions" : "https://v2.archive.subsquid.io/metadata/kusama" , "pallets" : { // add one such section for each pallet "Balances" : { "events" : [ // list of events to generate wrappers for "Transfer" ] , "calls" : [ // list of calls to generate wrappers for "transfer_allow_death" ] , "storage" : [ "Account" ] , "constants" : [ "ExistentialDeposit" ] } } } The specVersions field is either a metadata service endpoint URL, like https://v2.archive.subsquid.io/metadata/{network} or a path to a jsonl file generated by substrate-metadata-explorer(1) . To generate all items defined by a given pallet, set any of the events , calls , storage or constants fields to true , e.g. { "outDir" : "src/types" , "specVersions" : "kusamaVersions.jsonl" , "pallets" : { "Balances" : { // generate wrappers for all Balances pallet constants "constants" : true } } } info In the rare cases when the typegen needs a types bundle, supply it alongside metadata: { "outDir" : "src/types" , "specVersions" : "westendVersions.jsonl" , "typesBundle" : "westendTypes.json" , ... } TypeScript wrappers ​ Wrappers generated by the typegen command can be found in the specified outDir ( src/types by convention). Assuming that this folder is imported as types (e.g. with import * as types from './types' ), you'll be able to find the wrappers at: for events: types.events.${palletName}.${eventName} for calls: types.calls.${palletName}.${callName} for storage items: types.storage.${palletName}.${storageItemName} for constants: types.constants.${palletName}.${constantName} All identifiers (pallet name, call name etc) are lowerCamelCased. E.g. the constant Balances.ExistentialDeposit becomes types.events.balances.existentialDeposit and the call Balances.set_balance becomes types.calls.setBalance . Usage ​ Item name constants (e.g. events.balances.transfer.name ). Runtime versioning-aware decoding . Chain storage queries Runtime constants: import { constants } from './types' // ... processor . run ( new TypeormDatabase ( ) , async ctx => { for ( let block of ctx . blocks ) { if ( constants . balances . existentialDeposit . v1020 . is ( block . header ) ) { let c = new constants . balances . existentialDeposit . v1020 . get ( block . header ) ctx . log . info ( ` Balances.ExistentialDeposit (runtime version V1020): ${ c } ` ) } } } ) Use squid-ink-typegen to generate facade classes for decoding ink! smart contract data from JSON ABI metadata. Usage: npx squid-ink-typegen --abi abi/erc20.json --output src/abi/erc20.ts The generated source code exposes the decoding methods , some useful types and a class for performing wrapped RPC queries , using @subsquid/ink-abi under the hood Edit this page Previous Type-safe decoding Next Data decoding Usage TypeScript wrappers Usage
Saving to BigQuery | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data Store interface Saving to PostgreSQL Saving to filesystems Saving to BigQuery EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Persisting data Saving to BigQuery On this page BigQuery support Squids can store their data to BigQuery datasets using the @subsquid/bigquery-store package. Define and use the Database object as follows: src/main.ts import { Column , Table , Types , Database } from '@subsquid/bigquery-store' import { BigQuery } from '@google-cloud/bigquery' const db = new Database ( { bq : new BigQuery ( ) , dataset : 'subsquid-datasets.test_dataset' , tables : { TransfersTable : new Table ( 'transfers' , { from : Column ( Types . String ( ) ) , to : Column ( Types . String ( ) ) , value : Column ( Types . BigNumeric ( 38 ) ) } ) } } ) processor . run ( db , async ctx => { // ... let from : string = ... let to : string = ... let value : bigint | number = ... ctx . store . TransfersTable . insert ( { from , to , value } ) } ) Here, bq is a BigQuery instance. When created without arguments like this it'll look at the GOOGLE_APPLICATION_CREDENTIALS environment variable for a path to a JSON with authentication details. dataset is the path to the target dataset. warning The dataset must be created prior to running the processor. tables lists the tables that will be created and populated within the dataset. For every field of the tables object an eponymous field of the ctx.store object will be created; calling insert() or insertMany() on such a field will result in data being queued for writing to the corresponding dataset table. The actual writing will be done at the end of the batch in a single transaction, ensuring dataset integrity. Tables are made out of statically typed columns. Available types are listed on the reference page . Deploying to SQD Cloud ​ We discourage uploading any sensitive data with squid code when deploying to SQD Cloud. To pass your credentials JSON to your squid, create a Cloud secret variable populated with its contents: sqd secrets set GAC_JSON_FILE < creds.json Then in src/main.ts write the contents to a file: import fs from 'fs' fs . writeFileSync ( 'creds.json' , process . env . GAC_JSON_FILE || '' ) Set the GOOGLE_APPLICATION_CREDENTIALS variable and request the secret in the deployment manifest : squid.yaml deploy : processor : env : GAC_JSON_FILE : $ { { secrets.GAC_JSON_FILE } } GOOGLE_APPLICATION_CREDENTIALS : creds.json Examples ​ An end-to-end example geared towards local runs can be found in this repo . Look at this branch for an example of a squid made for deployment to SQD Cloud. Troubleshooting ​ Transaction is aborted due to concurrent update ​ This means that your project has an open session that is updating some of the tables used by the squid. Most commonly, the session is left by a squid itself after an unclean termination. You have two options: If you are not sure if your squid is the only app that uses sessions to access your BigQuery project, find the faulty session manually and terminate it. See Get a list of your active sessions and Terminate a session by ID . DANGEROUS If you are absolutely certain that the squid is the only app that uses sessions to access your BigQuery project, you can terminate all the dangling sessions by running FOR session in ( SELECT session_id , MAX ( creation_time ) AS last_modified_time , FROM ` region-us ` . INFORMATION_SCHEMA . SESSIONS_BY_PROJECT WHERE session_id IS NOT NULL AND is_active GROUP BY session_id ORDER BY last_modified_time DESC ) DO CALL BQ . ABORT_SESSION ( session . session_id ) ; END FOR ; Replace region-us with your dataset's region in the code above. You can also enable abortAllProjectSessionsOnStartup and supply datasetRegion in your database config to perform this operation at startup: const db = new Database ( { // ... abortAllProjectSessionsOnStartup : true , datasetRegion : 'region-us' } ) This method will cause data loss if, at the moment when the squid starts, some other app happens to be writing data anywhere in the project using the sessions mechanism. Error 413 (Request Entity Too Large) ​ Squid produced too much data per batch per table and BigQuery refused to handle it. Begin by finding out which table causes the issue (e.g. by counting insert() calls), then enable pagination for that table: const db = new Database ( { bq : new BigQuery ( ) , // set GOOGLE_APPLICATION_CREDENTIALS at .env dataset : ` ${ projectId } . ${ datasetId } ` , tables : { TransfersTable : new Table ( 'transfers' , { from : Column ( Types . String ( ) ) , to : Column ( Types . String ( ) ) , value : Column ( Types . BigNumeric ( 38 ) ) } , 5000 // <- page size for the insert operation ) } , ... Edit this page Previous Saving to filesystems Next EVM-specific Deploying to SQD Cloud Examples Troubleshooting Transaction is aborted due to concurrent update Error 413 (Request Entity Too Large)
sqd list | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd list On this page sqd list List squids deployed to the Cloud sqd list sqd list â€‹ List squids deployed to the Cloud USAGE $ sqd list [--interactive] [--truncate] [-r [<org>/]<name>(@<slot>|:<tag>) | -o <code> | -n <name> | [-s <slot>] | [-t <tag>]] FLAGS --[no-]interactive  Disable interactive mode --[no-]truncate     Truncate data in columns: false by default SQUID FLAGS -n, --name=<name>                               Name of the squid -r, --reference=[<org>/]<name>(@<slot>|:<tag>)  Fully qualified reference of the squid. It can include the organization, name, slot, or tag -s, --slot=<slot>                               Slot of the squid -t, --tag=<tag>                                 Tag of the squid ORG FLAGS -o, --org=<code>  Code of the organization ALIASES $ sqd ls See code: src/commands/ls.ts Edit this page Previous sqd init Next sqd logs sqd list
Direct RPC queries | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Generating utility modules Data decoding Direct RPC queries Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools Type-safe decoding Direct RPC queries On this page Direct RPC queries squid-evm-typegen squid-substrate-typegen squid-ink-typegen In order to make on-chain calls, one must set up a JSON-RPC endpoint using setRpcEndpoint() . We recommend using a private endpoint from e.g. BlastAPI or the SQD Cloud's RPC addon , and set it via an environment variable: //... . setRpcEndpoint ( process . env . RPC_ETH_HTTP ) //... You can define the RPC_ETH_HTTP in three ways: for local runs, simply update the local .env file; for squids deployed to Cloud define it as a secret on your Cloud account; if you are using the RPC addon , leave it to the Cloud to define it for you. Contract class ​ The EVM contract state is accessed using the Contract class generated by squid-evm-typegen . It takes a handler context and a contract address as constructor arguments. The state is always accessed at the context block height unless explicitly defined in the constructor. For example, assume that we index an ERC721 contract. Typescript ABI module generated with squid-evm-typegen will contain the following class: export class Contract extends ContractBase { //... balanceOf ( owner : string ) : Promise < ethers . BigNumber > { return this . call ( 'balanceOf' , [ owner ] ) } //... } Now suppose we want to query our contract from the batch handler. To create a Contract pass the context object and the current Block to its constructor, then query the contract state at that block: // ... const CONTRACT_ADDRESS = '0xb654611f84a8dc429ba3cb4fda9fad236c505a1a' processor . run ( new TypeormDatabase ( ) , async ctx => { for ( const block of ctx . blocks ) { const contract = new abi . Contract ( ctx , block . header , CONTRACT_ADDRESS ) // query the contract state at the current block const uri = await contract . balanceOf ( '0xd8da6bf26964af9d7eed9e03e53415d37aa96045' ) // ... } } ) Batch state queries ​ The MakerDAO Multicall contract was designed to batch multiple state queries into a single contract call. In the context of indexing, it normally significantly improves the indexing speed since JSON RPC calls are typically the bottleneck. Multicall contracts are deployed in many EVM chains, see the contract repo for addresses. You can use any of them with a multicall Typescript module that is generated when running squid-evm-typegen with --multicall option. The module exports a Multicall class with this method: tryAggregate < Args extends any [ ] , R > ( func : Func < Args , { } , R > , calls : [ address : string , args : Args ] [ ] , paging ? : number ) : Promise < MulticallResult < R > [ ] > The arguments are as follows: func : the contract function to be called calls : an array of tuples [contractAddress: string, args] . Each specified contract will be called with the specified arguments. paging an (optional) argument for the maximal number of calls to be batched into a single JSON PRC request. Note that large page sizes may cause timeouts. A typical usage is as follows: src/main.ts // generated by evm-typegen import { functions } from './abi/mycontract' import { Multicall } from './abi/multicall' const MY_CONTRACT = '0xac5c7493036de60e63eb81c5e9a440b42f47ebf5' const MULTICALL_CONTRACT = '0x5ba1e12693dc8f9c48aad8770482f4739beed696' processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let c of ctx . blocks ) { // some logic } const lastBlock = ctx . blocks [ ctx . blocks . length - 1 ] const multicall = new Multicall ( ctx , lastBlock , MULTICALL_CONTRACT ) // call MY_CONTRACT.myContractMethod('foo') and MY_CONTRACT.myContractMethod('bar') const args = [ 'foo' , 'bar' ] const results = await multicall . tryAggregate ( functions . myContractMethod , args . map ( a => [ MY_CONTRACT , a ] ) as [ string , any [ ] ] , 100 ) results . forEach ( ( res , i ) => { if ( res . success ) { ctx . log . info ( ` Result for argument ${ args [ i ] } is ${ res . value } ` ) } } ) } ) It is sometimes impossible to extract the required data with only event and call data without querying the runtime state.
The context exposes a lightweight gRPC client to the chain node accessible via ctx._chain .
It exposes low-level methods for accessing the storage. However, the recommended way to query the storage is with type-safe wrappers generated with squid-substrate-typegen . Substrate typegen tool exposes storage access wrappers at src/types/storage.ts . The wrappers follow the general naming pattern used by Substrate typegen: storage.${palletName}.${storageName} with all identifiers lowerCamelCased. Each wrapper exposes a generated get() query method and, if available, methods for multi-key queries, listing keys, key-value pairs and retrieving the default value. Note that the generated getters always query historical blockchain state at the height derived from their block argument . Here is an example of one such wrapper: src/types/balances/storage.ts import { sts , Block , Bytes , Option , Result , StorageType } from '../support' import * as v1050 from '../v1050' import * as v9420 from '../v9420' export const account = { v1050 : new StorageType ( 'Balances.Account' , 'Default' , [ v1050 . AccountId ] , v1050 . AccountData ) as AccountV1050 , v9420 : new StorageType ( 'Balances.Account' , 'Default' , [ v9420 . AccountId32 ] , v9420 . AccountData ) as AccountV9420 , } export interface AccountV1050 { getDefault ( block : Block ) : v1050 . AccountData get ( block : Block , key : v1050 . AccountId ) : Promise < v1050 . AccountData | undefined > getMany ( block : Block , keys : v1050 . AccountId [ ] ) : Promise < v1050 . AccountData | undefined [ ] > } export interface AccountV9420 { getDefault ( block : Block ) : v9420 . AccountData get ( block : Block , key : v9420 . AccountId32 ) : Promise < v9420 . AccountData | undefined > getMany ( block : Block , keys : v9420 . AccountId32 [ ] ) : Promise < v9420 . AccountData | undefined [ ] > getKeys ( block : Block ) : Promise < v9420 . AccountId32 [ ] > getKeys ( block : Block , key : v9420 . AccountId32 ) : Promise < v9420 . AccountId32 [ ] > getKeysPaged ( pageSize : number , block : Block ) : AsyncIterable < v9420 . AccountId32 [ ] > getKeysPaged ( pageSize : number , block : Block , key : v9420 . AccountId32 ) : AsyncIterable < v9420 . AccountId32 [ ] > getPairs ( block : Block ) : Promise < [ k : v9420 . AccountId32 , v : v9420 . AccountData | undefined ] [ ] > getPairs ( block : Block , key : v9420 . AccountId32 ) : Promise < [ k : v9420 . AccountId32 , v : v9420 . AccountData | undefined ] [ ] > getPairsPaged ( pageSize : number , block : Block ) : AsyncIterable < [ k : v9420 . AccountId32 , v : v9420 . AccountData | undefined ] [ ] > getPairsPaged ( pageSize : number , block : Block , key : v9420 . AccountId32 ) : AsyncIterable < [ k : v9420 . AccountId32 , v : v9420 . AccountData | undefined ] [ ] > } The generated access interface provides methods for accessing: the default storage value with getDefault(block) a single storage value with get(block, key) multiple values in a batch call with getMany(block, keys[]) all storage keys with getKeys(block) all keys with a given prefix with getKeys(block, keyPrefix) (only if the storage keys are decodable) paginated keys via getKeysPaged(pageSize, block) and getKeysPaged(pageSize, block, keyPrefix) key-value pairs via getPairs(block) and getPairs(block, keyPrefix) paginated key-value pairs via getPairsPaged(pageSize, block) and getPairsPaged(pageSize, block, keyPrefix) Example ​ src/main.ts import { storage } from './types' processor . run ( new TypeormDatabase ( ) , async ctx => { let aliceAddress = ss58 . decode ( '5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY' ) . bytes for ( const blockData of ctx . blocks ) { if ( storage . balances . account . v1050 . is ( blockData . header ) ) { let aliceBalance = ( await storage . balances . account . v1050 . get ( blockData . header , aliceAddress ) ) ?. free ctx . log . info ( ` Alice free account balance at block ${ blockData . header . height } : ${ aliceBalance } ` ) } } } ) The generated Contract class provides facades for all state calls that don't mutate the contract state. The info about the state mutability is taken from the contract metadata. // Generated code: export class Contract { // optional blockHash indicates the block at which the state is queried constructor ( private ctx : ChainContext , private address : string , private blockHash ? : string ) { } total_supply ( ) : Promise < Balance > { return this . stateCall ( '0xdb6375a8' , [ ] ) } balance_of ( owner : Uint8Array ) : Promise < bigint > { return this . stateCall ( '0x6568382f' , [ owner ] ) } } Example ​ processor . run ( new TypeormDatabase ( ) , async ctx => { for ( let block of ctx . blocks ) { for ( let event of block . events ) { // query the balance at the current block const contract = new Contract ( ctx , CONTRACT_ADDRESS , block . header . hash ) let aliceAddress = ss58 . decode ( '5GrwvaEF5zXb26Fz9rcQpDWS57CtERHpNehXCPcNoHGKutQY' ) . bytes let aliceBalance = await contract . balance_of ( aliceAddress ) } } } ) Edit this page Previous Data decoding Next Squid generation tools Contract class Batch state queries Example Example
prod | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI prod prod sqd prod and the production aliases system are deprecated starting from @subsquid/ [email protected] . Zero downtime updates are now handled by the slots and tags system . See also the changelog for a comparison between production aliases and the new system. Edit this page Previous sqd logs Next sqd remove
sqd gateways | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd gateways On this page sqd gateways Explore data sources for a squid sqd gateways list sqd gateways list â€‹ List available gateways USAGE $ sqd gateways list [--interactive] [-t <evm|substrate>] [-n <regex>] [-c <number>] FLAGS -c, --chain=<number>        Filter by chain ID or SS58 prefix -n, --name=<regex>          Filter by network name -t, --type=<evm|substrate>  Filter by network type --[no-]interactive      Disable interactive mode ALIASES $ sqd gateways ls See code: src/commands/gateways/ls.ts Edit this page Previous sqd explorer Next sqd init sqd gateways list
Deploy your indexers to SQD Cloud | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Deploy your indexers to SQD Cloud 📄️ Deployment workflow Run a production-ready squid in Cloud 📄️ Pricing Subscription types and pay-as-you-go prices 📄️ Troubleshooting "Secrets outdated. Please restart the squid" warning 🗃️ Resources 11 items 🗃️ Reference 7 items Previous SQD vs The Graph Next Deployment workflow
Environment set up | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Environment set up Indexer from scratch Development flow Project structure sqd CLI cheatsheet Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Getting started Environment set up On this page Environment set up Node.js ​ To install Node.js: Linux Mac Windows Use the package manager of your distro or the official binaries . Use the official binaries or Homebrew . The best bet is to leverage WSL2 and follow this guide . Official installer also works. Make sure that your Node.js installation is v16 or newer. To check an existing installation, run: node --version Squid CLI ​ Follow these instructions . Git ​ Squid CLI uses Git to retrieve templates. To install it: Linux Mac Windows Use the package manager of your distro. Use the official installer or any one of the alternative approaches . Use Git for Windows . Docker ​ Most squids use a database to store the processed data. Install Docker to manage local squid databases with convenience. Linux Mac Windows Here’s an instruction on how to install Docker on Ubuntu On other distros consider using their package managers Install the Desktop version . Install the Desktop version . Edit this page Previous Getting started Next Indexer from scratch Node.js Squid CLI Git Docker
Multichain | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Multichain On this page Multichain indexing Squids can extract data from multiple chains into a shared data sink. If the data is stored to Postgres it can then be served as a unified multichain GraphQL API . To do this, run one processor per source network: Make a separate entry point ( main.ts or equivalent) for each processor. The resulting folder structure may look like this: ├── src │   ├── bsc │   │   ├── main.ts │   │   └── processor.ts │   ├── eth │   │   ├── main.ts │   │   └── processor.ts Alternatively, parameterize your processor using environment variables: you can set these on a per-processor basis if you use a deployment manifest to run your squid. Arrange for running the processors alongside each other conveniently: Add sqd commands for running each processor to commands.json , e.g. commands.json ... "process:eth" : { "deps" : [ "build" , "migration:apply" ] , "cmd" : [ "node" , "lib/eth/main.js" ] } , "process:bsc" : { "deps" : [ "build" , "migration:apply" ] , "cmd" : [ "node" , "lib/bsc/main.js" ] } , ... Full example If you are going to use sqd run for local runs or deploy your squid to SQD Cloud , list your processors at the deploy.processor section of your deployment manifest : deploy : processor : - name : eth - processor cmd : [ "sqd" , "process:prod:eth" ] - name : bsc - processor cmd : [ "sqd" , "process:prod:bsc" ] Make sure to give each processor a unique name! On Postgres ​ Also ensure that State schema name for each processor is unique src/bsc/main.ts processor . run ( new TypeormDatabase ( { stateSchema : 'bsc_processor' } ) , async ctx => { // ... src/eth/main.ts processor . run ( new TypeormDatabase ( { stateSchema : 'eth_processor' } ) , async ( ctx ) => { // ... Schema and GraphQL API are shared among the processors. Handling concurrency ​ Cross-chain data dependencies are to be avoided. With the default isolation level used by TypeormDatabase , SERIALIZABLE , one of the processors will crash with an error whenever two cross-dependent transactions are submitted to the database simultaneously. It will write the correct data when restarted, but such restarts can impact performance, especially in squids that use many (>5) processors. The alternative isolation level is READ COMMITTED . At this level data dependencies will not cause the processors to crash, but the execution is not guaranteed to be deterministic unless the sets of records that different processors read/write do not overlap. To avoid cross-chain data dependencies, use per-chain records for volatile data. E.g. if you track account balances across multiple chains you can avoid overlaps by storing the balance for each chain in a different table row. When you need to combine the records (e.g. get a total of all balaces across chains) use a custom resolver to do it on the GraphQL server side. It is OK to use cross-chain entities to simplify aggregation. Just don't store any data in them: type Account @entity { id : ID ! # evm address balances : [ Balance ! ] ! @derivedFrom ( "field" : "account" ) } type Balance @entity { id : ID ! # chainId + evm address account : Account ! value : BigInt ! } On file-store ​ Ensure that you use a unique target folder for each processor. Example ​ A complete example is available here . Edit this page Previous External APIs and IPFS Next Serving GraphQL On Postgres Handling concurrency On file-store Example
Caching | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Caching Custom API extensions Subscriptions DoS protection Access control Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Configuration Caching On this page Caching The GraphQL API server provided by @subsquid/graphql-server supports caching via additional flags. It is done on a per-query basis. The whole response is cached for a specified amount of time ( maxAge ). To enable caching when deploying to SQD Cloud, add the caching flags to the serve:prod command definition at commands.json , then use that command to run the server in the deployment manifest . Cloud currently supports only in-memory cache.
For example, snippets below will deploy a GraphQL API server with a 100Mb in-memory cache and invalidation time of 5 seconds: commands.json ... "serve:prod" : { "description" : "Start the GraphQL API server with caching and limits" , "cmd" : [ "squid-graphql-server" , "--dumb-cache" , "in-memory" , "--dumb-cache-ttl" , "5000" , "--dumb-cache-size" , "100" , "--dumb-cache-max-age" , "5000" ] } ... squid.yaml # ... deploy : # other services ... api : cmd : [ "sqd" , "serve:prod" ] Caching flags list is available via npx squid-graphql-server --help . Here are some more details on them: --dumb-cache <cache-type> ​ Enables cache, either in-memory or redis . For redis , a Redis connection string must be set by a variable REDIS_URL . SQD Cloud deployments currently support only in-memory cache. --dumb-cache-size <mb> ​ Cache max size. Applies only to in-memory cache. --dumb-cache-max-age <ms> ​ A globally set max age in milliseconds. The cached queries are invalidated after that period of time. --dumb-cache-ttl <ms> ​ Time-to-live for in-memory cache entries. Applies only to in-memory cache. The entries are eligible for eviction from the cache if not updated for longer than the time-to-live time. Edit this page Previous Configuration Next Custom API extensions --dumb-cache <cache-type> --dumb-cache-size <mb> --dumb-cache-max-age <ms> --dumb-cache-ttl <ms>
Slots and tags | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Slots and tags On this page Slots and tags tip If you used SQD Cloud before the slots and tags update, you may want to take a look at the Changelog , including its Backwards compatibility section . Starting with @subsquid/cli>=3.0.0 SQD Cloud is using a new deployment system based on the following concepts: Slots: Given a squid name and an organization , slot name uniquely identifies a squid deployment. Slots are identified by short strings. They directly replace versions from the old workflow, which were identified by numbers. You can specify a slot on deployment or omit it. In the latter case the Cloud will create a new slot with a random identifier. Deploying to an already occupied slot will result in updating the initial deployment. Tags: A way to label deployments. Tags rules: Each tag can be assigned to only one deployment among those that have the same name + organization. Assigning an existing tag moves it. Multiple tags can be assigned to any one deployment. Tags uses: Each tag added to the slot adds a new custom endpoint. You can address deployments by their tags to perform various operations, including redeployment. Both tags and slots can be specified in squid manifest as well as via command line, giving rise to a variety of possible workflows. Let's take a look at a few common scenarios. Zero-downtime updates ​ info The functionality in this section mirrors the now-deprecated production aliases . As you develop your squid you will sometimes need to re-sync it from scratch. With tags you can do this without a maintenance downtime. Here is how to prepare your squid on its initial deployment: Go to your squid project (or the EVM template if you just want something to test this with) and open the squid manifest ( squid.yaml by default). Remove any deployment references: version: (old) slot: (new) tag: (new) This is needed because we'll let the Cloud auto generate a slot identifier for us. The top of your manifest file should look something like this: manifestVersion : subsquid.io/v0.1 name : my - squid description : 'The very first ... Deploy the squid with sqd deploy . while in the project folder. Once complete, you’ll see a message like this: ================================================= A new squid my-org/my-squid@fj0anc has been successfully created ================================================= fj0anc is the slot name that Cloud automatically assigned to your squid deployment. Here's how this deployment looks like in the app : At this stage you can already access your deployment's API via a slot-based URL. However, that API will go down if we reset the deployment's database. Instead, let us use a tag-based URL that can be redirected to an API of another deployment. Assign a tag to your deployment: sqd tags add production -n my-squid -s fj0anc This assigns the production tag to the deployment, making the API accessible via: https://my-org.squids.live/my-squid:production/api/graphql Use this URL to access the API in your frontend app. Now, let's say you've made some changes to your codebase and need to re-sync your squid from scratch. Follow these steps: Make one more deployment of your squid. For that you can just re-run sqd deploy . : since no slot, version or tag is supplied in the manifest, the Cloud will automatically create a new slot for you. Your Cloud deployments might now look like this: Wait for your new deployment to sync, then test its API using its slot-based ( canonical ) URL: https://my-org.squids.live/my-squid@u293zi/api/graphql When you're ready to switch, simply reassign the production tag to the new deployment: sqd tags add production -n my-squid -s u293zi The tag-based URL will now point to the new deployment, u293zi , with the switch typically taking just a few seconds. Finally, if you no longer need the old deployment, remove it by slot: sqd rm -n my-squid -s fj0anc Multiple major versions ​ One common scenario occurs when you release a breaking change and need to maintain several major versions of your API. With tags, you can keep all these major versions under the same squid name. Let’s say you’ve introduced a breaking change to the squid from the previous example. Deploy your new version as follows: sqd deploy . # then, assuming that the new deployment got slot f69x40 sqd tags add production-v1 -n my-squid -s f69x40 Alternatively, you can use the --add-tag option of sqd deploy to do the same thing without having to handle the autogenerated slot name: sqd deploy . --add-tag production-v1 This will produce exactly the same result. Here's how it may look like in the Cloud app: Now you have two independent APIs accessible by tag-based URLs and you can perform zero-downtime updates on each. Development workflows ​ In the previous section we considered the situation when multiple tags are used to denote major versions of the API. However, the exact same approach can be used to separate, for example, development from production: When working on your development deployment, you’ll likely need to perform a variety of operations on it, frequently and without the concern of downtime. For that, all Squid CLI commands allow addressing deployments by tag. For example, to access logs of your development deployment use sqd logs -n my-squid -t development This command allows you to pull logs directly from the deployment tagged development , without needing to remember or specify the slot name. Moreover, you can update your development -tagged deployment without having to know its slot name: sqd deploy . -t development It's even possible to ensure that whenever you deploy from the current directory, the deployment goes to the slot tagged development . Simply add the following line to squid.yaml : tag : development This may serve as a safeguard, preventing accidental deployments to production or the creation of unintended deployments. Next steps ​ Now that you've seen how slots and tags can help you with some common tasks, you may want to use them to design new workflows uniquely suitable for the needs of your project. Here are some resources that may help you with that: Squid CLI documentation , particularly the sqd deploy page . Most of it is also available via sqd deploy --help All the flags you can use to refer to deployments and to disable protective user prompts are described there. The detailed changelog that (for now) doubles as the reference documentation page. It describes the new system by comparing it to the old one that was based on versions and production aliases . If you used the old system or witnessed the migration of your organization, you may want to take a look at the Backwards compatibility section of the changelog. Edit this page Previous Organizations Next Query optimization Zero-downtime updates Multiple major versions Development workflows Next steps
Store interface | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data Store interface Saving to PostgreSQL Saving to filesystems Saving to BigQuery EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Persisting data Store interface On this page Store interface Store is a generic interface exposed by DataHandlerContext at .store . It is used in the batch handler to persist data. Its concrete type is inferred from the Database argument of the run() processor method : run < Store > ( db : Database < Store > , handler : ( ctx : DataHandlerContext < Store , F extends FieldSelection > ) => Promise < void > ) : void The Database implementation supplied here defines the store and the way the processor persists its status . Squid SDK supports Database implementations for: TypeORM -compatible databases file-based datasets Google BigQuery Support for more databases and analytics storage will be added in the future. Custom Database â€‹ A custom implementation of the Database interface is the recommended solution for squids with multiple data sinks and/or for non-transactional or non-relational databases. In such a case, the inferred Store facade exposed to the handlers may provide multiple targets for persisting the data. Database.transact should handle potential edge-cases when writes to either of the data sinks fail. In order to implement a custom data sink adapter, it suffices to implement the Database interface: export type Database < S > = FinalDatabase < S > | HotDatabase < S > export interface FinalDatabase < S > { supportsHotBlocks ? : false connect ( ) : Promise < HashAndHeight > transact ( info : FinalTxInfo , cb : ( store : S ) => Promise < void > ) : Promise < void > } export interface HotDatabase < S > { supportsHotBlocks : true connect ( ) : Promise < HotDatabaseState > transact ( info : FinalTxInfo , cb : ( store : S ) => Promise < void > ) : Promise < void > transactHot ( info : HotTxInfo , cb : ( store : S , block : HashAndHeight ) => Promise < void > ) : Promise < void > } Consult this file for details. The interface only defines how the processor advances with the indexing and connects to the data sink. The following is left as implementation details: persisting the indexing status opening and rolling back the transaction (if applicable) Consult these implementations for details: (Transactional) Postgres-based TypeormDatabase (Non-transactional) File-based database Edit this page Previous Persisting data Next Saving to PostgreSQL Custom Database
Step 1: Transfer events | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Step 1: Transfer events Step 2: Owners & tokens Step 3: External data Step 4: Optimization Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Indexing BAYC Step 1: Transfer events On this page Step 1: Indexing Transfer events In this step-by-step tutorial we will build a squid that gets data about Bored Ape Yacht Club NFTs, their transfers and owners from the Ethereum blockchain , indexes the NFT metadata from IPFS and regular HTTP URLs, stores all the data in a database and serves it over a GraphQL API. Here we do the first step: build a squid that indexes only the Transfer events emitted by the BAYC token contract . Pre-requisites: Node.js, Squid CLI , Docker. Starting with a template ​ Begin by retrieving the evm template and installing the dependencies: sqd init my-bayc-squid -t evm cd my-bayc-squid npm i The resulting code can be found at this commit . Interfacing with the contract ABI ​ First, we inspect which data is available for indexing. For EVM contracts, the metadata descrbing the shape of the smart contract logs, transactions and contract state methods is distributed as an Application Binary Interface (ABI) JSON file. For many popular contracts ABI files are published on Etherscan (as in the case of the BAYC NFT contract). SQD provides a tool for retrieving contract ABIs from Etherscan-like APIs and generating the boilerplate for retrieving and decoding the data. For the contract of interest, this can be done with npx squid-evm-typegen src/abi 0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d #bayc Here, src/abi is the destination folder and the bayc suffix sets the base name for the generated file. Checking out the generated src/abi/bayc.ts file we see all events and contract functions listed in the ABI. Among them there is the Transfer event: export const events = { ... Transfer : new LogEvent < ( [ from : string , to : string , tokenId : bigint ] & { from : string , to : string , tokenId : bigint } ) > ( abi , '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' ) , } Reading about elsewhere we learn that it is emitted every time an NFT changes hand and that its logs contain the addresses of both involved parties, as well the unique ID of the token. This is the data we need, so we proceed to configure our squid to retrieve it. Configuring the data filters ​ A "squid processor" is the Node.js process and the object that powers it . Together they are responsible for retrieving filtered blockchain data from a specialized data lake ( SQD Network ), transforming it and saving the result to a destination of choice. To configure the processor (object) to retrieve the Transfer events of the BAYC token contract, we initialize it like this: src/processor.ts // ... import * as bayc from './abi/bayc' export const CONTRACT_ADDRESS = '0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d' export const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( { url : '<my_eth_rpc_url>' , rateLimit : 10 } ) . setFinalityConfirmation ( 75 ) . setBlockRange ( { from : 12_287_507 , } ) . addLog ( { address : [ CONTRACT_ADDRESS ] , topic0 : [ bayc . events . Transfer . topic ] } ) . setFields ( { log : { transactionHash : true } } ) // ... Here, 'https://v2.archive.subsquid.io/network/ethereum-mainnet' is the URL the public SQD Network gateway for Ethereum mainnet. Check out the exhaustive SQD Network gateways list . '<eth_rpc_endpoint_url>' is a public RPC endpoint we chose to use in this example. When an endpoint is available, the processor will begin ingesting data from it once it reaches the highest block available within SQD Network. Please use a private endpoint or SQD Cloud's RPC addon in production. setFinalityConfirmation(75) call instructs the processor to consider blocks final after 75 confirmations when ingesting data from an RPC endpoint. 12_287_507 is the block at which the BAYC token contract was deployed. Can be found on the contract's Etherscan page . The argument of addLog() is a set of filters that tells the processor to retrieve all event logs emitted by the BAYC contract with topic0 matching the hash of the full signature of the Transfer event. The hash is taken from the previously generated Typescript ABI. The argument of setFields() specifies the exact data we need on every event to be retrieved. In addition to the data that is provided by default we are requesting hashes of parent transactions for all event logs. See configuration for more options. Decoding the event data ​ The other part of processor configuration is the callback function used to process batches of the filtered data, the batch handler . It is typically defined at the processor.run() call at src/main.ts , like this: processor . run ( db , async ( ctx ) => { // data transformation and persistence code here } ) Here, db is a Database implementation specific to the target data sink. We want to store the data in a PostgreSQL database and present with a GraphQL API, so we provide a TypeormDatabase object here. ctx is a batch context object that exposes a batch of data retrieved from SQD Network or a RPC endpoint (at ctx.blocks ) and any data persistence facilities derived from db (at ctx.store ). Batch handler is where the raw on-chain data is decoded, transformed and persisted. This is the part we'll be concerned with for the rest of the tutorial. We begin by defining a batch handler decoding the Transfer event: src/main.ts import { TypeormDatabase } from '@subsquid/typeorm-store' import { processor , CONTRACT_ADDRESS } from './processor' import * as bayc from './abi/bayc' processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let block of ctx . blocks ) { for ( let log of block . logs ) { if ( log . address === CONTRACT_ADDRESS && log . topics [ 0 ] === bayc . events . Transfer . topic ) { let { from , to , tokenId } = bayc . events . Transfer . decode ( log ) ctx . log . info ( ` Parsed a Transfer of token ${ tokenId } from ${ from } to ${ to } ` ) } } } } ) This goes through all the log items in the batch, verifies that they indeed are Transfer events emitted by the BAYC contract and decodes the data of each log item, then logs the results to the terminal. The verification step is required because the processor does not guarantee that it won't supply any extra data, only that it will supply the data matching the filters. The decoding is done with the bayc.events.Transfer.decode() function from the Typescript ABI we previously generated. At this point the squid is ready for its first test run. Execute docker compose up -d npm run build npx squid-typeorm-migration apply node -r dotenv/config lib/main.js and you should see lots of lines like these in the output: 03:56:02 INFO  sqd:processor Parsed a Transfer of token 6325 from 0x0000000000000000000000000000000000000000 to 0xb136c6A1Eb83d0b4B8e4574F28e622A57F8EF01A 03:56:02 INFO  sqd:processor Parsed a Transfer of token 6326 from 0x0000000000000000000000000000000000000000 to 0xb136c6A1Eb83d0b4B8e4574F28e622A57F8EF01A 03:56:02 INFO  sqd:processor Parsed a Transfer of token 4407 from 0x082C99d47E020a00C95460D50a83338d509A0e3a to 0x5cf0E6da6Ec2bd7165edcD52D3d31f2528dCf007 The full code can be found at this commit . Extending and persisting the data ​ TypeormDatabase requires us to define a TypeORM data model to actually send the data to the database. In SQD, the same data model is also used by the GraphQL server to generate the API schema. To avoid any potential discrepancies, processor and GraphQL server rely on a shared data model description defined at schema.graphql in a GraphQL schema dialect fully documented here . TypeORM code is generated from schema.graphql with the squid-typeorm-codegen tool and must be regenerated every time the schema is changed. This is usually accompanied by regenerating the database migrations and recreating the database itself. The migrations are applied before every run of the processor, ensuring that whenever any TypeORM code within the processor attempts to access the database, the database is in a state that allows it to succeed. The main unit of data in schema.graphql is entity . These map onto TypeORM entites that in turn map onto database tables. We define one for Transfer events by replacing the file contents with schema.graphql type Transfer @entity { id : ID ! tokenId : BigInt ! @index from : String ! @index to : String ! @index timestamp : DateTime ! blockNumber : Int ! txHash : String ! @index } Here, The id is a required field and the ID type is an alias for String . Entity IDs must be unique. @index decorators tell the codegen tool that the corresponding database columns should be indexed. Extra fields timestamp and blockNumber are added to make the resulting GraphQL API more convenient. We will fill them using the block metadata available in ctx . Once we're done editing the schema, we regenerate the TypeORM code, recreate the database and regenerate the migrations: npx squid-typeorm-codegen npm run build docker compose down docker compose up -d rm -r db/migrations npx squid-typeorm-migration generate The generated code is in src/model . We can now import a Transfer entity class from there and use it to perform various operations on the corresponding database table. Let us rewrite our batch handler to save the parsed Transfer events data to the database: src/main.ts import { TypeormDatabase } from '@subsquid/typeorm-store' import { processor , CONTRACT_ADDRESS } from './processor' import * as bayc from './abi/bayc' import { Transfer } from './model' processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { let transfers : Transfer [ ] = [ ] for ( let block of ctx . blocks ) { for ( let log of block . logs ) { if ( log . address === CONTRACT_ADDRESS && log . topics [ 0 ] === bayc . events . Transfer . topic ) { let { from , to , tokenId } = bayc . events . Transfer . decode ( log ) transfers . push ( new Transfer ( { id : log . id , tokenId , from , to , timestamp : new Date ( block . header . timestamp ) , blockNumber : block . header . height , txHash : log . transactionHash , } ) ) } } } await ctx . store . insert ( transfers ) } ) Note a few things here: A unique event log ID is available at log.id - no need to generate your own! tokenId returned from the decoder is an ethers.BigNumber , so it has to be explicitly converted to number . The conversion is valid only because we know that BAYC NFT IDs run from 0 to 9999; in most cases we would use BigInt for the entity field type and convert with tokenId.toBigInt() . block.header contains block metadata that we use to fill the extra fields. Accumulating the Transfer entity instances before using ctx.store.insert() on the whole array of them in the end allows us to get away with just one database transaction per batch. This is crucial for achieving a good syncing performance . At this point we have a squid that indexes the data on BAYC token transfers and is capable of serving it over a GraphQL API. Full code is available at this commit . Test it by running npm run build npx squid-typeorm-migration apply node -r dotenv/config lib/main.js then npx squid-graphql-server in a separate terminal. If all is well, a GraphiQL playground should become available at localhost:4350/graphql : Edit this page Previous Indexing BAYC Next Step 2: Owners & tokens Starting with a template Interfacing with the contract ABI Configuring the data filters Decoding the event data Extending and persisting the data
Solana API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK Network API Solana API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing Network API Solana API On this page Solana SQD Network API warning The Solana API of SQD Network is currently in beta. Breaking changes may be introduced in the future releases. SQD Network API distributes the requests over a ( potentially decentralized ) network of workers . The main gateway URL points at a router that provides URLs of workers that do the heavy lifting. Each worker has its own range of blocks on each dataset it serves. Suppose you want to retrieve an output of some query on a block range starting at firstBlock (can be the genesis block) and ending at the highest available block. Proceed as follows: Retrieve the dataset height from the router with GET /height and make sure it's above firstBlock . Save the value of firstBlock to some variable, say currentBlock . Query the router for an URL of a worker that has the data for currentBlock with GET /${currentBlock}/worker . Retrieve the data from the worker by posting the query ( POST / ), setting the "fromBlock" query field to ${currentBlock} . Parse the retrieved data to get a batch of query data plus the height of the last block available from the current worker. Take the header.number field of the last element of the retrieved JSON array - it is the height you want. Even if your query returns no data, you'll still get the block data for the last block in the range, so this procedure is safe. Set currentBlock to the height from the previous step plus one . Repeat steps 3-6 until all the required data is retrieved. The main URL of the Solana gateway is https://v2.archive.subsquid.io/network/solana-mainnet We also serve a dataset for Eclipse Testnet at https://v2.archive.subsquid.io/network/eclipse-testnet Implementation examples: Manually with cURL Suppose we want data on all successful Solana instructions starting block 241974500. We begin by finding the main URL for the Solana Mainnet dataset. Then we have to: Verify that the dataset has reached the required height: curl https://v2.archive.subsquid.io/network/solana-mainnet/height Output 243004249 Remember that our current height is 241974500. Get a worker URL for the current height: curl https://v2.archive.subsquid.io/network/solana-mainnet/241974500/worker Output https://lm02.sqd-archive.net/worker/query/czM6Ly9zb2xhbmEtbWFpbm5ldC0w Retrieve the data from the current worker curl https://rb04.sqd-archive.net/worker/query/czM6Ly9zb2xhbmEtbWFpbm5ldC1kZW1v \ -X 'POST' -H 'content-type: application/json' -H 'accept: application/json' \ -d '{ "type": "solana", "fromBlock":241974500, "toBlock": 243004249, "fields":{"instruction":{"programId":true, "data": true}}, "instructions":[ {"isCommitted": true} ] }' | jq Output: [ { "header" : { "number" : 241974500 , "hash" : "3pnS5TEVvbG7gnhnfNWkppm2ufgxhqB8z6jsdjNPhgQi" } , "instructions" : [ { "transactionIndex" : 24 , "instructionAddress" : [ 0 ] , "programId" : "ComputeBudget111111111111111111111111111111" , "data" : "GtQyqR" } , ... { "transactionIndex" : 1577 , "instructionAddress" : [ 2 ] , "programId" : "mineJKQoyEbSiyjooEVMGSbHMaDdv7Cnf8rhkKLgyVb" , "data" : "SSX8YzgXGaUDonrMFeCc1SJXnE2PpHo6Ak41asTZA4MRLaKTLyauDdy" } ] } , ... { "header" : { "number" : 241974599 , "hash" : "BUeG7rcpfTd8oo5vjnVfgdWTm72fycz7YoozqD1y13XQ" } , "instructions" : [ ... ] } ] Parse the retrieved data: Grab the network data you requested from the list items with non-empty data fields ( instructions , transactions , logs , balances , tokenBalances , rewards ). Observe that we received the data up to and including block 241974599. Note: the last block of the batch will be returned even if it has no matching data. To get the rest of the data, update the current height to 241974600 and go to step 3. Note how the worker URL you're getting while repeating step 3 occasionally points to a different host than before. This is how data storage and reads are distributed across the SQD Network. Repeat steps 3 through 6 until the dataset height of 243004249 reached. In Python def get_text ( url : str ) - > str : res = requests . get ( url ) res . raise_for_status ( ) return res . text def dump ( gateway_url : str , query : Query , first_block : int , last_block : int ) - > None : assert 0 <= first_block <= last_block query = dict ( query ) # copy query to mess with it later dataset_height = int ( get_text ( f' { gateway_url } /height' ) ) next_block = first_block last_block = min ( last_block , dataset_height ) while next_block <= last_block : worker_url = get_text ( f' { gateway_url } / { next_block } /worker' ) query [ 'fromBlock' ] = next_block query [ 'toBlock' ] = last_block res = requests . post ( worker_url , json = query ) res . raise_for_status ( ) blocks = res . json ( ) last_processed_block = blocks [ - 1 ] [ 'header' ] [ 'number' ] next_block = last_processed_block + 1 for block in blocks : print ( json . dumps ( block ) ) Full code here . Router API ​ GET /height (get height of the dataset) Example response: 243004249 . GET ${firstBlock}/worker (get a suitable worker URL) The returned worker is capable of processing POST / requests in which the "fromBlock" field is equal to ${firstBlock} . Example response: https://v2.archive.subsquid.io/worker/1/query/czM6Ly9ldGhlcmV1bS1tYWlubmV0 . Worker API ​ POST / (query Solana data) Query Fields ​ type : "solana" fromBlock : Block number to start from (inclusive). toBlock : (optional) Block number to end on (inclusive). If this is not given, the query will go on for a fixed amount of time or until it reaches the height of the dataset. includeAllBlocks : (optional) If true, the Network will include blocks that contain no data selected by data requests into its response. fields : (optional) A selector of data fields to retrieve. Common for all data items. instructions : (optional) A list of intructions requests . An empty list requests no data. transactions : (optional) A list of transaction requests . An empty list requests no data. logs : (optional) A list of log requests . An empty list requests no data. balances : (optional) A list of balances requests . An empty list requests no data. tokenBalances : (optional) A list of token balances requests . An empty list requests no data. rewards : (optional) A list of rewards requests . An empty list requests no data. The response is a JSON array of per-block data items that covers a block range starting from fromBlock . The last block of the range is determined by the worker. You can find it by looking at the header.number field of the last element in the response array. The first and the last block in the range are returned even if all data requests return no data for the range. In most cases the returned range will not contain all the range requested by the user (i.e. the last block of the range will not be toBlock ). To continue, retrieve a new worker URL for blocks starting at the end of the current range plus one block and repeat the query with an updated value of fromBlock . Example Request ​ { "type" : "solana" , "fromBlock" : 241974500 , "toBlock" : 243004249 , "fields" : { "instruction" : { "programId" : true , "data" : true } } , "instructions" : [ { "isCommitted" : true } ] } Example Response ​ Note: the first and the last block in the range are included even if they have no matching data. [ { "header" : { "number" : 241974500 , "hash" : "3pnS5TEVvbG7gnhnfNWkppm2ufgxhqB8z6jsdjNPhgQi" } , "instructions" : [ { "transactionIndex" : 24 , "instructionAddress" : [ 0 ] , "programId" : "ComputeBudget111111111111111111111111111111" , "data" : "GtQyqR" } , ... { "transactionIndex" : 1577 , "instructionAddress" : [ 2 ] , "programId" : "mineJKQoyEbSiyjooEVMGSbHMaDdv7Cnf8rhkKLgyVb" , "data" : "SSX8YzgXGaUDonrMFeCc1SJXnE2PpHo6Ak41asTZA4MRLaKTLyauDdy" } ] } , ... { "header" : { "number" : 241974599 , "hash" : "BUeG7rcpfTd8oo5vjnVfgdWTm72fycz7YoozqD1y13XQ" } , "instructions" : [ ... ] } ] Data requests ​ Instructions ​ { programId ? : string [ ] d1 ? : string [ ] d2 ? : string [ ] d3 ? : string [ ] d4 ? : string [ ] d8 ? : string [ ] a0 ? : string [ ] a1 ? : string [ ] a2 ? : string [ ] a3 ? : string [ ] a4 ? : string [ ] a5 ? : string [ ] a6 ? : string [ ] a7 ? : string [ ] a8 ? : string [ ] a9 ? : string [ ] isCommitted ? : boolean transaction ? : boolean transactionTokenBalances ? : boolean logs ? : boolean innerInstructions ? : boolean } An instruction will be included in the response if it matches all the requests. A request with an empty array (e.g. { a4: [] } ) matches no instructions; omit all requests/pass an empty object to match all instructions. See addInstruction() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Transactions ​ { feePayer ? : string [ ] instructions ? : boolean logs ? : boolean } A transaction will be included in the response if it matches all the requests. A request with an empty array (e.g. { feePayer: [] } ) matches no transactions; omit all requests/pass an empty object to match all transactions. See addTransaction() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Log messages ​ { programId ? : string [ ] kind ? : ( 'log' | 'data' | 'other' ) [ ] transaction ? : boolean instruction ? : boolean } A log message will be included in the response if it matches all the requests. A request with an empty array (e.g. { kind: [] } ) matches no log messages; omit all requests/pass an empty object to match all log messages. See addLog() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Balances ​ { account ? : string [ ] transaction ? : boolean transactionInstructions ? : boolean } A balance update message will be included in the response if it matches all the requests. A request with an empty array (e.g. { account: [] } ) matches no balance update messages; omit all requests/pass an empty object to match all balance update messages. See addBalance() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Token Balances ​ { account ? : string [ ] preProgramId ? : string [ ] postProgramId ? : string [ ] preMint ? : string [ ] postMint ? : string [ ] preOwner ? : string [ ] postOwner ? : string [ ] transaction ? : boolean transactionInstructions ? : boolean } A token balance update message will be included in the response if it matches all the requests. A request with an empty array (e.g. { preProgramId: [] } ) matches no token balance update messages; omit all requests/pass an empty object to match all token balance update messages. See addTokenBalance() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Rewards ​ { pubkey ? : string [ ] } A reward message will be included in the response if it matches all the requests. A request with an empty array (e.g. { pubkey: [] } ) matches no reward messages; omit all requests/pass an empty object to match all reward messages. See addReward() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Data fields selector ​ A JSON selector of fields for the returned data items. Documented in the Field selectors section. Edit this page Previous Network API Next Fuel indexing Router API Worker API Data requests Instructions Transactions Log messages Balances Token Balances Rewards Data fields selector
Substrate types bundles | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific ink! contracts support Frontier EVM support Gear support Substrate data sourcing Substrate types bundles Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Substrate-specific Substrate types bundles On this page Substrate types bundles Where do I get a types bundle for my chain? â€‹ Types bundle is only needed for pre-Metadata v14 blocks and only if SQD does not offer a built-in support for the chain in question. Most chains publish their type bundles as an npm package (for example: Edgeware ). One of the best places to check for the latest version is the polkadot-js/app and polkadot-js/api repositories. info Note: the type bundle format for typegen is slightly different from OverrideBundleDefinition of polkadot.js . The structure is as follows, all the fields are optional. { types : { } , // top-level type definitions, as `.types` option of `ApiPromise` typesAlias : { } , // top-level type aliases, as `.typesAlias` option of `ApiPromise` versions : [ // spec version specific overrides, same as `OverrideBundleDefinition.types` of `polkadot.js` { minmax : [ 0 , 1010 ] // spec range types : { } , // type overrides for the spec range typesAlias : { } , // type alias overrides for the spec range } ] } Edit this page Previous Substrate data sourcing Next Tools Where do I get a types bundle for my chain?
Indexing Hyperliquid with SQD | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing Hyperliquid with SQD On this page Indexing Hyperliquid with SQD Overview â€‹ SQD is a decentralized hyper-scalable data platform optimized for providing efficient, permissionless access to large volumes of data. SQD offers a powerful toolkit for creating custom data extraction and processing pipelines from over 200 networks, achieving an indexing speed of up to 150k blocks per second. Supported Networks â€‹ HyperEVM Testnet HyperEVM Mainnet HyperCore L1 Mainnet (coming soon) See the full, always-up-to-date network list: docs.sqd.ai/subsquid-network/reference/networks Quickstart: Drip.trade Events on HyperEVM â€‹ This example shows how to index the NFT marketplace Drip.trade on Hyperliquid using SQD. It captures: ItemSold events (direct NFT sales) BidAccepted events (successful bids) Prerequisites â€‹ Node.js Docker Basic terminal skills ðŸ§ª Setup & Run â€‹ # 0. Install the SQD CLI tool globally npm i -g @subsquid/cli # 1. Initialize the Drip.trade example project sqd init hyperliquid-drip-trade -t https://github.com/subsquid-labs/hyperliquid-drip-trade-example cd hyperliquid-drip-trade # 2. Install Node.js dependencies npm ci # 3. Set up environment variables mv .env.example .env # 4. Start a Postgres container sqd up # 5. Start the processor to fetch and transform data sqd process # 6. In a new terminal, launch the GraphQL server sqd serve Explore the data in the GraphQL playground at: http://localhost:4350/graphql This example shows how to build a custom on-chain indexer with SQD. It connects to the HyperEVM network starting from block #377,808, where the Drip.trade contract was deployed. It pulls data using: Archive endpoint: https://v2.archive.subsquid.io/network/hyperliquid-mainnet RPC endpoint: https://rpc.hyperliquid.xyz/evm You can find the full Quickstart example on GitHub: https://github.com/subsquid-labs/hyperliquid-drip-trade-example Learn more â€‹ To learn more about our SDK, visit: docs.sqd.ai/sdk For information about hosting your indexer using SQD Cloud, visit: docs.sqd.ai/cloud To see more examples, visit: docs.sqd.ai/sdk/examples Getting support â€‹ Need help or have questions? Join our dedicated Telegram support channel for SQD developers: link . Edit this page Overview Supported Networks Quickstart: Drip.trade Events on HyperEVM Prerequisites ðŸ§ª Setup & Run Learn more Getting support
SQD Cloud reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference SQD Cloud reference documentation 🗃️ Processors 3 items 🗃️ Data sinks 3 items 📄️ Logger Native logger of Squid SDK 🗃️ Schema file 6 items 🗃️ OpenReader 3 items 📄️ The frontier package Working with EVM running on Substrate Previous Case studies Next Processors
Field selection | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Block data for Tron General settings Transactions Logs Internal transactions Field selection Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing TronBatchProcessor Field selection On this page Field selection setFields(options) ​ Set the fields to be retrieved for data items of each supported type. The options object has the following structure: { block ? : // field selector for block headers transaction ? : // field selector for transactions log ? : // field selector for logs internalTransaction ? : // field selector for internal transactions } Every field selector is a collection of boolean fields that map to the fields of data items within the batch context iterables . Defining a field of a field selector of a given type and setting it to true will cause the processor to populate the corresponding field of all data items of that type. Here is a definition of a processor that requests hash and fee fields for transactions and the data field for logs: const processor = new TronBatchProcessor ( ) . setFields ( { transaction : { hash : true , fee : true , } , logs : { data : true , } , } ) ; Same fields will be available for all data items of any given type, including the items accessed via nested references. Suppose we used the processor defined above to subscribe to some transactions as well as some logs, and for each log we requested its parent transaction: processor . addTransaction ( { where : { // some transaction data requests } } ) . addLog ( { where : { // some log data requests } , include : { transaction : true } } ) The hash and fee fields will be available both within the data items of the transactions iterable of block data and within the transaction items populating the .transaction field of log data items: processor . run ( database , async ( ctx ) => { for ( let block of blocks ) { for ( let txn of block . transactions ) { let fee = txn . fee ; // OK } for ( let log of block . logs ) { if ( /* log matches the data request */ ) { let logTxFee = log . transaction ?. fee ; // also OK! } } } } ) Some data fields, like hash for transactions, are enabled by default but can be disabled by setting a field of a field selector to false . For example, this code will not compile: const processor = new TronBatchProcessor ( ) . setFields ( { transaction : { hash : false } } ) processor . run ( database , async ( ctx ) => { for ( let block of ctx . blocks ) { for ( let txn of block . transactions ) { let hash = txn . hash ; // ERROR: no such field } } } ) ; Disabling unused fields will improve sync performance, as the fields' data will not be fetched from the SQD Network gateway. Data item types and field selectors ​ tip Most IDEs support smart suggestions to show the possible field selectors. For VS Code, press Ctrl+Space . Here we describe the data item types as functions of the field selectors. On Tron, each data item type field maps to the eponymous field of its corresponding field selector. Item fields are divided into three categories: Fields that are always added regardless of the setFields() call. Fields that are enabled by default and can be disabled by setFields() . E.g. a hash field will be fetched for transactions by default, but can be disabled by setting hash: false within the transaction field selector. Fields that can be requested by setFields() . Transaction ​ Fields of Transaction data items may be requested by the eponymous fields of the field selector. Here's a detailed description of possible Transaction fields: Transaction { // independent of field selectors transactionIndex : number // can be disabled with field selectors hash : string type : string // can be enabled with field selectors ret ? : TransactionResult [ ] signature ? : string [ ] parameter : any permissionId ? : number refBlockBytes ? : string refBlockHash ? : string feeLimit ? : bigint expiration ? : number timestamp ? : bigint rawDataHex : string fee ? : bigint contractResult ? : string contractAddress ? : string resMessage ? : string withdrawAmount ? : bigint unfreezeAmount ? : bigint withdrawExpireAmount ? : bigint cancelUnfreezeV2Amount ? : Record < string , bigint > result ? : string energyFee ? : bigint energyUsage ? : bigint energyUsageTotal ? : bigint netUsage ? : bigint netFee ? : bigint originEnergyUsage ? : bigint energyPenaltyTotal ? : bigint } Here, TransactionResult is defined as follows: { contractRet ? : string } Request the fields with eponymous field request flags. Log ​ Fields of Log data items may be requested by the eponymous fields of the field selector. Here's a detailed description of possible Log fields: Log { // independent of field selectors transactionIndex : number logIndex : number // can be disabled with field selectors address : string data ? : string topics ? : string [ ] } Request the fields with eponymous field request flags. Internal transaction ​ Fields of InternalTransaction data items may be requested by the eponymous fields of the field selector. Here's a detailed description of possible InternalTransaction fields: InternalTransaction { // independent of field selectors transactionIndex : number internalTransactionIndex : number // can be disabled with field selectors callerAddress : string transferToAddress ? : string // can be enabled with field selectors hash : string callValueInfo : CallValueInfo [ ] note : string rejected ? : boolean extra ? : string } Here, CallValueInfo is defined as follows: { callValue ? : bigint tokenId ? : string } Request the fields with eponymous field request flags. Block header ​ BlockHeader data items may have the following fields: BlockHeader { // independent of field selectors height : number hash : string parentHash : string // can be disabled with field selectors timestamp : number // can be enabled with field selectors txTrieRoot : string version ? : number witnessAddress : string witnessSignature ? : string } Request the fields with eponymous field request flags. A complete example ​ TBA Edit this page Previous Internal transactions Next Cheatsheet Data item types and field selectors Transaction Log Internal transaction Block header A complete example
Case studies | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Case studies Case studies Follow the links from the SQD Medium blog for deep dives into the development process of larger projects built from scratch. They use the FireSquid version of the framework, so much of the involved code is outdated. Still, the general approaches they illustrate endure. DeFi Dashboard . A React app for a real-time dashboard showing key statistics of the Moonwell lending protocol Rave name service indexing on Fantom . Indexing Uniswap data into parquets . A step-by-step guide on how to extract, decode and analyze Uniswap trading data with SQD, Pandas and a Python notebook. Analyzing Lens Protocol . An end-to-end tutorial on how to extract and analyze the social data of the Lens protocol. Includes an extra step on how to build a UI for the dashboards. Edit this page Previous Processor in action Next Reference
Interfaces | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file Schema file and codegen Entities Indexes and constraints Entity relations Unions and typed JSON Interfaces OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Schema file Interfaces On this page Interfaces The schema file supports GraphQL Interfaces for modelling complex types sharing common traits. Interfaces are annotated with @query at the type level and do not affect the database schema, only enriching the GraphQL API queries with inline fragments . Currently, only OpenReader supports GraphQL interfaces defined in the schema file. Examples â€‹ interface MyEntity @query { id : ID ! name : String ref : Ref } type Ref @entity { id : ID ! name : String foo : Foo ! @unique bar : Bar ! @unique } type Foo implements MyEntity @entity { id : ID ! name : String ref : Ref @derivedFrom ( field : "foo" ) foo : Int } type Bar implements MyEntity @entity { id : ID ! name : String ref : Ref @derivedFrom ( field : "bar" ) bar : Int } type Baz implements MyEntity @entity { id : ID ! name : String ref : Ref baz : Int } The MyEntity interface above enables myEntities and myEntitiesConnection GraphQL API queries with inline fragments and the _type , __typename meta fields : query { myEntities ( orderBy : [ _type_DESC , id_ASC ] ) { id name ref { id name } __typename ... on Foo { foo } ... on Bar { bar } ... on Baz { baz } } } Edit this page Previous Unions and typed JSON Next OpenReader Examples
Overview | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Overview On this page Overview SQD Network is a distributed query engine and data lake that the SQD ecosystem is built around. Currently it serves historical blockchain data for 190+ networks. Compared to data access using a conventional chain node RPC, SQD Network allows one to access data at near zero cost, in a more granular fashion and from multiple blocks at once, thanks to its rich streaming and filtering capabilities. There are currently two independent instances of SQD Network: A decentralized permissionless network. A permissioned network ran by Subsquid Labs GmbH. It's been used by 400+ indexers in production. List of supported networks . Both variants are freely accessible to public using the same API . If you are a network developer and would like to see your chain supported by SQD, please fill a form or contact us in SquidDevs Telegram chat . Permissionless public network ​ The decentralized version of SQD Network is now in the mainnet phase. It serves data for almost all networks supported by the private cluster , with the (temporary) exception of Solana. The most recent version of the data access node, SQD portal, fetches data chunks from multiple worker nodes in parallel and exposes a streaming API. It is now in a closed beta. You can participate in the public network by running a worker or reward good workers by delegating SQD tokens and sharing in profits. Open private network ​ SQD's private data lake (formerly known as Archives) is a production-ready fork of early SQD Network that runs on SQD infrastructure. It provides a stand-in for the permissionless SQD Network until it matures; its API shares many similarities with the SQD portal API. The gateway of this private network is public and free to query. URLs of gateways of supported networks are available via sqd gateways list and from the exhaustive public gateways list . EVM Substrate List all supported networks: sqd gateways list -t evm Usage in squids' EvmBatchProcessor configuration : const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) See EVM gateways . List all supported networks: sqd gateways list -t substrate Usage in squids' SubstrateBatchProcessor configuration : const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/phala' ) See Substrate gateways . Edit this page Previous SQD Network Next Whitepaper Permissionless public network Open private network
Definition of the database schema, entity classes and the core GraphQL API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file Schema file and codegen Entities Indexes and constraints Entity relations Unions and typed JSON Interfaces OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Schema file Definition of the database schema, entity classes and the core GraphQL API 📄️ Schema file and codegen Intro to the schema file and the codegen tool 📄️ Entities Define high-level API entities 📄️ Indexes and constraints Annotate indexed fields for faster queries 📄️ Entity relations Define entity relations 📄️ Unions and typed JSON Union and JSON types 📄️ Interfaces Queriable interfaces Previous Logger Next Schema file and codegen
Hasura configuration tool | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools Hasura configuration tool On this page Hasura configuration tool info For info on migrating to @subsquid/hasura-configuration>=2.0.0 see this page . @subsquid/hasura-configuration is a tool for managing Hasura configuration in PostgreSQL-powered squids . Install it with npm i @subsquid/hasura-configuration Make sure that the following environment variables are set: HASURA_GRAPHQL_ENDPOINT for Hasura URL (defaults to http://localhost:8080 ). HASURA_GRAPHQL_ADMIN_SECRET for admin access (only required to use squid-hasura-configuration apply ). If your Hasura instance(s) use a role other than public to serve the anonymous part of your API, also set HASURA_GRAPHQL_UNAUTHORIZED_ROLE . Generating the initial configuration ​ The tool uses your squid's TypeORM models as input when generating the initial configuration. Make sure they are up to date. Here's how Finalize any edits to schema.graphql Update the TypeScript code of your models with npx squid-typeorm-codegen Compile your models with npm run build Regenerate your DB migrations with a clean database to make sure they match your updated schema. docker compose down docker compose up -d rm -r db npx squid-typeorm-migration generate You can turn off your database after doing that - Hasura configuration tool does not use it to make its initial configuration When done, run npx squid-hasura-configuration regenerate The generated configuration will be available at hasura_metadata.json . It enables: tracking all tables that correspond to schema entities ; SELECT permissions for the public (or $HASURA_GRAPHQL_UNAUTHORIZED_ROLE if it is defined) role for all columns in these tables; tracking all entity relationships . Applying the configuration ​ Make sure your database is up, your Hasura instance is connected to it and the schema is up to date. If necessary, apply the migrations: npx squid-typeorm-migration apply When done, you can apply the generated config with npx squid-hasura-configuration apply or import it using the Settings > Metadata Actions > Import metadata function of the web GUI. Persisting configuration changes ​ warning Regenerating hasura_metadata.json removes any modifications you might have made via metadata exporting. So, it is advisable that you finalize your schema before you begin any manual API fine-tuning. When running a squid with a dedicated Hasura instance you will notice that squid resetting operations ( docker compose down; docker compose up -d and sqd deploy -r ) restore your Hasura API to its non-configured state. As you develop your API further you may want to persist your changes. squid-hasura-configuration helps with that by being compatible with the Settings > Metadata Actions > Import/Export metadata functions of the web GUI. Any extra configuration you may make via the web GUI or Hasura metadata API can be persisted by exporting the metadata to hasura_metadata.json via the Export metadata function, then applying it to blank Hasura instances with npx squid-hasura-configuration apply Example ​ See this repo . Edit this page Previous Squid generation tools Next Migration guides Generating the initial configuration Applying the configuration Persisting configuration changes Example
Portal beta info | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Portal beta info On this page SQD Portal Open Beta Welcome to the SQD Portal Open Beta! Below you’ll find easy-to-follow instructions on what to do next and what you can expect from this release. What is the SQD Portal? ​ The SQD Portal is a decentralized, streaming-based data retrieval solution designed to replace our centralized gateways. It provides faster, more reliable, and flexible access to blockchain data. SQD Portal can supply both historical and real-time data seamlessly, eliminating the need for an RPC endpoint. Real-time capability is currently only available for Solana, with plans to add it for all EVM and Substate networks soon. Key Features ​ Fully Decentralized : Powered by 1,900+ independent worker nodes. High Replication : Redundant data storage for maximum reliability, capable of querying up to 20 million blocks per second . Improved Performance : A new Rust-based query engine now leverages parallelized queries and incorporates numerous query execution performance optimizations, delivering an overall 10-50x performance boost compared to the centralized gateways. Note: This performance improvement is specific to data retrieval. If your application faces other bottlenecks (like database limitations), you may experience a more moderate gain in indexing speed. Real-time Capability : Portal is now capable of streaming real-time data, including unfinalized blocks. Currently only available on Solana Coming soon for EVM and Substrate networks What You Can Do ​ 1. Explore Soldexer — Our New Brand for Solana ​ If you’re working with Solana, visit soldexer.dev to check out our new client architecture, powered by the updated Portal API. 2. Migrate to the Cloud Portal ​ Cloud users can now switch from centralized gateways to our dedicated Cloud Portal and benefit from enhanced performance and the latest features of our new query engine. Migration guides : For EVM and Substate users For Solana users 3. Set Up a Self-Hosted Portal ​ Take complete control of your data infrastructure by running your own Portal. Setup Guide : Self-Hosting Instructions Requirements Minimum 10,000 SQD tokens. A working Docker installation. Some Arbitrum ETH for gas. 4. Explore the Public Portal ​ Public Portal allows access to nearly all the datasets that were available from the centralized gateways. Use it for development and tests. Browse EVM datasets at portal-ui.sqd.dev . Solana (and compatible SVM networks) and Substrate datasets will be available soon. If you have any questions or feedback for us, please reach out to us on Telegram. We have created a special group for Portal Beta participants: link . — The SQD Team Edit this page What is the SQD Portal? What You Can Do 1. Explore Soldexer — Our New Brand for Solana 2. Migrate to the Cloud Portal 3. Set Up a Self-Hosted Portal 4. Explore the Public Portal
Subscriptions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Caching Custom API extensions Subscriptions DoS protection Access control Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Configuration Subscriptions On this page Subscriptions danger RAM usage of subscriptions scales poorly under high load, making the feature unsuitable for most production uses. There are currently no plans to fix this issue. OpenReader supports GraphQL subscriptions via live queries. To use these, a client opens a websocket connection to the server and sends a subscription query there. The query body is then repeatedly executed (every 5 seconds by default) and the results are sent to the client whenever they change. To enable subscriptions, add the additional --subscriptions flag to the squid-graphql-server startup command. The poll interval is configured with the --subscription-poll-interval flag. For details and a full list of available options, run npx squid-graphql-server --help For each entity types, the following queries are supported for subscriptions: ${EntityName}ById -- query a single entity ${EntityName}s -- query multiple entities with a where filter
Note that despite being deprecated from the regular query set, ${EntityName}s queries will continue to be available for subscriptions going forward. Local runs ​ To enable subscriptions, add the --subscriptions flag to serve and serve:prod commands at commands.json : commands.json ... "serve" : { "description" : "Start the GraphQL API server" , "cmd" : [ "squid-graphql-server" , "--subscriptions" ] } , "serve:prod" : { "description" : "Start the GraphQL API server in prod" , "cmd" : [ "squid-graphql-server" , "--subscriptions" , "--dumb-cache" , "in-memory" ] } , ... A ws endpoint will be available the usual localhost:<GQL_PORT>/graphql URL. SQD Cloud deployments ​ For SQD Cloud deployments, make sure to use the updated serve:prod command in the deployment manifest : squid.yaml # ... deploy : # other services ... api : cmd : [ "sqd" , "serve:prod" ] The subscription wss endpoint will be available at the canonical API endpoint wss://{org}.subsquid.io/{name}/v/v{version}/graphql . Example ​ Let's take the following simple schema with account and transfer entities: type Account @entity { " Account address " id : ID ! transfersTo : [ Transfer ! ] @derivedFrom ( field : "to" ) transfersFrom : [ Transfer ! ] @derivedFrom ( field : "from" ) } type Transfer @entity { id : ID ! timestamp : DateTime ! @index from : Account ! to : Account ! amount : BigInt ! @index } After modifying commands.json start the GraphQL server with subscriptions with npx squid-graphql-server The following sample script will subscribe to the most recent transfers (by timestamp ). const WebSocket = require ( 'ws' ) const { createClient } = require ( 'graphql-ws' ) ; const port = process . env . GQL_PORT || 4350 const host = process . env . GQL_HOST || 'localhost' const proto = process . env . GQL_PROTO || 'ws' const client = createClient ( { webSocketImpl : WebSocket , url : ` ${ proto } :// ${ host } : ${ port } /graphql ` , } ) ; client . subscribe ( { query : ` subscription { transfers(limit: 5, orderBy: timestamp_DESC) { amount blockNumber from { id } to { id } } } ` , } , { next : ( data ) => { console . log ( ` New transfers: ${ JSON . stringify ( data ) } ` ) ; } , error : ( error ) => { console . error ( 'error' , error ) ; } , complete : ( ) => { console . log ( 'done!' ) ; } , } ) ; Edit this page Previous Custom API extensions Next DoS protection Local runs SQD Cloud deployments Example
Token balances | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Token balances On this page Token balances addTokenBalance(options) â€‹ This allows for tracking token balances. options has the following structure: { // data requests where ? : { account ? : string [ ] preProgramId ? : string [ ] postProgramId ? : string [ ] preMint ? : string [ ] postMint ? : string [ ] preOwner ? : string [ ] postOwner ? : string [ ] } // related data retrieval include ? : { transaction ? : boolean transactionInstructions ? : boolean } range ? : { from : number to ? : number } } The data requests here are: account : the set of accounts to track. Leave undefined to subscribe to balance updates of all accounts across the network. preProgramId : pubkeys of the Token program that owns the account. postProgramId : pubkeys of the Token program that owns the account. preMint : pubkeys of the token's mint prior to execution. postMint : pubkeys of the token's mint after execution. preOwner : pubkeys of token balance's owner after execution. postOwner : pubkeys of token balance's owner after execution. All account/pubkeys should be base58-encoded strings. Related data retrieval flags: transaction = true : retrieve the transaction that gave rise to the balance update transactionInstructions = true : retrieve all instructions executed by the parent transaction The related data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/solana-objects on the block data to populate the convenience reference fields like balance.transaction . Selection of the exact fields to be retrieved for each token balance item and the related data is done with the setFields() method documented on the Field selection page. Edit this page Previous Balances Next Rewards
EVM API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Public gateways EVM API Substrate API Starknet API Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Reference EVM API On this page EVM SQD Network API warning The EVM API of SQD Network is currently in beta. Breaking changes may be introduced in the future releases. SQD Network API distributes the requests over a ( potentially decentralized ) network of workers . The main gateway URL points at a router that provides URLs of workers that do the heavy lifting. Each worker has its own range of blocks on each dataset it serves. Suppose you want to retrieve an output of some query on a block range starting at firstBlock (can be the genesis block) and ending at the highest available block. Proceed as follows: Retrieve the dataset height from the router with GET /height and make sure it's above firstBlock . Save the value of firstBlock to some variable, say currentBlock . Query the router for an URL of a worker that has the data for currentBlock with GET /${currentBlock}/worker . Retrieve the data from the worker by posting the query ( POST / ), setting the "fromBlock" query field to ${currentBlock} . Parse the retrieved data to get a batch of query data plus the height of the last block available from the current worker. Take the header.number field of the last element of the retrieved JSON array - it is the height you want. Even if your query returns no data, you'll still get the block data for the last block in the range, so this procedure is safe. Set currentBlock to the height from the previous step plus one . Repeat steps 3-6 until all the required data is retrieved. URLs of public EVM gateways are available on the Supported networks page . Implementation examples: Manual fetch with cURL Suppose we want data on Ethereum txs to vitalik.eth / 0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045 from block 16_000_000. We begin by finding the main URL for the Ethereum Mainnet gateway on the Supported networks page . Then we have to: Verify that the dataset has reached the required height: curl https://v2.archive.subsquid.io/network/ethereum-mainnet/height Output: 18593441 Remember that your current height is 16000000. Get a worker URL for the current height: curl https://v2.archive.subsquid.io/network/ethereum-mainnet/16000000/worker Output: https://rb05.sqd-archive.net/worker/query/czM6Ly9ldGhlcmV1bS1tYWlubmV0 Retrieve the data available from the current worker curl https://rb05.sqd-archive.net/worker/query/czM6Ly9ldGhlcmV1bS1tYWlubmV0 \ -X 'POST' -H 'content-type: application/json' -H 'accept: application/json' \ -d '{ "fromBlock":16000000, "fields":{"transaction":{"hash":true}}, "transactions":[{"to":["0xd8da6bf26964af9d7eed9e03e53415d37aa96045"]}] }' | jq Note how the address in the transactions data request is lowercased. Output: [ { "header" : { "number" : 16000000 , "hash" : "0x3dc4ef568ae2635db1419c5fec55c4a9322c05302ae527cd40bff380c1d465dd" , "parentHash" : "0x6f377dc6bd1f3e38b9ceb8c946a88c13211fa3f084622df3ee5cfcd98cc6bb16" } , "transactions" : [ ] } , // ... { "header" : { "number" : 16004961 , "hash" : "0x9edecebf424558386879fe7f1b79550b6ab7d94ae9a953b2ac552c5ec99ad061" , "parentHash" : "0xffcb16149563c7ea48c398693141c2024645d83e768d37ed6cbd283a609475af" } , "transactions" : [ { "transactionIndex" : 126 , "hash" : "0xcbf7ff2e3f0cb52f436eca83ba540a526c855c1e28253ba42b3b46cc791a40ca" } ] } , // ... { "header" : { "number" : 16039119 , "hash" : "0x6c7a394c01931704bc850fa82ab21fe51b086b1afcedae61885abace1bc1e7e9" , "parentHash" : "0xeef4364766af5b838ff8059de4229b7a3381746d0046e390150f31d56f1163af" } , "transactions" : [ ] } ] Parse the retrieved data: Grab the network data you requested from the list items with non-empty data fields ( logs , transactions , stateDiffs , traces ). For the example above, this data will include the txn 0xcbf7... . Observe that we received the data up to and including block 16031419. To get the rest of the data, update the current height to 16031420 and go to step 3. Note how the worker URL you're getting while repeating step 3 points to a different host than before. This is how data storage and reads are distributed across the SQD Network. Repeat steps 3 through 6 until the dataset height of 18593441 is reached. In Python def get_text ( url : str ) - > str : res = requests . get ( url ) res . raise_for_status ( ) return res . text def dump ( gateway_url : str , query : Query , first_block : int , last_block : int ) - > None : assert 0 <= first_block <= last_block query = dict ( query ) # copy query to mess with it later dataset_height = int ( get_text ( f' { gateway_url } /height' ) ) next_block = first_block last_block = min ( last_block , dataset_height ) while next_block <= last_block : worker_url = get_text ( f' { gateway_url } / { next_block } /worker' ) query [ 'fromBlock' ] = next_block query [ 'toBlock' ] = last_block res = requests . post ( worker_url , json = query ) res . raise_for_status ( ) blocks = res . json ( ) last_processed_block = blocks [ - 1 ] [ 'header' ] [ 'number' ] next_block = last_processed_block + 1 for block in blocks : print ( json . dumps ( block ) ) Full code here . Router API ​ GET /height (get height of the dataset) Example response: 16576911 GET ${firstBlock}/worker (get a suitable worker URL) The returned worker is capable of processing POST / requests in which the "fromBlock" field is equal to ${firstBlock} . Example response: https://rb02.sqd-archive.net/worker/query/czM6Ly9uZW9uLWRldm5ldC10cmFjZWxlc3MtMQ Worker API ​ POST / (query EVM data) Query Fields ​ fromBlock : Block number to start from (inclusive). toBlock : (optional) Block number to end on (inclusive). If this is not given, the query will go on for a fixed amount of time or until it reaches the height of the dataset. includeAllBlocks : (optional) If true, the Network will include blocks that contain no data selected by data requests into its response. fields : (optional) A selector of data fields to retrieve. Common for all data items. logs : (optional) A list of log requests . An empty list requests no data. transactions : (optional) A list of transaction requests . An empty list requests no data. traces : (optional) A list of traces requests . An empty list requests no data. stateDiffs : (optional) A list of state diffs requests . An empty list requests no data. The response is a JSON array of per-block data items that covers a block range starting from fromBlock . The last block of the range is determined by the worker. You can find it by looking at the header.number field of the last element in the response array. The first and the last block in the range are returned even if all data requests return no data for the range. In most cases the returned range will not contain all the range requested by the user (i.e. the last block of the range will not be toBlock ). To continue, retrieve a new worker URL for blocks starting at the end of the current range plus one block and repeat the query with an updated value of fromBlock . Example Request 1 ​ { "logs" : [ { "address" : [ "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48" ] , "topic0" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" ] , "transaction" : true } ] , "fields" : { "block" : { "gasUsed" : true } , "log" : { "topics" : true , "data" : true } } , "fromBlock" : 16000000 , "toBlock" : 16000000 } Gets all Transfer(address,address,address) event logs emitted by the USDC contract on block 16000000, plus their parent transactions. Run curl https://v2.archive.subsquid.io/network/eth-mainnet/16000000/worker to get an URL of a worker capable of processing this query. Example Response 1 ​ Since the request was for one block, the response contains exactly one block: [ { "header" : { "number" : 16000000 , "hash" : "0x3dc4ef568ae2635db1419c5fec55c4a9322c05302ae527cd40bff380c1d465dd" , "parentHash" : "0x6f377dc6bd1f3e38b9ceb8c946a88c13211fa3f084622df3ee5cfcd98cc6bb16" , "gasUsed" : "0x121cdff" } , "transactions" : [ { "transactionIndex" : 0 } , { "transactionIndex" : 124 } , { "transactionIndex" : 131 } , { "transactionIndex" : 140 } , { "transactionIndex" : 188 } , { "transactionIndex" : 205 } ] , "logs" : [ { "logIndex" : 0 , "transactionIndex" : 0 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x000000000000000000000000ffec0067f5a79cff07527f63d83dd5462ccf8ba4" , "0x000000000000000000000000e47872c80e3af63bd237b82c065e441fa75c4dea" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000007270e00" } , { "logIndex" : 30 , "transactionIndex" : 124 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x000000000000000000000000f42ed7184f3bdd07b0456952f67695683afd9044" , "0x0000000000000000000000009bbcfc016adcc21d8f86b30cda5e9f100ff9f108" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000032430d8b" } , { "logIndex" : 34 , "transactionIndex" : 131 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x0000000000000000000000001d76271fb3d5a61184ba00052caa636e666d11ec" , "0x00000000000000000000000074de5d4fcbf63e00296fd95d33236b9794016631" ] , "data" : "0x000000000000000000000000000000000000000000000000000000000fa56ea0" } , { "logIndex" : 35 , "transactionIndex" : 131 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x00000000000000000000000074de5d4fcbf63e00296fd95d33236b9794016631" , "0x000000000000000000000000af0b0000f0210d0f421f0009c72406703b50506b" ] , "data" : "0x000000000000000000000000000000000000000000000000000000000fa56ea0" } , { "logIndex" : 58 , "transactionIndex" : 140 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x00000000000000000000000048c04ed5691981c42154c6167398f95e8f38a7ff" , "0x000000000000000000000000f41d156a9bbc1fa6172a50002060cbc757035385" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000026273075" } , { "logIndex" : 230 , "transactionIndex" : 188 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x000000000000000000000000ba12222222228d8ba445958a75a0704d566bf2c8" , "0x00000000000000000000000053222470cdcfb8081c0e3a50fd106f0d69e63f20" ] , "data" : "0x00000000000000000000000000000000000000000000000000000002536916b7" } , { "logIndex" : 232 , "transactionIndex" : 188 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x00000000000000000000000053222470cdcfb8081c0e3a50fd106f0d69e63f20" , "0x00000000000000000000000088e6a0c2ddd26feeb64f039a2c41296fcb3f5640" ] , "data" : "0x00000000000000000000000000000000000000000000000000000002536916b7" } , { "logIndex" : 372 , "transactionIndex" : 205 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x0000000000000000000000001116898dda4015ed8ddefb84b6e8bc24528af2d8" , "0x0000000000000000000000002796317b0ff8538f253012862c06787adfb8ceb6" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000018307e19" } , { "logIndex" : 374 , "transactionIndex" : 205 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x0000000000000000000000002796317b0ff8538f253012862c06787adfb8ceb6" , "0x000000000000000000000000735b75559ebb9cd7fed7cec2372b16c3871d2031" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000018307e19" } ] } ] Example Request 2 ​ { "logs" : [ { "address" : [ "0xb0b86991c6218b36c1d19d4a2e9eb0ce3606eb48" ] } ] , "fields" : { "log" : { "topics" : true , "data" : true } } , "fromBlock" : 16000000 } Attempts to gets all event logs emitted by a nonexistent contract on blocks starting at 16000000. Run curl https://v2.archive.subsquid.io/network/eth-mainnet/16000000/worker to get an URL of a worker capable of processing this query. Example Response 2 ​ The query matches no data, so the data field "logs" is an empty array for all the returned block data items: [ { "header" : { "number" : 16000000 , "hash" : "0x3dc4ef568ae2635db1419c5fec55c4a9322c05302ae527cd40bff380c1d465dd" , "parentHash" : "0x6f377dc6bd1f3e38b9ceb8c946a88c13211fa3f084622df3ee5cfcd98cc6bb16" } , "logs" : [ ] } , ... (a bunch of similar items for different block heights , all with "logs" : [ ] ) ... { "header" : { "number" : 16039119 , "hash" : "0x6c7a394c01931704bc850fa82ab21fe51b086b1afcedae61885abace1bc1e7e9" , "parentHash" : "0xeef4364766af5b838ff8059de4229b7a3381746d0046e390150f31d56f1163af" } , "logs" : [ ] } ] 16039119 is the highest block that the worker could process. For the data beyond that block request a new worker from the router and repeat the request with "fromBlock": 16039120 . Data requests ​ warning Addresses in all data requests must be in lowercase. All addresses in the responses will be in lowercase, too. Logs ​ { address : string [ ] , topic0 : string [ ] , topic1 : string [ ] , topic2 : string [ ] , topic3 : string [ ] , transaction : boolean , transactionTraces : boolean , transactionLogs : boolean } A log will be included in the response if it matches all the requests. A request with an empty array (e.g. { address: [] } ) matches no logs; omit all requests/pass an empty object to match all logs. See addLog() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Get all Transfer(address,address,uint256) event logs emitted by the USDC contract on blocks starting at 16_000_000, plus their parent transactions. Get topics and data for each log item and hash for each transaction. { "logs" : [ { "address" : [ "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48" ] , "topic0" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" ] , "transaction" : true } ] , "fields" : { "transaction" : { "hash" : true } , "log" : { "topics" : true , "data" : true } } , "fromBlock" : 16000000 } Get all event logs network-wide on blocks starting from block 0. Get topics for each log. { "logs" : [ { } ] , "fields" : { "log" : { "topics" : true } } , "fromBlock" : 0 } Transactions ​ { from : string [ ] , to : string [ ] , sighash : string [ ] , logs : boolean , traces : boolean , stateDiffs : boolean } A transaction will be included in the response if it matches all the requests. A request with an empty array (e.g. { from: [] } ) matches no transactions; omit all requests/pass an empty object to match all transactions. See addTransaction() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Get all transactions directly calling the transfer(address,uint256) method of the USDC contract on blocks starting at 16_000_000, plus the logs they emitted. Get hash and gas for each transaction and data for all logs. { "transactions" : [ { "to" : [ "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48" ] , "sighash" : [ "0xa9059cbb" ] , "logs" : true } ] , "fields" : { "transaction" : { "hash" : true , "gas" : true } , "log" : { "data" : true } } , "fromBlock" : 16000000 } Get all transactions on the network starting from 0. Get hash for each transaction. { "transactions" : [ { } ] , "fields" : { "transaction" : { "hash" : true } } , "fromBlock" : 0 } Traces ​ { type : string [ ] , createFrom : string [ ] , callFrom : string [ ] , callTo : string [ ] , callSighash : string [ ] , suicideRefundAddress : string [ ] , rewardAuthor : string [ ] transaction : boolean , transactionLogs : boolean , subtraces : boolean , parents : boolean } A trace will be included in the response if it matches all the requests. A request with an empty array (e.g. { callTo: [] } ) matches no traces; omit all requests/pass an empty object to match all traces. See addTrace() SDK function reference for a detailed description of the fields of this data request; also see Field selection . State diffs ​ { address : string [ ] , key : string [ ] , kind : string [ ] , transaction : bool } A state diff will be included in the response if it matches all the requests. A request with an empty array (e.g. { address: [] } ) matches no state diffs; omit all requests/pass an empty object to match all state diffs. See addStateDiff() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Data fields selector ​ A JSON selector of fields for the returned data items. Documented in the Field selection section. Edit this page Previous Public gateways Next Substrate API Router API Worker API Data requests Logs Transactions Traces State diffs Data fields selector
Transactions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Transactions On this page Transactions addTransaction(options) ​ Get some or all transactions on the network. options has the following structure: { // data requests where ? : { feePayer ? : string [ ] } // related data retrieval include ? : { instructions ? : boolean logs ? : boolean } range ? : { from : number to ? : number } } Data requests: feePayer sets the addresses of the fee payers. Leave it undefined to subscribe to all transactions. Enabling the instructions and/or logs flags will cause the processor to retrieve instructions and logs that occured as a result of each selected transaction. The data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/solana-objects on the block data to populate the convenience reference fields like transaction.logs . Note that transactions can also be requested by the other SolanaDataSource methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Examples ​ Request all transactions with fee payer rec5EKMGg6MxZYaMdyBfgwp4d5rB9T1VQH5pJv5LtFJ and include logs and instructions: processor . addTransaction ( { where : { feePayer : [ 'rec5EKMGg6MxZYaMdyBfgwp4d5rB9T1VQH5pJv5LtFJ' ] , } , include : { logs : true , instructions : true } } ) . build ( ) Edit this page Previous Instructions Next Log messages Examples
JSON queries | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API JSON queries JSON queries The possibility of defining JSON objects as fields of a type in a GraphQL schema has been explained in the schema reference . This guide is focusing on how to query such objects and how to fully leverage their potential. Let's take the example of this (non-crypto related, for onceüòÅ) schema: schema.graphql type Entity @entity { id : ID ! a : A } type A { a : String b : B } type B { a : A b : String e : Entity } It's composed of one entity and two JSON objects definitions, used in a "nested" way. Let's now look at a simple query: query { entities ( orderBy : id_ASC ) { id a { a } } } This will return a result such as this one (imagining this data exists in the database): { entities : [ { id : ' 1 ' , a : { a : ' a ' } } , { id : ' 2 ' , a : { a : ' A ' } } , { id : ' 3 ' , a : { a : null } } , { id : ' 4 ' , a : null } ] } Simply enough, the first two objects have an object of type A with some content inside, the third one has an object, but its a field is null and the fourth one simply does not have an A object at all. Edit this page Previous Cross-relation queries Next Pagination
Deployment manifest | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference Deployment manifest On this page Deployment manifest The deployment manifest is named squid.yaml by convention. The manifest defines how the squid should be built, customized and scaled during the deployment to SQD Cloud. It is used together with sqd deploy to deploy or update an existing squid version. With the introduction of the deployment manifest the add-ons ( postgres ) and the API services ( api ) become optional. This allows flexible deployment configuration for analytic use-cases (e.g. transform and write to s3 or BigQuery). tip Cloud secrets of the squid's organization are exposed as the secrets context. You can set any variable defined in any of the env: sections mentioned below: env : RPC_ENDPOINT : $ { { secrets.FAST_RPC_ENDPOINT_URL } } See the Environment variables page for more info. Header ​ The manifest header defines the squid metadata Field Description manifest_version Only subsquid.io/v0.1 is currently supported name A globally unique squid name. Can only contain lowercase Latin letters ( a-z ) and a dash ( - ). Must be under 20 symbols. version Squid version. Must be an integer . A squid deployment is canonically identified as ${name}@${version} . description (Optional) A short description of the squid build: ​ Specifies the way the squid is built into a docker image. Name Type Default value node_version 18 | 20 | 21 20 package_manager auto | npm | pnpm | yarn auto cmd list For a successful build the following files and folders must be present in the root folder of the squid: /src tsconfig.json package.json commands.json The db and assets folders are added to the build context if present in the squid folder. See Project structure for more info. Under the hood, Cloud builds a Docker image and runs a docker container for each service ( api , processor , migrate ) using the same image.
See Self-hosting for instructions on how to build and run the Docker image locally.
Even though the squid services ( api , processor , migrate ) use the same single container image, the exec command is different and can is defined by the deploy: section as explained below. cmd: ​ Enables overriding the dependencies installation command, e.g. build : cmd : - npm - install The default is to use a shrinkwrap-based installation command appropriate for the detected package manager (e.g. npm ci ). deploy: ​ The deploy section may define: addons: ​ A list of add-on services to be deployed along the squid services. postgres: See Postgres add-on . rpc See RPC add-on . migrate: ​ Optional The init container ran before processor and api services are started. If it exits with an error, squid deployment fails. Name Description Type Default value Optional cmd Exec command passed to the squid container. By default runs TypeORM migrations in the db folder string[] ['npx', 'squid-typeorm-migration', 'apply'] Optional env A key-value list of env variables to be set {k:v}[] [] Optional processor: ​ A processor service or a list of processor services of the squid. Name Description Type Default value Optional cmd Processor exec command passed to the squid container string[] Required name Processor name string Required with more than one processor env A key-value list of env variables to be set {k:v}[] [] Optional With a single processor this section may look like this: processor : cmd : [ "sqd" , "process:prod" ] For the multiprocessor case: processor : - name : eth - processor cmd : [ "sqd" , "process:prod:eth" ] - name : bsc - processor cmd : [ "sqd" , "process:prod:bsc" ] where process:prod:bsc and process:prod:eth are extra sqd commands defined at commands.json : commands.json ... "process:prod:eth" : { "deps" : [ "migration:apply" ] , "cmd" : [ "node" , "lib/eth/main.js" ] } , "process:prod:bsc" : { "cmd" : [ "node" , "lib/bsc/main.js" ] } , ... api: ​ Optional The GraphQL API service of the squid. Automatically provisions a publicly available endpoint https://{org}.subsquid.io/{name}/v/v{version}/graphql and binds it to the server. Name Description Type Default value Optional cmd GraphQL API server exec command passed to the squid container. The server must be listening at port GRAPHQL_SERVER_PORT . string[] Required env A key-value list of env variables to be set {k:v}[] [] Optional env: ​ Optional A key-value list of deployment-wide (i.e. visible to all services of the squid) env variables to be set scale: ​ See the Scale the deployment section. Examples ​ A minimal example of manifest is below: squid.yaml manifest_version : subsquid.io/v0.1 name : sample - squid version : 1 description : | - My sample squid build : deploy : addons : postgres : processor : cmd : [ "sqd" , "process:prod" ] api : cmd : [ "sqd" , "serve:prod" ] An extended version: squid.yaml manifest_version : subsquid.io/v0.1 name : sample - squid version : 1 description : | - My advanced squid build : deploy : addons : postgres : migrate : env : FOO : bar cmd : [ "echo" , "skip migrations!" ] processor : # static and secret-valued env variables env : SQD_DEBUG : sqd : mapping RPC_ENDPOINT : $ { { secrets.ACALA_RPC_ENDPOINT } } COINGECKO_API_KEY : $ { { secrets.COINGECKO_API_KEY } } cmd : [ "sqd" , "process:prod" ] api : env : SQD_DEBUG : '*' # custom run command for the GraphQL server cmd : [ "npx" , "squid-graphql-server" , "--subscriptions" , "--max-root-fields" , "10" , "--sql-statement-timeout" , "1000" ] scale : addons : postgres : storage : 100G profile : medium processor : profile : medium api : profile : large # load-balance three replicas replicas : 3 Edit this page Previous Reference Next scale section Header build: cmd: deploy: addons: migrate: processor: api: env: scale: Examples
Block data for EVM | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM Block data for EVM On this page Block data for EVM EvmBatchProcessor follows the common squid processor architecture , in which data processing happens within the batch handler , a function repeatedly called on batches of on-chain data. The function takes a single argument called "batch context". Its structure follows the common batch context layout , with ctx.blocks being an array of BlockData objects containing the data to be processed, aligned at the block level. For EvmBatchProcessor the BlockData interface is defined as follows: export type BlockData < F extends FieldSelection = { } > = { header : BlockHeader < F > transactions : Transaction < F > [ ] logs : Log < F > [ ] traces : Trace < F > [ ] stateDiffs : StateDiff < F > [ ] } F here is the type of the argument of the setFields() processor method. BlockData.header contains the block header data. The rest of the fields are iterables containing four kinds of blockchain data. The canonical ordering within each iterable depends on the data kind: transactions and logs are ordered in the same way as they are within blocks; stateDiffs follow the order of transactions that gave rise to them; traces are ordered in a deterministic but otherwise unspecified way. The exact fields available in each data item type are inferred from the setFields() call argument. They are documented on the field selection page: transactions section ; logs section ; traces section ; state diffs section ; block header section . Example â€‹ The handler below simply outputs all the log items emitted by the contract 0x2E645469f354BB4F5c8a05B3b30A929361cf77eC in real time : import { TypeormDatabase } from '@subsquid/typeorm-store' import { EvmBatchProcessor } from '@subsquid/evm-processor' const CONTRACT_ADDRESS = '0x2E645469f354BB4F5c8a05B3b30A929361cf77eC' . toLowerCase ( ) const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setFinalityConfirmation ( 75 ) . setBlockRange ( { from : 17000000 } ) . addLog ( { address : [ CONTRACT_ADDRESS ] } ) . setFields ( { // could be omitted: this call does not change the defaults log : { topics : true , data : true } } ) processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let c of ctx . blocks ) { for ( let log of c . logs ) { if ( log . address === CONTRACT_ADDRESS ) { ctx . log . info ( log , ` Log: ` ) } } } } ) One can experiment with the setFields() argument and see how the output changes. For more elaborate examples, check EVM Examples . Edit this page Previous EVM Next General settings Example
sqd auth | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd auth On this page sqd auth Log in to the Cloud sqd auth sqd auth â€‹ Log in to the Cloud USAGE $ sqd auth -k <value> [--interactive] FLAGS -k, --key=<value>       (required) Cloud auth key. Log in to https://app.subsquid.io to create or update your key. --[no-]interactive  Disable interactive mode See code: src/commands/auth.ts Edit this page Previous commands.json Next sqd autocomplete sqd auth
Internal transactions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Block data for Tron General settings Transactions Logs Internal transactions Field selection Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing TronBatchProcessor Internal transactions On this page Internal transactions addInternalTransaction(options) ​ Get some or all internal transactions on the network. options has the following structure: { where ? : { caller ? : string [ ] transferTo ? : string [ ] } include ? : { transaction ? : boolean } range ? : { from : number to ? : number } } Data requests are located in the where field: caller is the set of caller addresses responsible for the internal transactons. Leave it undefined to subscribe to internal txs from all callers. transferTo is the set of receiver addresses that the internal txn is addressed to. Omit the where field to subscribe to all txs network-wide. Related data can be requested via the include field: transaction = true : will retrieve parent transactions for each selected internal txn. The data will be added to the .transactions iterable within block data and made available via the .transaction field of each internal transaction item. Note that internal transactions can also be requested by the other TronBatchProcessor methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Example ​ TBA Edit this page Previous Logs Next Field selection
ink! contracts support | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific ink! contracts support Frontier EVM support Gear support Substrate data sourcing Substrate types bundles Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Substrate-specific ink! contracts support On this page ink! contracts support This section describes additional options available for indexing ink!-based WASM contracts , supported by chains with a Contracts pallet. At the moment of writing, AlephZero, Shibuya (Astar testnet), Shiden (Kusama parachain) and Astar (Polkadot parachain) are the most popular chains for deploying ink! contracts. Generate an ink! indexing squid automatically , follow the WASM squid tutorial for a step-by-step instruction or check out the squid-wasm-template reference project. Processor configuration â€‹ Request events by the contract address as described on the SubstrateBatchProcessor reference page , e.g. import * as ss58 from '@subsquid/ss58' import { toHex } from '@subsquid/util-internal-hex' const ADDRESS = toHex ( ss58 . decode ( 'XnrLUQucQvzp5kaaWLG9Q3LbZw5DPwpGn69B5YcywSWVr5w' ) . bytes ) const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/shibuya-substrate' ) . setRpcEndpoint ( 'https://shibuya.public.blastapi.io' ) . addContractsContractEmitted ( { contractAddress : [ ADDRESS ] , extrinsic : true } ) . setFields ( { event : { phase : true } } ) Generate and use the facade classes for decoding ink! smart contract data as described in the typegens reference . You can also make direct contract state queries using the Contract class generated by squid-ink-typegen . Edit this page Previous Substrate-specific Next Frontier EVM support Processor configuration
Run a worker | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Procuring SQD Delegate Run a worker Run a gateway Self-host a portal Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Participate Run a worker On this page Run a worker Requirements ​ To run a single worker you'll need: 4 vCPU 16GB RAM 1TB SSD stable 24/7 Internet connection, at least 1Gbit a public IP and two ports open for incoming traffic: one UDP port for p2p communication (default 12345) one TCP port for Prometheus metrics (default 9090) Docker + Docker Compose 100_000 SQD tokens (in your wallet or in a special vesting contract) some Arbitrum ETH (for gas) The SQD tokens should be available to your Primary Wallet - either directly or through a vesting contract. With enough funds you can use one Primary Wallet to register multiple workers. Your Primary Wallet should have browser support. We recommend Metamask. You can run a worker from a Docker image or from its source code. Note that you'll need Docker either way, as our configuration script uses it. Configuring your setup ​ Pick a location for worker data. It should be able to accommodate at least 1Tb. Do not create the data folder manually, just find a place for it. Pick a location for your SQD Network key file. The location must be outside of the data directory. ⚠️ Warning: Ensure that that the key will not be deleted accidentally and cannot be accessed by unauthorized parties. Or else . Create a new directory for installation files and save the setup_worker.sh script in it: curl -fO https://cdn.subsquid.io/worker/setup_worker.sh Make the script executable: chmod +x ./setup_worker.sh Run the setup script. Supply the data directory path and the key path to it: ./setup_worker.sh < DATA_DIR_PATH > < KEY_PATH > The script will prompt you for an UDP port to use for P2P communication and give you an option to set your public IP address in worker config statically. Most users should not set the IP address here since setups with automatic IP discovery are more robust. Here's what the script does: creates a directory at <DATA_DIR_PATH> generates a key file at <KEY_PATH> generates a .env file in the current directory and populates it with reasonable defaults downloads a .mainnet.env file downloads a docker-compose.yaml file for running prebuilt worker images You can take a look at the files that the script downloads here . The last line of the script's output should look like this: Your peer ID is: 12D3KooWPfotj6qQapKeWg4RZysUjEeswLEpiSjgv16LgNTyYFTW. Now you can register it on chain. Please copy your peer ID , as it will be needed for on-chain worker registration . (optional) Feel free to edit the generated .env and .mainnet.env files if you'd like, e.g., to set a custom Prometheus port or use your own RPC provider. Proceed to Worker registration . Worker registration ​ Before you run a worker node, you need to register it on-chain using our web application. Here are the steps to do this: Go to https://network.subsquid.io . Connect your Primary Wallet. Go to the Workers tab and press the "Add Worker" button. You should see a worker registration form: Fill the form and submit it by signing a transaction: In the top drop down menu, choose either "Wallet" (to use SQD s from your wallet directly) or "Vesting contract" (to use SQD s from a vesting contract). Use the peer ID you copied at step 4 of Configuring your setup . Go to the "Workers" tab and wait until the status of the registered worker changes to "Offline" or "Active". Since workers can only be activated at the beginning of an epoch , you may have to wait for a few minutes. Proceed to Running a worker . Running a worker ​ Make sure you're still in the folder where you executed setup_worker.sh during configuration before proceeding. Using a pre-built Docker image ​ Run docker compose up -d then check the logs of the worker container with docker compose logs . After some time the worker should output some info on the downloaded data chunks. Building from the source ​ Install prerequisites (Rust, Git, Protobuf, etc), e.g. with apt install curl git build-essential pkg-config protobuf-compiler libssl-dev libsqlite3-dev curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Load the config generated during the setup . source .mainnet.env source .env Clone the worker repo. git clone --depth 1 --branch v1.0.0 https://github.com/subsquid/worker-rs.git This may take a few minutes. Build the worker. cd worker-rs cargo build --release Run the worker using the command below. It uses nohup , but you may also consider daemonizing it with systemd or openrc to tolerate server restarts. nohup cargo run --release -- --port $PROMETHEUS_PORT p2p After some time the worker should start downloading data and serving queries. Updating a worker ​ Sometimes you will need to update your worker. Here's the default procedure: Back up your key file. If you are running a docker image: go to your installation files folder fetch the updated docker-compose.yaml : curl -fO https://cdn.subsquid.io/worker/docker-compose.yaml run docker compose up to apply the update. If you're running a binary built from source: note the new worker version in the announcement go to your installation files folder enter the worker-rs subfolder fetch the new commits: git fetch --unshallow check out the new worker version: git checkout < new_version_tag > rebuild the worker: cargo build --release restart the worker process No need to erase the data folder or re-register your worker. Some releases may require that you deviate from this procedure, so please read release notes carefully. On jailing ​ Jailing is a scheduler-side mechanism that ensures that every data chunk is available for querying. The scheduler tries to predict which workers are "unreliable" and makes the best effort to guarantee that each chunk is available on at least several "reliable" worker nodes.
However, even if a worker is marked as unreliable, it continues serving queries from the chunks it had downloaded and getting the rewards. And it's only temporary — the worker will get unjailed automatically within ~3 hours. If the reason for jailing is gone, it won't be jailed next time. If the worker constantly gets jailed, it may affect its rewards, so it shouldn't be ignored. There are multiple reasons why the worker can be marked as unreliable (see worker's logs to find out which one applies): Worker didn't download any of the assigned chunks in last 900 seconds ( stale in our internal terminology) — this one is the most popular. The cause of the problem should be found in the worker's logs. The most probable one is the download timeouts . Worker didn't send pings for over 600 seconds ( inactive ) — it may occur for example when a worker got back after a long downtime. If the scheduler doesn't receive pings from the worker, it considers it offline and doesn't assign any chunks to it. Worker could not be reached on a public address ( unreachable ). This was our attempt to determine which nodes are not reachable through the p2p network, but the correct implementation turned out harder than it seemed, so it is disabled right now. Troubleshooting ​ Where do I find my peer ID ? ​ It is printed when you run setup_worker.sh (see Configuring your setup ). It is also in the first line of the worker log output. For the docker setup, inspect the worker container logs with docker compose logs . If you installed your worker from source, check the network.log file. In both cases, the log line you are looking for should look like this 2024-05-28T07:43:55.591440Z  INFO subsquid_worker::transport::p2p: Local peer ID: <PEER_ID> I see Failed to download chunk ... operation timed out in the worker logs ​ Depending on your connection quality, you might want to tune the S3_TIMEOUT and CONCURRENT_DOWNLOADS environment variables in the .env file. If you encounter this error frequently,  try to set S3_TIMEOUT to 180 . If it still doesn't help, set CONCURRENT_DOWNLOADS to 1 and S3_READ_TIMEOUT to 30 . I see Unable to get assignment: deadline has elapsed in the worker logs ​ This should only be an issue on version 2.0.2. As a workaround please set ASSIGNMENT_FETCH_TIMEOUT_SEC=90 in the .env file and restart. I see an Insufficient peers... error in the worker logs ​ Just ignore it Can I move my worker to another server? ​ Yes, copy the key file (at <KEY_PATH> ) to the new working directory before starting. You don't need to re-register your worker. Don't forget to update the data directory on the new server. I have just started my worker but see no logs ​ This is normal. Wait for a few minutes and the logs should show some data being downloaded. Should I build it from source or run with Docker? ​ Docker makes for an easier setup. Building from sources is suitable only for experienced Linux/Unix users. How do I check that my worker is updated to the latest version? ​ Check the pings endpoint and locate the version by your peer ID. Which Linux distro is recommended? ​ We recommend Ubuntu 22.04 LTS. I see error from daemon in stream: Error grabbing logs ​ This is a Docker issue, not a problem with the worker. Look at this GitHub issue and this Stackoverflow thread for more context. How do I check if my worker is up-to-date and running? ​ Copy your peer ID and look for an entry on this page . If the last ping timestamp is 1 minute ago, and the listed version is the most recent one, you should be good. I'm getting Clone succeeded, but checkout failed when cloning the worker repo ​ Enter the repo folder and check out v1.0.0 manually: cd worker-rs git checkout v1.0.0 What are the consequences of losing my key file / getting it stolen? ​ If you lose your key file you won't be able to run your worker until you get a new one and register it. If your key file gets stolen the perpetrator will be able to cause connectivity issues for your worker. If any of that happens, unregister your worker (on the "Workers" tab of network.subsquid.io ), generate a new key file and register your new peer ID. Edit this page Previous Delegate Next Run a gateway Requirements Configuring your setup Worker registration Running a worker Using a pre-built Docker image Building from the source Updating a worker On jailing Troubleshooting
Cross-relation queries | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Cross-relation queries On this page Cross-relation field queries Introduction ​ The previous section has already demonstrated that queries can return not just scalars such as a String, but also fields that refer to object or entity types. What's even more interesting is that queries can leverage fields of related objects to filter results. Let's take this sample schema with two entity types and a one-to-many relationship between them: schema.graphql type Account @entity { id : ID ! wallet : String ! balance : Int ! history : [ HistoricalBalance ! ] @derivedFrom ( field : "account" ) } type HistoricalBalance @entity { " Unique identifier " id : ID ! " Related account " account : Account ! " Balance " balance : Int ! } With the functionality offered by cross-relation field queries, we could ask for Account s that have at least some historicalBalance s  with a balance smaller than a certain threshold: query MyQuery { accounts ( where : { historicalBalances_some : { balance_lt : "10000000000" } } ) { id } } This allows to query based not just on the entity itself, but on the related entities as well, which is intuitively a very powerful feature. *_some is not the only operator available for making cross-relation field queries. A short description of each such operator is provided in the sections below. The *_every filter ​ Returns entities for which all of the nested entities linked via the related field satisfy the condition. Example: schema.graphql query MyQuery { accounts ( where : { historicalBalances_every : { balance_lt : "10000000000" } } ) { id } } This query will return all Account s where each and every one of the HistoricalBalance entities related to them have a balance smaller than the threshold. It is sufficient for a single HistoricalBalance to have a balance larger than the set value to make sure that the related Account is not returned in the query. The *_none filter ​ Returns entities for which none of the nested entities linked via the related field satisfy the condition. Example: query MyQuery { accounts ( where : { historicalBalances_none : { balance_lt : "10000000000" } } ) { id } } The query will return all Account s in which not a single related HistoricalBalance has a balance smaller than the set threshold. The *_some filter ​ Returns entities for which at least one of the nested entities linked via the related field satisfies the condition. Example: query MyQuery { accounts ( where : { historicalBalances_some : { balance_lt : "10000000000" } } ) { id } } All Account s that have at least some historicalBalance s with a balance smaller than 10000000000 will be returned. This means that a single HistoricalBalance satisfying the condition is sufficient for the related Account to become a part of the results. {entityName}sConnection queries ​ Same as always, the where argument works for these queries in exactly the same way as it does for {entityName}s queries used in examples above. For example this query query MyQuery { accountsConnection ( orderBy : id_ASC , where : { historicalBalances_some : { balance_lt : "10000000000" } } ) { edges { node { id } } } } will return (in an appropriately shaped response) IDs for all Accounts that have at least some historicalBalance s with a balance smaller than 10000000000 . Edit this page Previous Nested field queries Next JSON queries Introduction The *_every filter The *_none filter The *_some filter {entityName}sConnection queries
Extract, transform, load and query historical blockchain data from SQD data lakes | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Extract, transform, load and query historical blockchain data from SQD data lakes 📄️ Quickstart A practical intro into squids 📄️ Overview Architecture of squid indexers 🗃️ Getting started 5 items 🗃️ Features & Guides 11 items 🗃️ Tutorials 9 items 🗃️ Reference 6 items 📄️ Troubleshooting Common issues arising when developing squids 📄️ Examples Example squids 📄️ FAQ Frequently asked questions 📄️ SQD vs The Graph Comparison of SQD and The Graph Previous Portal beta info Next Quickstart
Processor architecture | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors Processor architecture On this page Processor architecture The processor service is a Node.js process responsible for data ingestion, transformation and data persisting into the target database. By convention , the processor entry point is at src/main.ts . It is run as node lib/main.js For local runs, one normally additionally exports environment variables from .env using dotenv : node -r dotenv/config lib/main.js Processor choice ​ The Squid SDK currently offers specialized processor classes for EVM ( EvmBatchProcessor ) and Substrate networks ( SubstrateBatchProcessor ). More networks will be supported in the future. By convention, the processor object is defined at src/processor.ts . Navigate to a dedicated section for each processor class: EvmBatchProcessor SubstrateBatchProcessor Configuration ​ A processor instance should be configured to define the block range to be indexed, and the selectors of data to be fetched from SQD Network and/or a node RPC endpoint. processor.run() ​ The actual data processing is done by the run() method called on a processor instance (typically at src/main.ts ). The method has the following signature: run < Store > ( db : Database < Store > , batchHander : ( context : DataHandlerContext < Store , F extends FieldSelection > ) => Promise < void > ) : void The db parameter defines the target data sink , and batchHandler is an async void function defining the data transformation and persistence logic. It repeatedly receives batches of SQD Network data stored in context.blocks , transforms them and persists the results to the target database using the context.store interface (more on context in the next section). To jump straight to examples, see EVM Processor in action and Substrate Processor in action . Batch context ​ Batch handler takes a single argument of DataHandlerContext type: export interface DataHandlerContext < Store , F extends FieldSelection > { _chain : Chain log : Logger store : Store blocks : BlockData < F > [ ] isHead : boolean } Here, F is the type of the argument of the setFields() ( EVM , Substrate ) processor configuration method. Store type is inferred from the Database instance passed into the run() method. ctx._chain ​ Internal handle for direct access to the underlying chain state via RPC calls. Rarely used directly, but rather by the facade access classes generated by the typegen tools . ctx.log ​ The native logger handle. See Logging . ctx.store ​ Interface for the target data sink. See Persisting data . ctx.blocks ​ On-chain data items are grouped into blocks, with each block containing a header and iterables for all supported data item types. Boundary blocks are always included into the ctx.blocks iterable with valid headers, even when they do not contain any requested data. It follows that batch context always contains at least one block. The set of iterables depends on the processor type (docs for EVM / Substrate ). Depending on the data item type, items within the iterables can be canonically ordered by how the data is recorded on-chain (e.g. transactions are ordered but traces are not). The shape of item objects is determined by the processor configuration done via the .setFields() method. An idiomatic use of the context API is to iterate first over blocks and then over each iterable of each block: EVM Substrate processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let block of ctx . blocks ) { for ( let log of block . logs ) { // filter and process logs } for ( let txn of block . transactions ) { // filter and process transactions } for ( let stDiff of block . stateDiffs ) { // filter and process state diffs } for ( let traces of block . traces ) { // filter and process execution traces } } } ) processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let block of ctx . blocks ) { for ( let event of block . events ) { // filter and process events } for ( let call of block . calls ) { // filter and process calls } for ( let extrinsic of block . extrinsics ) { // filter and process extrinsics } } } ) The canonical ordering of ctx.blocks enables efficient in-memory data processing. For example, multiple updates of the same entity can be compressed into a single database transaction. Please be aware that the processor cannot ensure that data not meeting its filters will be excluded from iterables. It only guarantees the inclusion of data that matches the filters. Therefore, it is necessary to filter the data in the batch handler prior to processing. ctx.isHead ​ Is true if the processor has reached the chain head. The last block ctx.blocks is then the current chain tip. Edit this page Previous Processors Next EVM Processor choice Configuration processor.run() Batch context
The frontier package | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference The frontier package On this page @subsquid/frontier The way the Frontier EVM pallet exposes EVM logs and transaction may change due to runtime upgrades. @subsquid/frontier provides helper methods that are aware of the upgrades: getEvmLog(event: Event): EvmLog ​ Extract the EVM log data from EVM.Log event. getTransaction(call: Call): LegacyTransaction | EIP2930Transaction | EIP1559Transaction ​ Extract the transaction data from Ethereum.transact call with additional fields depending on the EVM transaction type. getTransactionResult(ethereumExecuted: Event): {from: string, to: string, transactionHash: string, status: 'Succeed' | 'Error' | 'Revert' | 'Fatal', statusReason: string} ​ Extract transaction result from an Ethereum.Executed event. See also the Frontier EVM guide . Example ​ const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/astar-substrate' ) . setRpcEndpoint ( 'https://astar-rpc.dwellir.com' ) . addEthereumTransaction ( { } ) . addEvmLog ( { } ) processor . run ( new TypeormDatabase ( ) , async ctx => { for ( const block of ctx . blocks ) { for ( const event of block . events ) { if ( event . name === 'EVM.Log' ) { // no need to supply any extra data to determine // the runtime version: event has all necessary references const { address , data , topics } = getEvmLog ( event ) // process evm log data } } for ( const call of block . calls ) { if ( call . name === 'Ethereum.transact' ) { const txn = getTransaction ( call ) // process evm txn data } } } } ) Edit this page Previous Union type resolution Next Troubleshooting
sqd remove | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd remove On this page sqd remove Remove a squid deployed to the Cloud sqd remove sqd remove â€‹ Remove a squid deployed to the Cloud USAGE $ sqd remove [--interactive] [-f] [-r [<org>/]<name>(@<slot>|:<tag>) | -o <code> | [-s <slot> -n <name>] | [-t <tag> ]] FLAGS -f, --force             Does not prompt before removing a squid or its version --[no-]interactive  Disable interactive mode SQUID FLAGS -n, --name=<name>                               Name of the squid -r, --reference=[<org>/]<name>(@<slot>|:<tag>)  Fully qualified reference of the squid. It can include the organization, name, slot, or tag -s, --slot=<slot>                               Slot of the squid -t, --tag=<tag>                                 Tag of the squid ORG FLAGS -o, --org=<code>  Code of the organization ALIASES $ sqd rm See code: src/commands/rm.ts Edit this page Previous prod Next sqd restart sqd remove
Supported data sinks for squid processors | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data Store interface Saving to PostgreSQL Saving to filesystems Saving to BigQuery EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Persisting data Supported data sinks for squid processors üìÑÔ∏è Store interface Store API for persisting the transformed data üìÑÔ∏è Saving to PostgreSQL Working with TypeORM-compatible DBs üìÑÔ∏è Saving to filesystems Save data to filesystem-based datasets üìÑÔ∏è Saving to BigQuery Support for Google's data warehouse Previous Self-hosting Next Store interface
Access control | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Caching Custom API extensions Subscriptions DoS protection Access control Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Configuration Access control On this page Access control To implement access control, define the following function in the designated src/server-extension/check module: import { RequestCheckContext } from '../../node_modules/@subsquid/graphql-server/src/check' export async function requestCheck ( req : RequestCheckContext ) : Promise < boolean | string > { ... } Once defined, this function will be called every time a request arrives. Then, if the function returns true , the request is processed as usual; if the function returns false , the server responds with '{"errors":[{"message":"not allowed"}]}' ; if the function returns an errorString , the server responds with `{{"errors":[{"message":"${errorString}"}]}` . The request information such as HTTP headers and GraphQL selections is available in the context. This makes it possible to authenticate the user that sent the query and either allow or deny access. The decision may take the query contents into account, allowing for some authorization granularity. RequestCheckContext ​ The context type has the following interface: RequestCheckContext { http : { uri : string , method : string , headers : HttpHeaders } operation : OperationDefinitionNode operationName : string | null schema : GraphQLSchema context : Record < string , any > model : Model } Here, http field contains the low level HTTP info. Information on headers is stored in a Map from lowercase header names to values. For example, req.http.headers.get('authorization') is the value of the authorization header. operation is the root OperationDefinitionNode of the tree describing the query. Useful if the authorization decision depends on the query contents. operationName is the query name. schema is a GraphQLSchema object. context holds a PoolOpenreaderContext at context.openreader . It can be used to access the database, though this is highly discouraged: the interfaces involved are considered to be internal and are subject to change without notice. model is an Openreader data Model . Sending user data to resolvers ​ Authentication data such as user name can be passed from requestCheck() to a custom resolver through Openreader context: export async function requestCheck ( req : RequestCheckContext ) : Promise < boolean | string > { ... // obtain user name e.g. by decoding the authentication header let user = ... // save user name to Openreader context req . context . openreader . user = user ... } A custom resolver that retrieves it may look like this: @ Resolver ( ) export class UserCommentResolver { constructor ( private tx : ( ) => Promise < EntityManager > ) { } @ Query ( ( ) => [ UserCommentCountQueryResult ] ) async countUserComments ( @ Ctx ( ) ctx : any ) : Promise < UserCommentCountQueryResult [ ] > { let user = ctx . openreader . user let manager = await this . tx ( ) let result : UserCommentCountQueryResult [ ] = await manager . getRepository ( UserComment ) . query ( ` SELECT COUNT(*) as total FROM user_comment WHERE "user" = ' ${ user } ' ` ) return result } @ Mutation ( ( ) => Boolean ) async addComment ( @ Arg ( 'text' ) comment : string , @ Ctx ( ) ctx : any ) : Promise < Boolean > { let user = ctx . openreader . user let manager = await this . tx ( ) await manager . save ( new UserComment ( { id : ` ${ user } - ${ comment } ` , user , comment } ) ) return true } } See full code in this branch . This approach does not work with subscriptions . Examples ​ A simple strategy that authorizes anyone with a 12345 token to perform any query can be implemented with src/server-extension/check.ts import { RequestCheckContext } from '../../node_modules/@subsquid/graphql-server/src/check' export async function requestCheck ( req : RequestCheckContext ) : Promise < boolean | string > { return req . http . headers . get ( 'authorization' ) === 'Bearer 12345' } A more elaborate example with two users authorized to perform different query sets is available in this repo . Another great example of using requestCheck() for authorization can be spotted in the wild in the code of a squid used by Reef . Edit this page Previous DoS protection Next Core API RequestCheckContext Sending user data to resolvers Examples
Receipts | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource Receipts On this page Receipts addReceipt(options) ​ Get some or all transactions on the network. options has the following structure: { // data requests type ? : ReceiptType [ ] contract ? : string [ ] // related data retrieval transaction ? : boolean range ? : { from : number to ? : number } } Data requests: type sets the type of the receipt. Receipt type has the following options: 'CALL' | 'RETURN' | 'RETURN_DATA' | 'PANIC' | 'REVERT' | 'LOG' | 'LOG_DATA' | 'TRANSFER' | 'TRANSFER_OUT' | 'SCRIPT_RESULT' | 'MESSAGE_OUT' | 'MINT' | 'BURN' . Leave it undefined to subscribe to all receipts. contract sets the contract addresses to track. Leave it undefined to subscribe to all receipts. Enabling the transaction flag will cause the processor to retrieve transactions that gave rise to the matching receipts. The data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/fuel-objects on the block data to populate the convenience reference fields like receipt.transaction . Note that receipts can also be requested by the other FuelDataSource methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Examples ​ Request all receipts of the LOG_DATA type and include parent transactions: processor . addReceipt ( { type : [ "LOG_DATA" ] , transaction : true , } ) . build ( ) ; Edit this page Previous Outputs Next Transactions Examples
production-alias | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources production-alias On this page danger Production aliasing feature is deprecated in @subsquid/cli>=3.0.0 . Use Slots and tags instead. Alias to the production endpoint Version aliasing is used to switch between squid versions without a downtime and updates of the downstream clients.
Each squid has a canonical production endpoint URL of the form https:// < org name > .subsquid.io/ < squid name > /graphql To alias a squid version to the production endpoint, use sqd prod : sqd prod < squid name > @ < version > Note that after promoting to the production the version-specific endpoint URL of the form https:// < org name > .subsquid.io/ < squid name > /v/v < version > /graphql remains to be available. Example â€‹ Assuming your organization is called my-org , running sqd prod my-squid@v1 will make the endpoint of the v1 of my-squid accessible at https://my-org.subsquid.io/my-squid/graphql . Edit this page Previous Migrate to the Cloud portal Next Reference Example
Self-host a portal | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Procuring SQD Delegate Run a worker Run a gateway Self-host a portal Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Participate Self-host a portal On this page Self-host a portal info SQD Network portals are currently in closed beta. Please report any bugs or suggestions to the SQD Portal chat or to Squid Devs . warning If you want to use your private portal to index Solana, please read the Using your Portal - On Solana section before committing. Running a portal enables you to access the permissionless SQD Network data without relying on any centralized services. You can run a private portal for your own needs, or make a high-throughput public portal. In either scenario you will need A working Docker installation 10000 or more SQD tokens You won't have to spend the tokens, but you will need to keep them locked while you're using your portal. The more tokens you lock, the more bandwidth you get. For the minimum amount of 10000 SQD you'll get enough bandwidth to run a few squids. If you expect heavier workloads, read the Token requirements and compute units section to estimate the required sum. The tokens can be in your wallet or in an SQD-issued vesting contract. Some Arbitrum ETH (for gas) Portal hardware requirements: at least 25Gb of unused RAM (we're working to reduce this) The rest of the hardware requirements depend on how the portal will be used. A private portal with a single user can run on a laptop, provided that it has enough RAM. Setting up a portal ​ Step 1: Lock some SQD ​ Go to network.subsquid.io and connect your wallet (we recommend Metamask). Use the wallet that holds the tokens or is the beneficiary of your vesting contract. Go to the portals page and press the "Lock" button. Specify the amount of SQD you want to lock and the duration of the lockup. Press "Confirm" then confirm the transaction in your wallet. The page should update to display the amount of SQD you locked and the duration of the lockup: Now you have some locked tokens associated with your wallet and are ready to set up the portal itself. tip By default, your portal will go down at the end of the lock period. To prevent that, click the "Auto Extension" switch and confirm the transaction after locking. This will cause your SQD to be immediately relocked once the locking period ends. In this setup you have to unlock then wait for the end of the current locking period to withdraw your tokens. Step 2: Generate a peer ID ​ SQD Network is a decentralized peer-to-peer system. Portals are a type of peers participating in it. Like all peers, your portal requires a private key and a public unique identifier: the peer ID . Generate one of these with docker run --rm subsquid/rpc-node:0.2.5 keygen > < KEY_FILE_PATH > The key will be written to the binary file at <KEY_FILE_PATH> . You will see your peer ID in the output: Your peer ID: <THIS IS WHAT YOU NEED TO COPY> Please copy your peer ID. ⚠️ Note: Please make sure that the generated file is safe and secure at <KEY_FILE_PATH> (i.e. it will not be deleted accidentally and cannot be accessed by unauthorized parties). Or else . Step 3: Register your portal ​ Go to the portals page and click the "Add portal" button. Fill the portal registration form. If you plan to make your portal public, click the "Publicly available" switch and populate the additional fields. Once done, click "Confirm" and confirm the transaction in your wallet. Step 4: Run your portal ​ Clone the portal repo and enter the folder. git clone https://github.com/subsquid/sqd-portal cd sqd-portal Prepare the environment. Begin with cp mainnet.env .env then set the path to your key file: echo KEY_PATH = < KEY_FILE_PATH > >> .env ⚠️ Warning: Be careful when supplying the path to the key you created at step 4. If you make a mistake here, a new random key will be automatically created there and your node will attempt to operate with a new (unregistered) peer ID - unsuccessfully. Run your portal. You can either utilize a pre-built Docker image: docker compose up -d or build it from source: cargo run --release ⚠️  Tested on cargo-1.82.0 ; may not work on older versions. ⚠️  If you get a Could not find protoc error, please install the Protobuf compiler. E.g. on Ubuntu run apt-get install protobuf-compiler If you're running the Docker image you can watch the logs with docker compose logs -f Step 5: Wait for your stake to become active ​ This will happen at the beginning of the next epoch , that is, at some point during the next 20 minutes. The page will change to show that your lockup is now active: Using your portal ​ Portal serves a new version of API that's not compatible with @latest SDK and the old gateways. Update your packages with npx --yes npm-check-updates --filter "@subsquid/*" --target "@portal-api" --upgrade then freeze the versions of @portal-api packages at the new ones, either by removing any version range specifiers ( ^ , ~ , < , > , >= , <= ) preceding the package versions or by running sed -i -e 's/[\^~=<>]*\([0-9\.]*-portal-api\.[0-9a-f]\{6\}\)/\1/g' package.json to do the same thing automatically. Finally, reinstall all packages from scratch: rm -r node_modules package-lock.json npm i A working portal exposes its dataset-specific APIs via URLs such as this one: http://<host>:8000/datasets/<dataset-slug> Here, <dataset-slug> is the last path segment of the network-specific gateway URL found on this page . For example, a local portal will expose its Ethereum dataset API at http://127.0.0.1:8000/datasets/ethereum-mainnet On EVM ​ You should be able to simply replace the setGateway call with setPortal and get exactly the same behavior as before: + .setPortal('http://127.0.0.1:8000/datasets/ethereum-mainnet') - .setGateway('https://v2.archive.subsquid.io/network/ethereum-mainnet') If your squid uses RPC to ingest unfinalized blocks , @subsquid/evm-processor@portal-api will smoothly transition to that regime as it catches up to the network. On Solana ​ Solana SDK in its @portal-api version is capable of ingesting real-time data from portal, but is no longer capable of ingesting it from RPC. Since real-time Solana data is not yet available for private portals, that means that at the moment it is not possible to get real-time Solana data if the private portal is used as the main data source . This will be fixed soon, but as of 2025-03-04 this is one limitation we have. If you just want to try ingesting historical Solana data you can take a look at the portal-api branch of our Solana template: git clone https://github.com/subsquid-labs/solana-example/ cd solana-example git checkout portal-api npm i npm run build docker compose up -d npx squid-typeorm-migration apply node -r dotenv/config lib/main.js If the last command fails due to an HTTP 400 response, try raising the lowest block at described in this comment . If you want to migrate an existing squid, here is a full instruction A. Replace all exising data sources with the portal: + .setPortal('https://portal.tethys.sqd.dev/datasets/solana-beta') - .setGateway('https://v2.archive.subsquid.io/network/solana-mainnet') - .setRpc({ - client: new SolanaRpcClient({ - url: process.env.SOLANA_NODE - }) - }) Also, please remove any mentions of SolanaRpcClient , for example: - import {DataSourceBuilder, SolanaRpcClient} from '@subsquid/solana-stream' + import {DataSourceBuilder} from '@subsquid/solana-stream' B. Replace any block height literals with slot number literals. + .setBlockRange({from: 325000000}) - .setBlockRange({from: 303262650}) A convenient converter from block heights to slot numbers is TBA; for now bisecting the block range is your best bet. To get block height by slot number use this script: SLOT_NUMBER = 320276063 curl --request POST --url https://api.mainnet-beta.solana.com --header 'accept: application/json' --header 'content-type: application/json' --data "{ \" id \" : 1, \" jsonrpc \" : \" 2.0 \" , \" method \" : \" getBlock \" , \" params \" : [ ${SLOT_NUMBER} , { \" encoding \" : \" jsonParsed \" , \" maxSupportedTransactionVersion \" : 0}]}" | jq | grep blockHeight C. If you used the slot field of block headers anywhere in your code, replace it with .number : - slot: block.header.slot, + slot: block.header.number, D. If you need the block height (for example to stay compatible with your old code) request it in the .setFields call: .setFields({ block: { // block header fields timestamp: true, + height: true }, Token requirements and compute units ​ The minimum token locking requirement for a portal is 10000 SQD . This should be enough for simple user cases like running 2-3 squid or grabbing some data for analytics. If you expect heavier workloads, read on. The rate limiting mechanism of SQD Network relies on the concept of a compute unit , or CU for short. CUs do not map directly to the amount of data fetched by a portal; instead, they (roughly) represent the amount of work that the network does internally while serving the portal's requests. The definition of CU SQD Network datasets are partitioned by the block number. Dataset chunks are randomly distributed among worker nodes . When an SQD portal receives a data request, the following happens: The portal forwards the request to several workers that hold the chunks of the relevant dataset. The workers are executing the request separately on each chunk. Workers send the results back to the portal. For lightweight queries they send one response for each dataset chunk; however, if any response ends up being larger than 100 Mbytes, it's split into several parts. The portal concatenates the workers' replies and serves a continuous stream of data to the user. Each response made by any of the workers in step 3 spends exactly 1 CU. The more SQD you lock and the greater the lockup period is, the more CUs you get. Currently, each locked SQD generates 1-3 CU at the beginning of each epoch . Here's how the number of CUs per SQD changes with the lockup period: In principle, any valid amount of locked SQD s generates an infinite amount of CUs. However, if the rate at which your queries consume CUs exceeds the rate at which they are produced, your app will be throttled. To avoid that, you may want to understand how much CUs your queries spend. Currently there is no tool available for estimating the number of CUs that a query needs before actually running it. However, on EVM you can use the following formula to get an order of magnitude estimate: 10^-5 * range_length_blocks * transactions_per_block Notes: This assumes lightweight queries - that is, queries that fetch much less data than the total dataset size. For heavyweight queries multiply the estimate by a factor of 2-5. This is a rough estimate. Multiply it by ten to get a somewhat safe figure. If you want to minimize your SQD lockup, start at that safe figure, then measure the actual amount of CU you spend and reduce the lockup accordingly. If your network has an Etherscan-style explorer, you can estimate the transactions_per_block by visiting its front page, reading the "Transactions" stat and dividing it by the "Last finalized block" height. How the estimate is made For a lightweight query, the amount of CU spent on it is determined by how many dataset chunks the network needs to look at to process it. The ingester makes chunks of roughly the same size within each dataset. Since the amount of data per block is roughly proportional to the number of transactions in the said block, we can assume that the number of chunks in any given range is proportional to the number of transactions per block. Extrapolating from the Ethereum dataset, we get num_chunks_in_range = range_length * (eth_chunks / eth_height) * (chain_txs_per_block / eth_txs_per_block) Here, eth_chunks = 3.1e4 eth_height = 2.1e7 eth_txs_per_block = 1.2e2 Multiplying all the known values together and rounding to one decimal digit, we get the 1e-5 coefficient of the final formula. The implicit assumption here is that all EVM datasets have the same chunk size as Ethereum. This does not hold: the chunk sizes vary, but are generally kept within the range of 50-1000 Mbytes. Ethereum chunk size is roughly 500 Mbytes; therefore we can expect the estimate to be off by a factor of 0.5-10, which is within the "order of magnitude" definition. Heavyweight queries scale in the same way, but may spend more than one CU per chunk. The heaviest possible queries (fetching the whole dataset) on Ethereum consume roughly 5 CUs per chunk. Now, if your queries consume X CUs each and you run them once per Y epochs, you need to lock up at least this much SQD : X / (Y * boost_factor) Here, boost_factor is a multiplier ranging from 1 to 3 depending on the lockup length (see the graph above). Example ​ Suppose your query traverses the last 1M blocks of the Binance Smart Chain and you want to run it once every hour. Visiting bscscan.com we find that there's a total of 6.4B txs made over 44M blocks. We can roughly estimate the chain density at 150 txs/block. We can now estimate the cost of each query to be on the order of 10^-5 * 10^6 * 150 = 1500 CU. When ran once every 60 minutes or 3 epochs at 20 min per epoch, such queries would require roughly 1500 / 3 = 500 SQD locked up. This assumes a short lockup (under 60 days). Finally we apply the recommended margins: For lightweight queries it is 10x and the final sum amounts to 5000 SQD - less than the global minimum lockup of 10000. If you're running extremely heavy queries, the recommended margin is 50x and the final recommended lockup is 25000 SQD . Use this value as a starting point and iterate if you'd like to minimize your lockup. If you'd like to reduce the amount of SQD even further, one option is to lock the tokens up for longer: with a two-year long lockup heavy queries you'll get three times the CUs and the final recommended lockup will be 10000 even for heavy queries. High throughput portals ​ The recommended way to make a portal that can serve a large number of requests is deploy multiple portals associated with a single wallet: Use a single wallet to register as many peer IDs as you need portal instances. Make one SQD lockup for all your portal instances. See Token locking requirements and compute units to get an idea of how large your stake should be. Run portals in parallel, balancing traffic between the instances. If you plan to automate running your portal instances, you may find this helm chart useful. Troubleshooting ​ What are the consequences of losing my key file / getting it stolen? ​ If you lose your key file you won't be able to run your portal until you get a new one and register it. If your key file get stolen the perpetrator will be able to cause connectivity issues for your portal, effectively causing a downtime. If any of that happens, unregister your portal (on the "Portals" tab of network.subsquid.io ), then generate a new key file and register the new portal peer ID. My portal is slower than I expected. How do I examine its throttling stats? ​ The stats are exposed at /metrics . For a local portal, take a look at the output of curl http://127.0.0.1:8000/metrics | grep throttled The lower the value of portal_stream_throttled_ratio_sum , the better. Edit this page Previous Run a gateway Next Reference Setting up a portal Using your portal On EVM On Solana Token requirements and compute units High throughput portals Troubleshooting
addons.hasura section | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference addons.hasura section On this page Hasura add-on Running Hasura ​ To provision a Hasura instance, add an empty deploy.addons.hasura section to the deployment manifest . Provide some basic configuration: deploy : env : HASURA_GRAPHQL_ADMIN_SECRET : "${{ secrets.HASURA_SECRET }}" HASURA_GRAPHQL_UNAUTHORIZED_ROLE : user HASURA_GRAPHQL_STRINGIFY_NUMERIC_TYPES : "true" addons : postgres : hasura : Note the use of a Cloud secret for storing the admin password. Configuring a Hasura API ​ For a squid ​ Use the Hasura configuration tool for squids running dedicated Hasura instances. To make Cloud initialize Hasura configuration on squid restarts, make sure that the tool runs on squid startup by adding a deploy.init section to the manifest, e.g. like this: deploy : init : env : HASURA_GRAPHQL_ENDPOINT : 'http://hasura:8080' cmd : - npx - squid - hasura - configuration - apply See also the Hasura section of the GraphQL guide and the complete squid example . For a DipDup indexer ​ DipDup also configures Hasura automatically. See the DipDup section for details. Edit this page Previous addons.postgres section Next RPC service networks Running Hasura Configuring a Hasura API For a squid For a DipDup indexer
Sorting | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Sorting On this page Sorting Sort order ​ The sort order (ascending vs. descending) is set by specifying an ASC or DESC suffix for the column name in the orderBy input object, e.g. title_DESC . Sorting entities ​ Example: Fetch a list of videos sorted by their titles in an ascending order: query { videos ( orderBy : title_ASC ) { id title } } or query { videos ( orderBy : [ title_ASC ] ) { id title } } Sorting entities by multiple fields ​ The orderBy argument takes an array of fields to allow sorting by multiple columns. Example: Fetch a list of videos that is sorted by their titles (ascending) and then on their published date (descending): query { videos ( orderBy : [ title_ASC , publishedOn_DESC ] ) { id title publishedOn } } Edit this page Previous Pagination Next Union type resolution Sort order Sorting entities Sorting entities by multiple fields
Substrate API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Public gateways EVM API Substrate API Starknet API Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Reference Substrate API On this page Substrate SQD Network API warning The Substrate API of SQD Network is currently in beta. Breaking changes may be introduced in the future releases. SQD Network API distributes the requests over a ( potentially decentralized ) network of workers . The main gateway URL points at a router that provides URLs of workers that do the heavy lifting. Each worker has its own range of blocks on each dataset it serves. Suppose you want to retrieve an output of some query on a block range starting at firstBlock (can be the genesis block) and ending at the highest available block. Proceed as follows: Retrieve the dataset height from the router with GET /height and make sure it's above firstBlock . Save the value of firstBlock to some variable, say currentBlock . Query the router for an URL of a worker that has the data for currentBlock with GET /${currentBlock}/worker . Retrieve the data from the worker by posting the query ( POST / ), setting the "fromBlock" query field to ${currentBlock} . Parse the retrieved data to get a batch of query data plus the height of the last block available from the current worker. Take the header.number field of the last element of the retrieved JSON array - it is the height you want. Even if your query returns no data, you'll still get the block data for the last block in the range, so this procedure is safe. Set currentBlock to the height from the previous step plus one . Repeat steps 3-6 until all the required data is retrieved. Main URLs of EVM gateways are available on the Supported networks page . Implementation example: In Python def get_text ( url : str ) - > str : res = requests . get ( url ) res . raise_for_status ( ) return res . text def dump ( gateway_url : str , query : Query , first_block : int , last_block : int ) - > None : assert 0 <= first_block <= last_block query = dict ( query ) # copy query to mess with it later dataset_height = int ( get_text ( f' { gateway_url } /height' ) ) next_block = first_block last_block = min ( last_block , dataset_height ) while next_block <= last_block : worker_url = get_text ( f' { gateway_url } / { next_block } /worker' ) query [ 'fromBlock' ] = next_block query [ 'toBlock' ] = last_block res = requests . post ( worker_url , json = query ) res . raise_for_status ( ) blocks = res . json ( ) last_processed_block = blocks [ - 1 ] [ 'header' ] [ 'number' ] next_block = last_processed_block + 1 for block in blocks : print ( json . dumps ( block ) ) Full code here . Router API ​ GET /height (get height of the dataset) Example response: 20856599 . GET ${firstBlock}/worker (get a suitable worker URL) The returned worker is capable of processing POST / requests in which the "fromBlock" field is equal to ${firstBlock} . Example response: https://rb05.sqd-archive.net/worker/query/czM6Ly9wb2xrYWRvdC00 . Worker API ​ POST / (query Substrate data) Query Fields ​ type : "substrate" fromBlock : Block number to start from (inclusive). toBlock : (optional) Block number to end on (inclusive). If this is not given, the query will go on for a fixed amount of time or until it reaches the height of the dataset. includeAllBlocks : (optional) If true, SQD Network workers will include blocks that contain no data selected by data requests into their responses. fields : (optional) A selector of data fields to retrieve. Common for all data items. events : (optional) A list of event requests . calls : (optional) A list of call requests . contractsEvents : (optional) A list of Contracts.ContractEmitted event requests . evmLogs : (optional) A list of EVM.Log event requests . ethereumTransactions : (optional) A list of Ethereum.transact call requests . gearMessagesQueued : (optional) A list of Gear.MessageQueued event requests . gearUserMessagesSent : (optional) A list of Gear.UserMessageSent event requests . The response is a JSON array of per-block data items that covers a block range starting from fromBlock . The last block of the range is determined by the worker. You can find it by looking at the header.number field of the last element in the response array. The first and the last block in the range are returned even if all data requests return no data for the range. In most cases the returned range will not contain all the range requested by the user (i.e. the last block of the range will not be toBlock ). To continue, retrieve a new worker URL for blocks starting at the end of the current range plus one block and repeat the query with an updated value of fromBlock . Example Request ​ { "type" : "substrate" , "fromBlock" : 4669000 , "toBlock" : 4669010 , "fields" : { "event" : { "name" : true , "args" : true } , "call" : { "name" : true , "args" : true } } , "events" : [ { "name" : [ "Balances.Transfer" ] } ] } Run curl https://v2.archive.subsquid.io/network/ < your-network > /4669000/worker to get an URL of a worker capable of processing this query. Example Response ​ Note that the last block in the range is included despite having no matching events. [ { "header" : { "number" : 4669000 , "hash" : "0xa4667263922a1f71708993dc923b974bdece3a117538d3654f44ace403e6614f" , "parentHash" : "0x068d9e6dc7f3245df45a00a6a18ed1a07e64a53997f5f3f89e8b09c9db267b2b" } , "events" : [ ] } , { "header" : { "number" : 4669005 , "hash" : "0x7ae89bccf9d8a3fcb33b9310bff5d83aaf905099e32dd7766443c9b96143cde9" , "parentHash" : "0x36c515ee7a74db78a4ddee5734e8a79440e08bafd7e440886c7fcfd0d6389088" } , "events" : [ { "index" : 1 , "extrinsicIndex" : 1 , "callAddress" : [ ] , "name" : "Balances.Transfer" , "args" : [ "0x3a7b188d341fcd76ffdc8e684ac26c1e0720e35ca01b3f7c2308c3bde14571c2" , "0x8cdbbe675b1ea872e9f4b1d1f7258c3757b4247ef4e4d8f5d3a600d2a6dc7e59" , "378293330000" ] } ] } , { "header" : { "number" : 4669010 , "hash" : "0xbfd4448702ab2def722c6638c3d2062b7ca2cd62f71801ab467b1484bcb259a6" , "parentHash" : "0x5cb9bfcd169b9cacfa8f3b58212c151fc69e5cadf661162cd88f221888420942" } , "events" : [ ] } ] Data requests ​ Events ​ { name : string [ ] , call : boolean , stack : boolean , extrinsic : boolean } An event will be included in the response if it matches all the requests. A request with an empty array (e.g. { name: [] } ) matches no events; omit all requests/pass an empty object to match all events. See addEvent() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Calls ​ { name : string [ ] , subcalls : boolean , extrinsic : boolean , stack : boolean , events : boolean } A call will be included in the response if it matches all the requests. A request with an empty array (e.g. { name: [] } ) matches no calls; omit all requests/pass an empty object to match all calls. See addCall() SDK function reference for a detailed description of the fields of this data request; also see Field selection . Contracts.ContractEmitted events ​ info Contract addresses supplied with this data request must be hexadecimal (i.e. decoded from SS58) and lowecased. Addresses in all responses will be in the same format. { contractAddress : string [ ] , call : boolean , stack : boolean , extrinsic : boolean } A Contracts.ContractEmitted event will be included in the response if it matches all the requests. A request with an empty array (e.g. { contractAddress: [] } ) matches no events; omit all requests/pass an empty object to match all events. See addContractsContractEmitted() SDK function reference for a detailed description of the fields of this data request; also see Field selection and ink! contracts support . EVM.Log events ​ info Contract addresses supplied with this data request must be in lowercase. Addresses in all responses will be in the same format. { address : string [ ] , topic0 : string [ ] , topic1 : string [ ] , topic2 : string [ ] , topic3 : string [ ] , call : boolean , stack : boolean , extrinsic : boolean } An EVM.Log event will be included in the response if it matches all the requests. A request with an empty array (e.g. { topic0: [] } ) matches no events; omit all requests/pass an empty object to match all events. See addEvmLog() SDK function reference for a detailed description of the fields of this data request; also see Field selection and Frontier EVM support . Ethereum.transact calls ​ info Contract addresses supplied with this data request must be in lowercase. Addresses in all responses will be in the same format. { to : string [ ] , sighash : string [ ] , extrinsic : boolean , stack : boolean , events : boolean } An Ethereum.transact call will be included in the response if it matches all the requests. A request with an empty array (e.g. { sighash: [] } ) matches no calls; omit all requests/pass an empty object to match all calls. See addEthereumTransaction() SDK function reference for a detailed description of the fields of this data request; also see Field selection and Frontier EVM support . Gear.MessageQueued events ​ { programId : string [ ] , call : boolean , stack : boolean , extrinsic : boolean } A Gear.MessageQueued event will be included in the response if it matches all the requests. A request with an empty array (e.g. { programId: [] } ) matches no events; omit all requests/pass an empty object to match all events. See addGearMessageQueued() SDK function reference for a detailed description of the fields of this data request; also see Field selection and Gear support . Gear.UserMessageSent events ​ { programId : string [ ] , call : boolean , stack : boolean , extrinsic : boolean } A Gear.UserMessageSent event will be included in the response if it matches all the requests. A request with an empty array (e.g. { programId: [] } ) matches no events; omit all requests/pass an empty object to match all events. See addGearUserMessageSent() SDK function reference for a detailed description of the fields of this data request; also see Field selection and Gear support . Data fields selector ​ A JSON selector of fields for the returned data items. Documented in the Field selectors section. Edit this page Previous EVM API Next Starknet API Router API Worker API Data requests Events Calls Contracts.ContractEmitted events EVM.Log events Ethereum.transact calls Gear.MessageQueued events Gear.UserMessageSent events Data fields selector
sqd whoami | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd whoami On this page sqd whoami Show the user details for the current Cloud account sqd whoami sqd whoami â€‹ Show the user details for the current Cloud account USAGE $ sqd whoami [--interactive] FLAGS --[no-]interactive  Disable interactive mode See code: src/commands/whoami.ts Edit this page Previous sqd tags Next External tools sqd whoami
General settings | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM General settings On this page General settings tip The method documentation is also available inline and can be accessed via suggestions in most IDEs. The following setters configure the global settings of EvmBatchProcessor . They return the modified instance and can be chained. Certain configuration methods are required: one or both of setGateway() and setRpcEndpoint() setFinalityConfirmation() whenever RPC ingestion is enabled, namely when a RPC endpoint was configured with setRpcEndpoint() RPC ingestion has NOT been explicitly disabled by calling setRpcDataIngestionSettings({ disabled: true }) Here's how to choose the data sources depending on your use case: If you need real-time data and your network has a SQD Network gateway , use both setGateway() and setRpcEndpoint() . The processor will obtain as much data as is currently available from the network, then switch to ingesting recent data from the RPC endpoint. If you can tolerate your data being several thousands of blocks behind the chain head, you do not want to use a RPC endpoint and your network has a SQD Network gateway , use setGateway() only. If your EVM network does not have a SQD Network gateway, use setRpcEndpoint() only. You can use this regime to work with local development nodes . If your squid uses direct RPC queries then setRpcEndpoint() is a hard requirement. You can reduce the RPC usage by adding a Network data source with setGateway() . Further, if you can tolerate a latency of a few thousands of blocks, you can disable RPC ingestion with setRpcDataIngestionSettings({ disabled: true }) . In this scenario RPC will only be used for the queries you explicitly make in your code. setGateway(url: string | GatewaySettings) ​ Adds a SQD Network data source. The argument is either a string URL of a SQD Network gateway or { url : string // gateway URL requestTimeout ? : number // in milliseconds } See EVM gateways . setRpcEndpoint(rpc: ChainRpc) ​ Adds a RPC data source. If added, it will be used for RPC ingestion (unless explicitly disabled with setRpcDataIngestionSettings() ) any direct RPC queries you make in your squid code A node RPC endpoint can be specified as a string URL or as an object: type ChainRpc = string | { url : string // http, https, ws and wss are supported capacity ? : number // num of concurrent connections, default 10 maxBatchCallSize ? : number // default 100 rateLimit ? : number // requests per second, default is no limit requestTimeout ? : number // in milliseconds, default 30_000 headers : Record < string , string > // http headers } Setting maxBatchCallSize to 1 disables batching completely. tip We recommend using private endpoints for better performance and stability of your squids. For SQD Cloud deployments you can use the RPC addon . If you use an external private RPC, keep the endpoint URL in a Cloud secret . setDataSource(ds: {archive?: string, chain?: ChainRpc}) (deprecated) ​ Replaced by setGateway() and setRpcEndpoint() . setRpcDataIngestionSetting(settings: RpcDataIngestionSettings) ​ Specify the RPC ingestion settings. type RpcDataIngestionSettings = { disabled ? : boolean preferTraceApi ? : boolean useDebugApiForStateDiffs ? : boolean debugTraceTimeout ? : string headPollInterval ? : number newHeadTimeout ? : number } Here, disabled : Explicitly disables data ingestion from an RPC endpoint. preferTraceApi : By default, debug_traceBlockByHash is used to obtain call traces . This flag instructs the processor to utilize trace_ methods instead. This setting is only effective for finalized blocks. useDebugApiForStateDiffs : By default, trace_replayBlockTransactions is used to obtain state diffs for finalized blocks. This flag instructs the processor to utilize debug_traceBlockByHash instead. This setting is only effective for finalized blocks. WARNING: this will significantly increase the amount of data retrieved from the RPC endpoint. Expect download rates in the megabytes per second range. debugTraceTimeout : If set, the processor will pass the timeout parameter to debug trace config . headPollInterval : Poll interval for new blocks in milliseconds. Poll mechanism is used to get new blocks via HTTP connections. Default: 5000. newHeadTimeout : When ingesting from a websocket, this setting specifies the timeout in milliseconds after which the connection will be reset and subscription re-initiated if no new blocks were received. Default: no timeout. setFinalityConfirmation(nBlocks: number) ​ Sets the number of blocks after which the processor will consider the consensus data final. Use a value appropriate for your network. For example, for Ethereum mainnet a widely cited value is 15 minutes/75 blocks. setBlockRange({from: number, to?: number}) ​ Limits the range of blocks to be processed. When the upper bound is specified, processor will terminate with exit code 0 once it reaches it. Note that block ranges can also be specified separately for each data request. This method sets global bounds for all block ranges in the configuration. includeAllBlocks(range?: {from: number, to?: number}) ​ By default, processor will fetch only blocks which contain requested items. This method modifies such behavior to fetch all chain blocks. Optionally a range of blocks can be specified for which the setting should be effective. setPrometheusPort(port: string | number) ​ Sets the port for a built-in prometheus health metrics server (serving at http://localhost:${port}/metrics ). By default, the value of PROMETHEUS_PORT environment variable is used. When it is not set, processor will pick an ephemeral port. Edit this page Previous Block data for EVM Next Event logs setGateway(url: string | GatewaySettings) setRpcEndpoint(rpc: ChainRpc) setDataSource(ds: {archive?: string, chain?: ChainRpc}) (deprecated) setRpcDataIngestionSetting(settings: RpcDataIngestionSettings) setFinalityConfirmation(nBlocks: number) setBlockRange({from: number, to?: number}) includeAllBlocks(range?: {from: number, to?: number}) setPrometheusPort(port: string | number)
SQD Portal Closed Beta Instructions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Portal Closed Beta Instructions On this page SQD Portal Closed Beta Instructions Welcome to the SQD Portal Closed Beta! We appreciate your interest. Below, you’ll find clear instructions on what to do next and what to expect. What is the SQD Portal? ​ The SQD Portal is a decentralized, streaming-based data retrieval solution designed to replace our centralized archives. It provides faster, more reliable, and flexible access to blockchain data. Key Features ​ Fully Decentralized : Powered by 1,600+ independent worker nodes. 30x Replication : Redundant data storage for maximum reliability, capable of querying up to 20 million blocks per second . Faster Performance : A new Rust-based query engine now leverages parallelized queries and incorporates numerous query execution performance optimizations, delivering an overall 10-50x performance boost compared to the centralized Archives. What You Can Do ​ 1. Explore the Public Portal ​ Access data for 100+ EVM networks during the closed beta. Public Portal supports filtering only by logs at the moment. We aim to include all full filtering capabilities in the general availability release ( reference ) and other VMs as well. Note: The current Public Portal rate limit is 20 requests per 10 seconds. Login here : https://portal-ui.sqd.dev/ Username : sqd Password : portal2025 2. Migrate to the Cloud Portal ​ Transition from centralized archives to the Cloud Portal. Start here : Cloud Portal Migration Details Get $100 in SQD Cloud credits : Redeem by contacting us on Telegram after migrating your squid/s by the 31st of December 2024. 3. Set Up a Self-Hosted Portal ​ Take full control of your data infrastructure by running your own Portal. Setup Guide : Self-Hosting Instructions Requirements Minimum 10,000 SQD tokens. A working Docker installation. Some Arbitrum ETH for gas. If you have any questions or feedback for us, please reach out to us on Telegram. We have created a special group for Portal Closed Beta participants: https://t.me/+JHrJZPz34kRjYmFk . Thank you for being part of the beta! — The SQD Team Edit this page What is the SQD Portal? What You Can Do 1. Explore the Public Portal 2. Migrate to the Cloud Portal 3. Set Up a Self-Hosted Portal
General settings | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource General settings On this page General settings tip The method documentation is also available inline and can be accessed via suggestions in most IDEs. The following setters configure the global settings of DataSourceBuilder for Fuel Procesor. They return the modified instance and can be chained. The only required configuration method is setGateway() . If you need real-time data, please also use setGraphql() . If you add both a SQD Network gateway and an RPC endpoint, the processor will obtain as much data as is currently available from the gateway, then switch to ingesting recent data via RPC. If you only add a SQD Network gateway, your data will be being several thousands of blocks behind the chain head most of the time. setGateway(url: string | GatewaySettings) ​ Use a SQD Network gateway. The argument is either a string URL of the gateway or { url : string // gateway URL requestTimeout ? : number // in milliseconds } setGraphql(settings?: GraphqlSettings) ​ We must use regular GraphQL endpoint to get through the last mile and stay on top of the chain. This is a limitation, and we promise to lift it in the future. type GraphqlSettings = { url : string // e.g. https://mainnet.fuel.network/v1/graphql strideSize ? : number ; // `getBlock` batch call size, default 5 strideConcurrency ? : number ; // num of concurrent connections, default 10 } ; setBlockRange({from: number, to?: number}) ​ Limits the range of blocks to be processed. When the upper bound is specified, processor will terminate with exit code 0 once it reaches it. Note that block ranges can also be specified separately for each data request. This method sets global bounds for all block ranges in the configuration. includeAllBlocks(range?: {from: number, to?: number}) ​ By default, processor will fetch only blocks which contain requested items. This method modifies such behavior to fetch all chain blocks. Optionally a range of blocks can be specified for which the setting should be effective. Edit this page Previous Block data for Fuel Network Next Inputs setGateway(url: string | GatewaySettings) setGraphql(settings?: GraphqlSettings) setBlockRange({from: number, to?: number}) includeAllBlocks(range?: {from: number, to?: number})
Generators of code for tech-specific tasks such as decoding and querying chains directly | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Generating utility modules Data decoding Direct RPC queries Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools Type-safe decoding Generators of code for tech-specific tasks such as decoding and querying chains directly üìÑÔ∏è Generating utility modules Task-specific code generation üìÑÔ∏è Data decoding Using the generated modules to decode data üìÑÔ∏è Direct RPC queries Get chain state via wrapped RPC calls Previous TypeORM model generation Next Generating utility modules
Monitoring | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Monitoring On this page Monitoring Each deployed squid version exposes Prometheus metrics for external monitoring with e.g. Grafana. Processor metrics ​ The processor metrics are available at https://${org}.squids.live/${name}@${slot}/processors/${processor}/metrics , and at https://${org}.squids.live/${name}:${tag}/processors/${processor}/metrics for each tag attached to the slot. See the slots and tags guide . ${processor} here is the processor name; it defaults to processor unless specified. The metrics are documented inline. They include some values reflecting the squid health: sqd_processor_last_block . The last processed block. sqd_processor_chain_height . Current chain height as reported by the RPC endpoint (when RPC ingestion is enabled) or by SQD Network (when it is disabled). Inspect the metrics endpoint for a full list. Postgres metrics ​ Postgres metrics will be available in the future SQD Cloud releases. API metrics ​ API metrics will be available in the future SQD Cloud releases. Edit this page Previous Inspect logs Next Organizations Processor metrics Postgres metrics API metrics
ApeWorx plugin | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions ApeWorx plugin On this page ApeWorx SQD plugin info The subsquid ApeWorx plugin is currently in beta. Please report any bugs or suggestions to Squid Devs or open an issue at the GitHub repo of the plugin. ApeWorx is a modular Web3 development framework for Python programmers. Among other things, it is capable of retrieving blockchain data in bulk . The data can come from various sources, including SQD Network. The network provides free access to blocks and event logs data. On long block ranges (>1k blocks) data retrieval is orders of magnitude (>10x) faster compared to RPC-based data sources. Basic usage ​ In an existing ApeWorx installation , run the following to install the subsquid plugin: ape plugins install "ape-subsquid@git+https://github.com/subsquid/ape-subsquid.git@main" To source data from SQD Network, start ape console as usual, e.g. with ape console --network ethereum:mainnet:geth then pass engine_to_use = 'subsquid' to your data query methods. You can speed up the following calls: Bulk block data retrieval with chain.blocks.query : df = chain . blocks . query ( '*' , start_block = 18_000_000 , stop_block = 19_000_000 , engine_to_use = 'subsquid' ) This query retrieves data on 1M blocks in about 11 minutes. Contract events retrieval: contract = Contract ( '0xdac17f958d2ee523a2206206994597c13d831ec7' , abi = 'usdt.json' ) df = contract . Transfer . query ( '*' , start_block = 18_000_000 , stop_block = 18_100_000 , engine_to_use = 'subsquid' ) This query retrieves 1.6M events emitted over 100k block in about 17 minutes. warning At the moment, all SQD Network datasets are updated only once every several thousands of blocks. The current dataset height can be retrieved with get_network_height() : from ape_subsquid import get_network_height get_network_height ( ) # currently 19212115 while the chain is at 19213330 Queries that request blocks above the current dataset height will fail with an ape_subsquid.exceptions.DataRangeIsNotAvailable exception. That includes queries without an explicitly set stop_block . If you don't need the recent data, you can explicitly request the data up to the block height of the SQD dataset, e.g. subsquid_height = get_network_height ( ) df = chain . blocks . query ( '*' , start_block = 19_000_000 , stop_block = subsquid_height , engine_to_use = 'subsquid' ) To get the latest data, retrieve the tail with the default engine and append it to the dataframe: taildf = chain . blocks . query ( '*' , start_block = subsquid_height + 1 ) df = pd . concat ( [ df , taildf ] , ignore_index = True , copy = False ) info When working with block ranges much longer than 1M blocks, the plugin may occasionally fail due to HTTP 503 errors rarely returned by the network. If you encounter this issue, split your block range into sub-1M blocks intervals and retrieve the data for each interval separately, retrying when the queries throw exceptions. Networks support ​ The following networks available via plugins from ape plugins list --all are supported: arbitrum avalanche base bsc ethereum fantom optimism polygon polygon-zkevm Extra functionality ​ Low-level queries ​ The plugin supports the following low-level queries : BlockQuery ContractEventQuery ContractCreationQuery (only for networks with traces support ) AccountTransactionQuery The following queries are NOT supported: BlockTransactionQuery : uses hashes instead of numbers to identify blocks. This is not supported by SQD Network. ContractMethodQuery : not used by any high-level methods at the moment. If you need it, let us know at the Squid Devs channel . Account history retrieval ​ The plugin can be used to retrieve historical data for individual accounts, although this is much slower than working with specialized APIs such as Etherscan. accountHistory = chain . history . _get_account_history ( 'vitalik.eth' ) accountHistory . query ( 'value' , engine_to_use = 'subsquid' ) Edit this page Previous SQD Firehose Next Squid CLI Basic usage Networks support Extra functionality Low-level queries Account history retrieval
SubstrateBatchProcessor reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Substrate Block data for Substrate General settings Data requests Fields selection Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors Substrate SubstrateBatchProcessor reference documentation üìÑÔ∏è Block data for Substrate Block data for Substrate üìÑÔ∏è General settings Data sourcing and metrics üìÑÔ∏è Data requests Subscribe to events and calls üìÑÔ∏è Fields selection Fine-tuning data requests with setFields() Previous Field selection Next Block data for Substrate
Migrate to the Cloud portal | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Migrate to the Cloud portal On this page Migrate to the Cloud portal info SQD Network portals are currently in closed beta. Please report any bugs or suggestions to the SQD Portal Closed Beta Portal chat or to Squid Devs . This guide walks you through replacing a gateway of the open private version of SQD Network with a portal of the permissionless SQD Network as the primary source of data for your Cloud squids. SQD team operates two independent portals that serve the needs of Cloud users: The dedicated Cloud Portal is only visible from within the Cloud, ensuring stable performance for squids deployed there. The Public Portal can be accessed from anywhere for easy experimentation and local development. info We're currently experimenting with tightening the data request complexity limits that may become a part of this release. If you see an HTTP 400 error with a message like this: Couldn't parse query: query contains X item requests, but only 50 is allowed where X is some number above 50, or any other HTTP 400 response, please let us know. Here are the steps to migrate: Step 1 ​ Navigate to your squid's folder and install the portal-api version of the processor package: EVM Substrate npm i @subsquid/evm-processor@portal-api npm i @subsquid/substrate-processor@portal-api Step 2 ​ Follow the network-specific instructions from the Cloud: enable the preview mode navigate to the Portal's page click on the tile of your network to see the instructions Once you're done, your squid will use the Cloud Portal when deployed and the public portal for local runs. Step 3 ​ Evaluate the performance of your new data source by re-syncing your squid. Follow the zero-downtime update procedure: Deploy your squid into a new slot. Wait for it to sync, observing the improved data fetching. Assign your production tag to the new deployment to redirect the GraphQL requests there. See this section for details. Edit this page Step 1 Step 2 Step 3
TypeORM model generation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools TypeORM model generation TypeORM model generation TypeORM entities can be automatically generated from the schema file defined in schema.graphql .
The tool is called squid-typeorm-codegen(1) and has no additonal options. Install with npm i @subsquid/typeorm-codegen --save-dev Invoke with npx squid-typeorm-codegen Edit this page Previous TypeORM migration generation Next Type-safe decoding
FAQ | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK FAQ On this page FAQ What are some real-world applications for which Squid SDK was a good fit? ​ Here is an incomplete list: DeFi dashboards, tracking addresses and internal transactions NFT marketplaces, with a dynamic sets of NFT contracts to watch Historical price feeds, tracking Uniswap trades and Chainlink oracle contracts Mining smart contract deployments and the bytecode Real-time bots (<1sec delay) triggered by on-chain activity How does Squid SDK handle unfinalized blocks? ​ The SQD Network only serves finalized blocks and is typically ~1000 blocks behind the tip. The most recent blocks, as well as the unfinalized blocks are seamlessly handled by the SDK from a complementary RPC data source, set by the chain config. Potential chain reorgs are automatically handled under the hood. See Indexing unfinalized blocks for details. What is the latency for the data served by the squid? ​ Since the ArrowSquid release, the Squid SDK has the option to ingest unfinalized blocks directly from an RPC endpoint, making the indexing real-time. How do I enable GraphQL subscriptions for local runs? ​ Add --subscription flag to the serve command defined in commands.json . See Subscriptions for details. How do squids keep track of their sync progress? ​ Depends on the data sink used. Squid processors that use TypeormDatabase keep their state in a schema , not in a table. By default the schema is called squid_processor (name must be overridden in multiprocessor squids ). You can view it with select * from squid_processor . status ; and manually drop it with drop schema squid_processor cascade ; to reset the processor status. Squids that store their data in file-based datasets store their status in status.txt by default. This can be overridden by defining custom database hooks . Is there a healthcheck endpoint for the indexer? ​ Yes, the processor exposes the key prometheus metrics at the ${process.env.PROMETHEUS_PORT}/metric endpoint. The squids deployed to the SQD Cloud also publicly explose the metrics, see Monitoring in the Cloud Do squids have a debug mode? ​ Yes. To see all debug messages, set the SQD_DEBUG anv variable to * : .env SQD_DEBUG = * You can also enable debug messages just for a specific namespace. For example, .env SQD_DEBUG = sqd:processor:archive will show only the messages related to your squid's queries to the SQD Network. Edit this page Previous Examples Next SQD vs The Graph What are some real-world applications for which Squid SDK was a good fit? How does Squid SDK handle unfinalized blocks? What is the latency for the data served by the squid? How do I enable GraphQL subscriptions for local runs? How do squids keep track of their sync progress? Is there a healthcheck endpoint for the indexer? Do squids have a debug mode?
Serving GraphQL | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Serving GraphQL On this page Serving GraphQL It is common (although not required) for squids to serve GraphQL APIs. Historically, the most common way to do that was to persist the squid data to PostgreSQL , then attach OpenReader to it. Although this is still supported, we encourage using PostGraphile or Hasura in new PostgreSQD-based projects. See OpenReader's limitations if you're curious about our motivation. PostGraphile ​ PostGraphile is an open-source tool that builds powerful, extensible and performant GraphQL APIs from PostgreSQL schemas. Its pros include: aggregations; reliable support for subscriptions; capability for deep API customization; organization of API customization code into plugins. The recommended way of integrating PostGraphile into squid projects is by making a dedicated entry point at src/api.ts . A complete example squid implementing this approach is available in this repository . With this entry point in place, we create a sqd command for running PostGraphile with commands.json , then use it in the deploy.api entry of Squid manifest . Although none of this is required, this makes it easier to run the squid both locally (with sqd run ) and in the Cloud . As per usual with PostGraphile installations, you can freely extend it with plugins, including your own. Here is an example plugin for serving the _squidStatus queries from the standard Squid SDK GraphQL server schema. A plugin for making PostGraphile API fully compatible with old APIs served by OpenReader will be made available soon. Hasura ​ Hasura is a powerful open-source GraphQL engine. You can use it to: expose multiple data sources of different kinds (various databases, APIs etc) via a single API; reliably serve subscriptions; perform aggregations; deeply customize your CRUD API. You can integrate Hasura with your squid in two ways: Use Hasura to gather data from multiple sources, including your squid. For this scenario we recommend separating your Hasura instance from your squid, which should consist of just one service, the processor , plus the database. Supply your database credentials to Hasura, then configure it to produce the desired API. If you run your squid in our Cloud you can find database credentials in the app : Run a dedicated Hasura instance for serving the data just from your squid. A complete example implementing this approach is available in this repository . Here's how it works: Locally, Hasura runs in a Docker container . In the Cloud it is managed via the Hasura addon . Hasura metadata is shared among all squid instances by means of the Hasura configuration tool . The tool can automatically create an initial configuration based on your TypeORM models , then persist any changes you might make with the web GUI and metadata exports. Admin authentication secret is set via the HASURA_GRAPHQL_ADMIN_SECRET . The variable is set in .env locally and from a secret in Cloud deployments. See the configuration tool page and the repo readme for more details. OpenReader ​ OpenReader is a GraphQL server developed by the SQD team. Although still supported, it's not recommeded for new PostgreSQL-powered projects due to its limitations , especially for APIs implementing GraphQL subscriptions. The server uses the schema file to produce its core API that can be extended with custom resolvers . Extra features include DoS protection and caching . Edit this page Previous Multichain Next Self-hosting PostGraphile Hasura OpenReader
How to Start with Solana Indexing SDK | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start Indexing Orca Whirlpool Cheatsheet SDK Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing How to start How to Start with Solana Indexing SDK üìÑÔ∏è Indexing Orca Whirlpool Indexing Orca Whirlpool transactions on Solana üìÑÔ∏è Cheatsheet Commonly used CLI commands Previous Solana indexing Next Indexing Orca Whirlpool
Frontier EVM support | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific ink! contracts support Frontier EVM support Gear support Substrate data sourcing Substrate types bundles Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Substrate-specific Frontier EVM support On this page Frontier EVM support tip Method documentation is also available inline and can be accessed via suggestions in most IDEs. info Check out the Caveats section of this page to avoid common issues. This section describes additional options available for Substrate chains with the Frontier EVM pallet like Astar. On ArrowSquid, use the frontier-evm template as the starting point. The overall approach is to use the SubstrateBatchProcessor methods addEvmLog() and addEthereumTransaction() to request the required EVM data. The data can then be converted into a standard EVM format using the utils from the frontier package and decoded with utility classes generated by squid-evm-typegen . Contract state can also be accessed via wrapped RPC queries using the classes provided by the squid-evm-typegen tool . Examples ​ Subscribe to event logs ​ const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/astar-substrate' ) . setRpcEndpoint ( 'https://astar-rpc.dwellir.com' ) . addEvmLog ( { address : [ '0xb654611f84a8dc429ba3cb4fda9fad236c505a1a' , '0x6a2d262d56735dba19dd70682b39f6be9a931d98' ] , topic0 : [ erc721 . events . Transfer . topic ] , extrinsic : true } ) . setFields ( { event : { phase : true } , extrinsics : { hash : true } } ) Request all EVM calls to two contracts ​ processor . addEthereumTransaction ( { to : [ '0x6a2d262d56735dba19dd70682b39f6be9a931d98' '0x3795c36e7d12a8c252a20c5a7b455f7c57b60283' ] } ) Request all transfer(address,uint256) EVM calls on the network: ​ processor . addEthereumTransaction ( { sighash : [ '0xa9059cbb' ] } ) Parse events and transactions ​ const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/astar-substrate' ) . setRpcEndpoint ( 'https://astar-rpc.dwellir.com' ) . addEthereumTransaction ( { } ) . addEvmLog ( { } ) processor . run ( new TypeormDatabase ( ) , async ctx => { for ( const block of ctx . blocks ) { for ( const event of block . events ) { if ( event . name === 'EVM.Log' ) { // no need to supply any extra data to determine // the runtime version: event has all necessary references const { address , data , topics } = getEvmLog ( event ) // process evm log data } } for ( const call of block . calls ) { if ( call . name === 'Ethereum.transact' ) { const txn = getTransaction ( call ) // process evm txn data } } } } ) Access contract state ​ // ... const CONTRACT_ADDRESS = "0xb654611f84a8dc429ba3cb4fda9fad236c505a1a" processor . run ( new TypeormDatabase ( ) , async ctx => { for ( const block of ctx . blocks ) { for ( const event of block . events ) { if ( event . name === "EVM.Log" ) { const contract = new erc721 . Contract ( ctx , block , CONTRACT_ADDRESS ) ; // query the contract state const uri = await contract . tokenURI ( 1137 ) } } } } ) Factory contracts ​ It some cases the set of contracts to be indexed by the squid is not known in advance. For example, a DEX contract typically creates a new contract for each trading pair added, and each such trading contract is of interest. While the set of handler subscriptions is static and defined at the processor creation, one can leverage the wildcard subscriptions and filter the contracts of interest in runtime. This pattern is described extensively in EVM documentation, but it can be used with EVM methods of SubstrateBatchProcessor as well. A (somewhat outdated) example is available in this archive repo . Caveats ​ If your use case does not require any Substrate-specific data (e.g. extrinsic hashes), use EvmBatchProcessor instead. EVM-only SQD Network gateways are available for all major EVM-on-Substrate chains. Passing [] as a set of parameter values selects no data . Pass undefined for a wildcard selection: . addEvmLog ( { address : [ ] } ) // selects no events . addEvmLog ( { } ) // selects all events If contract address(es) supplied to the processor configuration methods are stored in any wide-scope variables, it is recommended to convert them to flat lower case. This precaution is necessary because same variable(s) are often reused in the batch handler for item filtration, and all contract addresses in the items are always in flat lower case. Edit this page Previous ink! contracts support Next Gear support Examples Factory contracts Caveats
sqd restart | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd restart On this page sqd restart Restart a squid deployed to the Cloud sqd restart sqd restart â€‹ Restart a squid deployed to the Cloud USAGE $ sqd restart [--interactive] [-r [<org>/]<name>(@<slot>|:<tag>) | -o <code> | [-s <slot> -n <name>] | [-t <tag> ]] FLAGS --[no-]interactive  Disable interactive mode SQUID FLAGS -n, --name=<name>                               Name of the squid -r, --reference=[<org>/]<name>(@<slot>|:<tag>)  Fully qualified reference of the squid. It can include the organization, name, slot, or tag -s, --slot=<slot>                               Slot of the squid -t, --tag=<tag>                                 Tag of the squid ORG FLAGS -o, --org=<code>  Code of the organization See code: src/commands/restart.ts Edit this page Previous sqd remove Next sqd run sqd restart
Troubleshooting | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Troubleshooting On this page Troubleshooting "Secrets outdated. Please restart the squid" warning ​ This occurs when you have a squid deployed, then create, remove or change some secrets of relevance . Squids must be restarted manually for such changes to have effect. Navigate to the squid version page (e.g. by clicking on the warning sign) and click restart. The restart will not touch the database, so unless your new secret values cause the squid to crash this procedure should be quick and easy. My squid is stuck in "Building", "Deploying" or "Starting" state ​ Run with SQD_DEBUG=* as explained on the Logging page Update the squid CLI to the latest version with npm update -g @subsquid/cli Update the Squid SDK dependencies: npm run update Check that the squid adheres to the expected structure Make sure you can build and run Docker images locally Validation error when releasing a squid ​ Make sure the squid name contains only alphanumeric characters, underscores and hyphens. The squid version must be also alphanumeric.
Since both the squid and version name become part of the squid API endpoint URL, slashes and dots are not accepted. My squid ran out of disk space ​ Edit the postgres addon section of squid.yaml and request more space for the database. My squid is behind the chain, but is shows that it is in sync ​ Check that your processor uses both a RPC endpoint as one of its data sources (in addition to a SQD Network gateway). Edit this page Previous Pricing Next Resources "Secrets outdated. Please restart the squid" warning My squid is stuck in "Building", "Deploying" or "Starting" state Validation error when releasing a squid My squid ran out of disk space My squid is behind the chain, but is shows that it is in sync
Entities | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file Schema file and codegen Entities Indexes and constraints Entity relations Unions and typed JSON Interfaces OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Schema file Entities On this page Entities Entities are defined by root-level GraphQL types decorated with @entity . Names and properties of entities are expected to be camelCased. They are converted into snake_case for use as the corresponding database table and column names. The primary key column is always mapped to the entity field of a special ID type mapped as string ( varchar ). Non-nullable fields are marked with an exclamation mark ( ! ) and are nullable otherwise. The following scalar types are supported by the schema.graphql dialect: String (mapped to text ) Int (mapped to int4 ) Float (mapped to numeric , ts type number ) Boolean (mapped to bool ) DateTime (mapped to timestamptz , ts type Date ) BigInt (mapped to numeric , ts type bigint ) BigDecimal (mapped to numeric , ts type BigDecimal of @subsquid/big-decimal ) Bytes (mapped to bytea , ts type UInt8Array ) JSON (mapped to jsonb , ts type unknown ) Enums (mapped to text ) User-defined scalars (non-entity types). Such properties are mapped as jsonb columns. Example type Scalar @entity { id : ID ! boolean : Boolean string : String enum : Enum bigint : BigInt dateTime : DateTime bytes : Bytes json : JSON deep : DeepScalar } type DeepScalar { bigint : BigInt dateTime : DateTime bytes : Bytes boolean : Boolean } enum Enum { A B C } Arrays â€‹ An entity field can be an array of any scalar type except BigInt and BigDecimal . It will be mapped to the corresponding Postgres array type. Array elements may be defined as nullable or non-nullable. Example type Lists @entity { id : ID ! intArray : [ Int ! ] ! enumArray : [ Enum ! ] datetimeArray : [ DateTime ! ] bytesArray : [ Bytes ! ] listOfListsOfInt : [ [ Int ] ] listOfJsonObjects : [ Foo ! ] } enum Enum { A B C D E F } type Foo { foo : Int bar : Int } Edit this page Previous Schema file and codegen Next Indexes and constraints Arrays
SQD Cloud deployments, now upgraded! | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud deployments, now upgraded! On this page SQD Cloud deployments, now upgraded! What's new? ​ Over the last 6 months we've seen hundreds of teams deploy thousands of squids to SQD Cloud . Many of you have given us feedback that you'd like to see a more flexible and collaborative set of tools for your deployment flow. We hear you. We agree. So today we're introducing two new features: Slots: Forget incrementing a single version number! Versions will be retired and replaced by slots, a customisable identifier for each deployment. Tags: Remembering which squid is which just got a lot easier! Now you can create and assign multiple custom tags to any deployment. As a result we'll be retiring the Production alias feature. Now tags act as aliases, making it possible to create, identify and consume endpoints. I’ll read the story later, let me see what I can do with it! Why the change? ​ When we initially built the original deployment flow versions felt appropriate. A simple number you could increment on each deployment if necessary. This worked well in simple scenarios, but over time this gradually became more difficult to manage for those of you with any kind of deployment complexity. At the same time it became clear that our solution to endpoint management, allowing you to mark a squid as production and create a permalink, was equally powerful in basic scenarios but limited as requirements grew. It became clear that was lacking here was the ability for larger teams to collaborate effectively. Our goal with these changes is to provide the power and flexibility that more complex deployment flows need. How does it work? ​ Let's take an example, we want to create and test a squid, and then over time gradually promote it to production. In our old workflow we can either keep redeploying over our v1 squid as we develop or eventually deploy v2 and v3 etc, which could be confusing as they may not be actually better! This means we need to spend time manually tracking what each version contains. Not ideal! Now let's walk through our new workflow with a common example! Let's start with the first developer on the team deploying a squid called ethprice into a slot of our choosing 4f5sfc . Here's the (shortened) manifest file they configure: .. . name: ethprice slot: 4f5sfc .. . And now deploying it to the cloud: sqd deploy . Next, let's imagine they tag the cloud deployment as test . sqd tags add test -n ethprice -s 4f5sfc Finally, we might imagine a second developer comes along and redeploys the squid to a new slot tagging it as staging . sqd deploy . -s 6x5efr --add-tag staging If the team then took a look at SQD cloud they'd see: At a glance they can see what each deployment contains and the stage of development is at. The tagging system can be used for any custom workflow that your team needs. We can't wait to see how you use it! FAQs ​ How do I get full access to the new features? ​ Just update your Squid CLI to >=3.0.0 . It's in @latest : npm install -g @subsquid/cli@latest Do I need to redeploy anything? ​ No. But we suggest you take a look at the Backwards compatibility section in our changelog. We really like versions, they work well for us, do we need to change our workflow? ​ No, you don't need to change your workflow at all! Your version will be migrated into the new slot field, and slots are a just an arbitrary string between 2 and 6 characters in length. Our goal was to expand functionality rather than remove it. Do I need to use tags? ​ No, they are completely optional, and purely to identify and alias the squids endpoint. Each deployment can always be consumed from it's immutable endpoint, which is based on it's slot. Why should I use tags? Convince me. ​ Tags serve two key roles. Firstly as a labelling mechanism, this allows teams to explicitly describe what each squid does and what stage it's at in development. Secondly as a way to preserve urls across slots. Tags act as an alias by creating a unqiue endpoint for each tag. This allows you to migrate a consistent endpoint across slots with ease. Did any of my endpoints change on this release? ​ No. Every endpoint remains unchanged. Where can I learn more about these updates? ​ Learn how to use these new features here . To read about the indepth details of the changes please head here . We'd love to hear your feedback on these changes. Tag us on twitter @helloSQD or come talk to us in Telegram . Edit this page What's new? Why the change? How does it work? FAQs
Deployment workflow | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Deployment workflow On this page Deployment workflow Here we show how to deploy a production-ready indexer ("squid") to SQD Cloud. See Development flow and the rest of the Indexing SDK section for more info on how to develop one of these. Pre-requisites: Docker, Squid CLI 1. Prepare the squid for deployment ​ Make sure that the squid is ready for deployment. This includes: Verifying that the squid is working as expected locally. Ensuring that the squid is free of any major performance issues. See Best practices for guidance. Updating the squid.yaml file (a.k.a. "deployment manifest") with the correct values for your use case. Setting up SQD Cloud secrets if necessary. 2. Register a SQD account ​ You can register a SQD account by visiting the SQD cloud . Click Create an account and fill in the required information, or sign in with your GitHub or Google account. 3. Create a professional organization ​ Once you're logged in, create a new organization and upgrade it to professional by attaching a valid credit card in the "Billing" tab. You can skip this step, but then you will only be able to deploy to your account's playground . Playground squids are a trial / development solution with limited functionality; they are not suitable for use in production. 4. Edit the squid.yaml file ​ 4.1. Basic configuration ​ First, set squid name, description, and other metadata in the header section of the squid.yaml file. If necessary, configure your build options next. If the defaults - NodeJS v20 and npm for package manager - work for you, just create an empty build section. Finally, edit the deploy section to specify the deployment options. The resulting configuration file may look like this: manifest_version : subsquid.io/v0.1 name : sample - squid build : deploy : processor : cmd : [ "sqd" , "process:prod" ] api : cmd : [ "sqd" , "serve:prod" ] Note that the processor and api commands are commands.json -based. This is optional. 4.2. Using addons ​ SQD Cloud addons enable extra services for your squid. Enable them in the deploy.addons section. rpc addon ​ For real time data you can use the rpc addon . First, open the RPC endpoints tab in the SQD cloud sidebar and copy the URL of the chosen endpoint. Add it to the .env file: RPC_ARBITRUM_ONE_HTTP = < endpoint-url > This allows using the Cloud RPC in local runs. To make the same endpoint available in Cloud, enable the addon in the addons section: deploy : addons : rpc : - arbitrum - one.http To use this endpoint in your squid, set the RPC endpoint like so in src/main.ts : import { assertNotNull } from '@subsquid/util-internal' export const processor = new EvmBatchProcessor ( ) . setRpcEndpoint ( assertNotNull ( process . env . RPC_ARBITRUM_ONE_HTTP , 'Required env variable RPC_ARBITRUM_ONE_HTTP is missing' ) ) ; // ...the rest of the processor configuration This configuration will use the Arbitrum RPC endpoint provided by SQD. postgres addon ​ For squids that write their data to PostgreSQL use the postgres addon : deploy : addons : postgres : You should also configure the scale section for the addon when deploying to production, e.g.: scale : addons : postgres : storage : 100G profile : medium hasura addon ​ If your squid uses a dedicated Hasura instance , configure the hasura addon to supply it. 4.3. API scale ​ Squids come with a GraphQL service out-of-the-box. You can enable or disable the service by adding or removing the deploy.api section of the squid.yaml file. In the scale.api section you can also set the scale and number of replicas. deploy : api : cmd : [ "sqd" , "serve:prod" ] scale : api : profile : large # load-balance three replicas replicas : 3 This approach works if you use OpenReader (the default in most current examples) or PostGraphile . If your squid uses a dedicated Hasura instance for its GraphQL needs, consult the hasura addon page . 4.4. Processor scale ​ Next, set the scale of the indexer processor. See this reference section to learn about your options. scale : processor : profile : medium 4.5. Dedicated deployment ​ Dedicated deployment profile is now the default, so if it isn't explicitly disabled in your manifest you don't need to do anything. However, if you have a dedicated: false in your scale: section (e.g. because you previously deployed your squid to a playground ) you must either erase that or replace it with an explicit scale : dedicated : true We strongly discourage using collocated squids in production, so be sure to check. 4.6. The resulting squid.yaml ​ Here is an example of a squid.yaml file with all the options set: manifest_version : subsquid.io/v0.1 name : sample - squid build : deploy : addons : postgres : rpc : - arbitrum - one.http processor : cmd : [ "sqd" , "process:prod" ] api : cmd : [ "sqd" , "serve:prod" ] scale : addons : postgres : storage : 100G profile : medium processor : profile : medium api : profile : large # load-balance three replicas replicas : 3 For all deployment options, check out the deployment manifest page. 5. Set any required secrets ​ If your squid uses any sensitive data such as a private URL or an access key, you need to store it in a SQD Cloud secret . You can do this by going to the Secrets tab in the SQD cloud sidebar and adding the required values. Alternatively, use sqd secrets . 6. Deploy the squid ​ To deploy the squid to the cloud, open Squids in the sidebar and press the Deploy a squid button in the SQD cloud. You will be prompted to install the Squid CLI if you haven't already. Follow the instructions to install the CLI.
Next, set up your auth key as shown in the SQD cloud.
Type the squid name to be the same as in the squid.yaml file. Finally, deploy the squid: sqd deploy < path_to_squid_project_root > 7. Monitor the squid ​ tip Take a look at logging page for tips on emitting and reading logs. After deploying the squid, you can monitor its status in SQD Cloud. You can see the logs, metrics, and other information about the squid in the Cloud dashboard. Open the monitoring tab in the SQD cloud sidebar to see the status of your squid. Deployed quids are available in the Squid tab. You can see memory usage, CPU usage, and other metrics in the monitoring tab. Here is an example of the monitoring tab: 8. Use the squid ​ If your squid uses a database, you'll have direct access. Take a look at the DB access tab of your squid's card in SQD Cloud console. Squid deployments are organized with slots and tags . This structure enables a rich variety of workflows, notably including zero downtime updates of squid GraphQL APIs. We recommend that you use this workflow for all production squids. What's next? ​ Take a look at our pricing . Learn more about organizations and the slots and tags system . Browse the sqd CLI documentation to learn how to perform common squid maintenance tasks. create or update a deployment tag a deployment kill a deployment Edit this page Previous SQD Cloud Next Pricing 1. Prepare the squid for deployment 2. Register a SQD account 3. Create a professional organization 4. Edit the squid.yaml file 4.1. Basic configuration 4.2. Using addons 4.3. API scale 4.4. Processor scale 4.5. Dedicated deployment 4.6. The resulting squid.yaml 5. Set any required secrets 6. Deploy the squid 7. Monitor the squid 8. Use the squid What's next?
Index to local CSV files | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Index to local CSV files On this page Save indexed data to local CSV files Objective ​ This tutorial describes how to use the SQD indexing framework for saving processed blockchain data to local CSV files. The intent is to show how Squid SDK can be used for data analytics prototyping. File-based data formats like CSV are convenient for data analysis, especially in the early prototyping stages. This convenience motivated SQD Team to develop some extensions to allow saving processed data to file-based storage. We chose MATIC transactions on Ethereum mainnet for this example. This selection provides enough data to highlight the performance of the SQD framework and is interesting from a data analysis standpoint. An article about this demo project has been published on Medium . The project source code can be found in the local-csv-indexing repository . warning As of 2023-12-17, the local-csv-indexing repo is mostly still sound, but already somewhat outdated. You can take a look at the less sophisticated, yet regularly updated example here . Pre-requisites ​ Squid CLI (optional) Python Setup ​ Let's start by creating a new blockchain indexer, or "squid" in SQD terminology. In a terminal, launch this command: sqd init local-csv-indexing -t evm Here, local-csv-indexing is the name of the project, and can be changed to anything else. The -t evm option specifies that the evm template should be used as a starting point. info Note: The template actually has more than what we need for this project. Unnecessary packages have been removed in the tutorial repository. You can grab package.json from there to do the same. Files-wise, docker-compose.yml , schema.graphql and squid.yaml were removed. commands.json , the list of local sqd scripts, has been significantly shortened ( here is the updated version ). ERC-20 token ABI ​ To be able to index transfers of a token, it's necessary to know the address and the ABI ( Application Binary Interface ) of the token contract. The ABI defines the contract's functions and events, including their typed inputs and outputs. Luckily, both of these can be found on block explorers like Etherscan . The indexer needs the ABI for locating the contract events or functions in the EVM execution trace and decoding their inputs. Squid SDK has a handy command to generate some boilerplate TypeScript code to achieve this: npx squid-evm-typegen src/abi 0x7D1AfA7B718fb893dB30A3aBc0Cfc608AaCfeBB0 #matic info The typegen tool uses Etherscan API to fetch the contract ABI. Other compatible APIs are supported via the --etherscan-api flag. For example, if the contract was deployed to Polygon and its ABI was available from Polygonscan, it could still be fetched with the same command extended with --etherscan-api https://api.polygonscan.io . Alternatilvely, the same command can be used with a path (local or URL) to a JSON ABI in place of the contract address. This will generate some files under the src/abi folder, the most interesting of which is matic.ts . CSV, Tables and Databases ​ For writing local CSVs we will need the file-store-csv package: npm i @subsquid/file-store-csv info Packages are also available for writing to parquet files and for uploading to S3-compatible cloud services . Next, let's create a new file at src/tables.ts . This is where it's possible to provide filenames for the CSV files, as well as configure their data structure, in much the same way as if they were a database table (the class name is no coincidence): import { Table , Column , Types } from '@subsquid/file-store-csv' export const Transfers = new Table ( 'transfers.csv' , { blockNumber : Column ( Types . Numeric ( ) ) , timestamp : Column ( Types . DateTime ( ) ) , contractAddress : Column ( Types . String ( ) ) , from : Column ( Types . String ( ) ) , to : Column ( Types . String ( ) ) , amount : Column ( Types . Numeric ( ) ) , } , { header : false , } ) Let's create another file next, this time named src/db.ts , to configure the data abstraction layer. Here we export an instance of the Database class implementation from the file-store package (a dependency of file-store-csv ). We will use this instance in much the same way as we would use a TypeormDatabase instance in a PostgreSQL-based squid. import { Database , LocalDest , Store } from '@subsquid/file-store' import { Transfers } from './tables' export const db = new Database ( { tables : { Transfers , } , dest : new LocalDest ( './data' ) , chunkSizeMb : 100 } ) info Note the chunkSizeMb option. A new chunk (that is, a new folder with a new CSV file in it) will be written when either the amount of data stored in the processor buffer exceeds chunkSizeMb , or at the end of the batch during which ctx.store.setForceFlush() was called. Data indexing ​ All the indexing logic is defined in src/processor.ts , so let's open it and edit the EvmBatchProcessor class configuration. We should request data for the right smart contract and the right EVM event log: export const contractAddress = '0x7D1AfA7B718fb893dB30A3aBc0Cfc608AaCfeBB0' . toLowerCase ( ) ; const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( { url : process . env . RPC_ENDPOINT , rateLimit : 10 } ) . setFinalityConfirmation ( 75 ) . addLog ( { address : [ contractAddress ] , topic0 : [ events . Transfer . topic ] } ) ; warning Note: the RPC_ENDPOINT environment variable is used, so make sure to edit the .env file and use a valid URL of an Ethereum RPC node, e.g.: DB_NAME = squid DB_PORT = 23798 GQL_PORT = 4350 # JSON-RPC node endpoint, both wss and https endpoints are accepted RPC_ENDPOINT = "<eth_rpc_endpoint_url>" Let's define the logic to process a batch of EVM log data, and save it to CSV files. A double loop is necessary to explore all the blocks in each batch and the logs in each block. The processor provides no guarantees that it won't provide any data not matching the filters, so it's necessary to check that the logs have been generated by the right address and that they have the right topic signature. It's then possible to decode the events and prepare objects with the right data structure, which are then written to the Transfers CSV file.
Here is a short summary of the logic: processor . run ( db , async ( ctx ) => { let decimals = 18 for ( let block of ctx . blocks ) { for ( let log of block . logs ) { if ( log . address !== contractAddress ) continue ; if ( log . topic0 !== events . Transfer . topic ) continue ; const { from , to , value } = events . Transfer . decode ( log ) ; ctx . store . Transfers . write ( ( { blockNumber : block . header . height , timestamp : new Date ( block . header . timestamp ) , contractAddress : log . address , from : from . toLowerCase ( ) , to : to . toLowerCase ( ) , amount : BigDecimal ( value . toBigInt ( ) , decimals ) . toNumber ( ) , } ) ; } } } ) ; info The file in the GitHub repository is slightly different, as there's some added logic to obtain the number of decimals for the token. For that, the processor interacts with the smart contract deployed on chain . Launch the project ​ Build, then launch the project with npm run build node -r dotenv/config lib/main.js And in a few minutes, a few sub-folders (whose names are the block ranges where the data is coming from) should be created under the data directory, each containing a transfer.csv file. Data analysis with Python ​ If you want to learn how to analyze this data using Python and Pandas, refer to the Medium article dedicated to this demo project , or consult the project's repository on GitHub . Conclusions ​ The purpose of this project was to demonstrate how to use SQD's indexing framework for data analytics prototyping: the indexer was able to ingest all this data, process it, and dump it to local CSV files in roughly 20 minutes (figure out of date). The simple Python script in the project's repository shows how to read multiple CSV files, and perform some data analysis with Pandas. SQD Team seeks feedback on this new tool. If you want to share any thoughts or have any suggestions, feel free to reach out to us at the SquidDevs Telegram channel . Edit this page Previous Step 4: Optimization Next Index to Parquet files Objective Pre-requisites Setup ERC-20 token ABI CSV, Tables and Databases Data indexing Launch the project Data analysis with Python Conclusions
Changelog: slots and tags | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference Changelog: slots and tags On this page Сhangelog: slots and tags update tip Take a look at the slots and tags guide page for practical instructions on using the system. Motivation ​ When we initially built the original deployment flow versions felt appropriate. A simple number you could increment on each deployment if necessary. This worked well in simple scenarios, but over time this gradually became more difficult to manage for those of you with any kind of deployment complexity. At the same time it became clear that our solution to endpoint management, allowing you to mark a squid as production and create a permalink, was equally powerful in basic scenarios but limited as requirements grew. It became clear that was lacking here was the ability for larger teams to collaborate effectively. Our goal with these changes is to provide the power and flexibility that more complex deployment flows need. Concepts ​ Concepts Before After Version Existed Removed (replaced with slots) Slot Did not exist Introduced (unique identifier for configs) Production alias Existed Deprecated (use tags) Tag Did not exist Introduced (user-defined labels for deployments) Reference Existed Exists, but the format has changed New concepts are explained in detail in the guide . Format changes ​ Value Before After Version Must be a number Deprecated Slot Did not exist A string of up to six lower case alphanumeric characters, dashes allowed Tag Did not exist A string of lowercase alphanumeric characters, dashes allowed Reference <name>@v<version> [<org>/]<name>(@<slot>|:<tag>) Canonical URL https://<org>.squids.live/<name>/v/v<version>/graphql https://<org>.squids.live/<name>@<slot>/api/graphql Production URL https://<org>.squids.live/<name>/graphql Deprecated. See also backwards compatibility . Tag URL Did not exist https://<org>.squids.live/<name>:<tag>/api/graphql Manifest changes ​ Field Before After name Present Present version Present Deprecated (mapped to slot) slot Absent New field (unique identifier) tag Absent New field (user-defined label) CLI changes ​ Changes to CLI behavior are rather extensive: New flags --name/-n , --tag/-t , --slot/-s , --reference/-r have been added to nearly all commands. For sqd deploy they now can override their corresponding manifest fields . Heterogenous overrides also work: -t in CLI overrides slot: in manifest, and -s overrides tag: . For other commands ( sqd logs , sqd restart etc) they allow for flexible specification of deployment. New commands sqd tags add and sqd tags remove have been added. The sqd prod command has been removed. Here's what the new commands look like for some common tasks: Action Before After Deploying prod version sqd deploy . && sqd prod my-squid@v1 sqd deploy . --add-tag prod Promoting to prod sqd prod my-squid@v1 sqd tags add prod -n my-squid -s v1 Marking a deployment as a "dev" version Was not possible sqd tags add dev -n my-squid -s v1 Updating the "dev" version Was not possible sqd deploy . -t dev Fetching logs sqd logs my-squid@v1 sqd logs -n my-squid -s v1 Cheatsheet ​ Add tag testtag to the deployment of my-squid running in slot v1 sqd tags add testtag -n my-squid -s v1 (Re)deploy the squid and immediately assign the tag testtag to the deployment sqd deploy . --add-tag testtag The squid will be redeployed if one of the tag: , slot: or version: fields are defined in the manifest and the Cloud already has a deployment that matches the reference. If the fields are undefined or there's no matching squid in the Cloud, a new deployment will be created in a new slot. Tag testtag will be assigned to the slot of the (possibly new) deployment. Update the deployment tagged dev sqd deploy . --tag dev If there's a deployment tagged dev , it will be updated. Otherwise, the command will exit with an error. Read logs of the deployment of my-squid tagged prod sqd logs -n my-squid -t prod Backwards compatibility ​ Here are the measures we've taken to make the migration smoother: Existing deployments lose their versions; instead, they are assigned to the corresponding v<version> slots. New deployments that specify version: in the manifest will be assigned to the v<version> slot, too. Hence, for the time being the lines version : 42 and slot : v42 in the manifest file are equivalent. Tag prod has a special meaning for the duration of the transition period: deployments that have it are made available via old-style production URLs . Tag prod is assigned to all existing deployments with production aliases . You can, for the time being, add an old-style production URL to any deployment - just assign the prod tag to it. FAQs ​ Do I need to redeploy anything? ​ No. But we suggest you take a look at the Backwards compatibility section. We really like versions, they work well for us, do we need to change our workflow? ​ No, you don't need to change your workflow at all! Your version will be migrated into the new slot field, and slots are a just an arbitrary string between 2 and 6 characters in length. Our goal was to expand functionality rather than remove it. Why should I use tags? Convince me. ​ Tags serve two key roles. Firstly as a labelling mechanism, this allows teams to explicitly describe what each squid does and what stage it's at in development. Secondly as a way to preserve urls across slots. Tags act as an alias by creating a unqiue endpoint for each tag. This allows you to migrate a consistent endpoint across slots with ease. Will any of my endpoints change when this is released? ​ No. Every endpoint will remain unchanged. Where should I send my feedback on the new system? ​ We're keen to hear from you. Come talk to us in the SquidDevs Telegram channel . Edit this page Previous .squidignore file(s) Next Solana indexing Motivation Concepts Format changes Manifest changes CLI changes Cheatsheet Backwards compatibility FAQs
Transactions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource Transactions On this page Transactions addTransaction(options) ​ Get some or all transactions on the network. options has the following structure: { // data requests type ? : TransactionType [ ] // related data retrieval receipts ? : boolean inputs ? : boolean outputs ? : boolean range ? : { from : number to ? : number } } Data requests: type sets the type of the transaction: 'Script' | 'Create' | 'Mint' | 'Upgrade' | 'Upload' . Leave it undefined to subscribe to all transactions. Enabling the receipts and/or inputs and outputs flags will cause the processor to retrieve receipts, inputs and outputs that occurred as a result of each selected transaction. The data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/fuel-objects on the block data to populate the convenience reference fields like transaction.receipts . Note that transactions can also be requested by the other FuelDataSource methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Examples ​ Request all transactions with Create and Mint types and include receipts, inputs and outputs: processor . addTransaction ( { type : [ "Create" , "Mint" ] , receipts : true , inputs : true , outputs : true , } ) . build ( ) ; Edit this page Previous Receipts Next Field selection Examples
Topics specific to Substrate | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific ink! contracts support Frontier EVM support Gear support Substrate data sourcing Substrate types bundles Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Substrate-specific Topics specific to Substrate 📄️ ink! contracts support ink! WASM smart contracts support 📄️ Frontier EVM support Indexing EVMs running on Substrate 📄️ Gear support Additional support for indexing Gear programs 📄️ Substrate data sourcing Picking the right data on Substrate 📄️ Substrate types bundles Sourcing Substrate metadata Previous Proxy contracts Next ink! contracts support
Getting started with Squid SDK | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Environment set up Indexer from scratch Development flow Project structure sqd CLI cheatsheet Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Getting started Getting started with Squid SDK 📄️ Environment set up Prepare for working with Squid SDK 📄️ Indexer from scratch Learn to compose SDK packages 📄️ Development flow A general approach to squid development 📄️ Project structure The folder layout of a squid project 📄️ sqd CLI cheatsheet Commonly used CLI commands Previous Overview Next Environment set up
Field selection | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM Field selection On this page Field selection setFields(options) ​ Set the fields to be retrieved for data items of each supported type. The options object has the following structure: { log ? : // field selector for logs transaction ? : // field selector for transactions stateDiff ? : // field selector for state diffs trace ? : // field selector for traces block ? : // field selector for block headers } Every field selector is a collection of boolean fields, typically (with a notable exception of trace field selectors ) mapping one-to-one to the fields of data items within the batch context iterables . Defining a field of a field selector of a given type and setting it to true will cause the processor to populate the corresponding field of all data items of that type. Here is a definition of a processor that requests gas and value fields for transactions: let processor = new EvmBatchProcessor ( ) . setFields ( { transaction : { gas : true , value : true } } ) Same fields will be available for all data items of any given type, including nested items. Suppose we used the processor defined above to subscribe to some transactions as well as some logs, and for each log we requested a parent transaction: processor . addLog ( { // some log data requests transaction : true } ) . addTransaction ( { // some transaction data requests } ) As a result, gas and value fields would be available both within the transaction items of the transactions iterable of block data and within the transaction items that provide parent transaction information for the logs: processor . run ( db , async ctx => { for ( let block of ctx . blocks ) { for ( let txn of block . transactions ) { let txnGas = txn . gas // OK } for ( let log of block . logs ) { let parentTxnGas = log . transaction . gas // also OK! } } } ) Some data fields, like hash for transactions, are enabled by default but can be disabled by setting a field of a field selector to false . For example, this code will not compile: let processor = new EvmBatchProcessor ( ) . setFields ( { transaction : { hash : false } } ) . addTransaction ( { // some transaction data requests } ) processor . run ( db , async ctx => { for ( let block of ctx . blocks ) { for ( let txn of block . transactions ) { let txnHash = txn . hash // ERROR: no such field } } } ) Disabling unused fields will improve sync performance, as the disabled fields will not be fetched from the SQD Network gateway. Data item types and field selectors ​ tip Most IDEs support smart suggestions to show the possible field selectors. For VS Code, press Ctrl+Space . Here we describe the data item types as functions of the field selectors. Unless otherwise mentioned, each data item type field maps to the eponymous field of its corresponding field selector. Item fields are divided into three categories: Fields that are added independently of the setFields() call. These are either fixed or depend on the related data retrieval flags (e.g. transaction for logs). Fields that can be disabled by setFields() . E.g. a topics field will be fetched for logs by default, but can be disabled by setting topics: false within the log field selector. Fields that can be requested by setFields() . E.g. a transactionHash field will only be available in logs if the log field selector sets transactionHash: true . Logs ​ Log data items may have the following fields: Log { // independent of field selectors id : string logIndex : number transactionIndex : number block : BlockHeader transaction ? : Transaction // can be disabled with field selectors address : string data : string topics : string [ ] // can be requested with field selectors transactionHash : string } See the block headers section for the definition of BlockHeader and the transactions section for the definition of Transaction . Transactions ​ Transaction data items may have the following fields: Transaction { // independent of field selectors id : string transactionIndex : number block : BlockHeader // can be disabled with field selectors from : string to ? : string hash : string // can be requested with field selectors gas : bigint gasPrice : bigint maxFeePerGas ? : bigint maxPriorityFeePerGas ? : bigint input : string nonce : number value : bigint v ? : bigint r ? : string s ? : string yParity ? : number chainId ? : number gasUsed ? : bigint cumulativeGasUsed ? : bigint effectiveGasPrice ? : bigint contractAddress ? : string type ? : number status ? : number sighash : string // limited availability (see below) l1Fee ? : bigint l1FeeScalar ? : number l1GasPrice ? : bigint l1GasUsed ? : bigint l1BlobBaseFee ? : bigint l1BlobBaseFeeScalar ? : number l1BaseFeeScalar ? : number } status field contains the value returned by eth_getTransactionReceipt : 1 for successful transactions, 0 for failed ones and undefined for chains and block ranges not compliant with the post-Byzantinum hard fork EVM specification (e.g. 0-4,369,999 on Ethereum). type field is populated similarly. For example, on Ethereum 0 is returned for Legacy txs, 1 for EIP-2930 and 2 for EIP-1559. Other networks may have a different set of types. See the block headers section for the definition of BlockHeader . l1* fields can only be requested for networks from this list . Requesting them for other networks may cause HTTP 500 responses. State diffs ​ StateDiff data items may have the following fields: StateDiff { // independent of field selectors transactionIndex : number block : BlockHeader transaction ? : Transaction address : string key : 'balance' | 'code' | 'nonce' | string // can be disabled with field selectors kind : '=' | '+' | '*' | '-' prev ? : string | null next ? : string | null } The meaning of the kind field values is as follows: '=' : no change has occured; '+' : a value was added; '*' : a value was changed; '-' : a value was removed. The values of the key field are regular hexadecimal contract storage key strings or one of the special keys 'balance' | 'code' | 'nonce' denoting ETH balance, contract code and nonce value associated with the state diff. See the block headers section for the definition of BlockHeader and the transactions section for the definition of Transaction . Traces ​ Field selection for trace data items is somewhat more involved because its fixed fields action and result may contain different fields depending on the value of the type field. The retrieval of each one of these subfields is configured independently. For example, to ensure that all traces of 'call' type contain the .action.gas field, the processor must be configured as follows: processor . setFields ( { trace : { callGas : true } } ) The full Trace type with all its possible (sub)fields looks like this: Trace { // independent of field selectors transactionIndex : number block : BlockHeader transaction ? : Transaction traceAddress : number [ ] type : 'create' | 'call' | 'suicide' | 'reward' subtraces : number // can be disabled with field selectors error : string | null // can be requested with field selectors // if (type==='create') action : { // request the subfields with from : string // createFrom: true value : bigint // createValue: true gas : bigint // createGas: true init : string // createInit: true } result ? : { gasUsed : bigint // createResultGasUsed: true code : string // createResultCode: true address ? : string // createResultAddress: true } // if (type==='call') action : { from : string // callFrom: true to : string // callTo: true value : bigint // callValue: true gas : bigint // callGas: true sighash : string // callSighash: true input : string // callInput: true } result ? : { gasUsed : bigint // callResultGasUsed: true output : string // callResultOutput: true } // if (type==='suicide') action : { address : string // suicideAddress: true refundAddress : string // suicideRefundAddress: true balance : bigint // suicideBalance: true } // if (type==='reward') action : { author : string // rewardAuthor: true value : bigint // rewardValue: true type : string // rewardType: true } } Block headers ​ BlockHeader data items may have the following fields: BlockHeader { // independent of field selectors hash : string height : number id : string parentHash : string // can be disabled with field selectors timestamp : number // can be requested with field selectors nonce ? : string sha3Uncles : string logsBloom : string transactionsRoot : string stateRoot : string receiptsRoot : string mixHash ? : string miner : string difficulty ? : bigint totalDifficulty ? : bigint extraData : string size : bigint gasLimit : bigint gasUsed : bigint baseFeePerGas ? : bigint // limited availability (see below) l1BlockNumber : number } The l1BlockNumber field can only be requested for networks from this list . Requesting it for other networks may cause HTTP 500 responses. A complete example ​ import { EvmBatchProcessor } from '@subsquid/evm-processor' import * as gravatarAbi from './abi/gravatar' import * as erc721abi from './abi/erc721' import { TypeormDatabase } from '@subsquid/typeorm-store' const gravatarRegistryContract = '0x2e645469f354bb4f5c8a05b3b30a929361cf77ec' const gravatarTokenContract = '0xac5c7493036de60e63eb81c5e9a440b42f47ebf5' const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setFinalityConfirmation ( 75 ) . setBlockRange ( { from : 6_000_000 } ) . addLog ( { address : [ gravatarRegistryContract ] , topic0 : [ gravatarAbi . events . NewGravatar . topic , gravatarAbi . events . UpdatedGravatar . topic , ] } ) . addTransaction ( { to : [ gravatarTokenContract ] , range : { from : 15_500_000 } , sighash : [ erc721abi . functions . setApprovalForAll . sighash ] } ) . setFields ( { log : { topics : true , data : true , } , transaction : { from : true , input : true , to : true } } ) processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { // Simply output all the items in the batch. // It is guaranteed to have all the data matching the data requests, // but not guaranteed to not have any other data. ctx . log . info ( ctx . blocks , "Got blocks" ) } ) Edit this page Previous Traces Next Substrate Data item types and field selectors Logs Transactions State diffs Traces Block headers A complete example
Index to Parquet files | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Index to Parquet files On this page Save indexed data in Parquet files Objective ​ This tutorial describes how to use the SQD indexing framework to save the processed blockchain data to Parquet files instead of a database. The intent is to show how Squid SDK can be used for data analytics, this time with focus on tools suitable for larger datasets. File-based data formats like CSV are convenient for data analysis, especially in the early prototyping stages. However, when working with large datasets the ability to read data files partially is a common requirement. This is rarely possible with CSV. In contrast, the Parquet format is designed for efficient read/write operations without processing the entire file. To better serve data analysts' needs, the SQD Team developed a library for storing indexer data in this format. The subject of this tutorial is the Uniswap DApp, namely the data from its pool contracts and positions held by investors. Uniswap was chosen because it generates a very large amount of information, and ultimately this helps to better show how to leverage a more performance-oriented format. An article about this demo project has been published on Medium . The project source code can be found in the squid-parquet-storage repo . warning As of 2023-12-17, the squid-parquet-storage repo is mostly still sound, but already somewhat outdated. You can take a look at the less sophisticated, yet regularly updated example here . Pre-requisites ​ Squid CLI (optional) Python Setup ​ Let's start by creating a new blockchain indexer, or a "squid" in SQD terminology. In a terminal, launch this command: sqd init local-parquet-indexing -t evm Here, local-parquet-indexing is the name of the project, and can be changed to anything else. The -t evm option specifies that the evm template should be used as a starting point. info Note: The template actually has more than what we need for this project. Unnecessary packages have been removed in the tutorial repository. You can grab package.json from there to do the same. Files-wise, docker-compose.yml , schema.graphql and squid.yaml were removed. commands.json , the list of local sqd scripts, has been significantly shortened ( here is the updated version ). Finally, make sure to install the dependencies: npm i ERC-20 token ABI ​ Due to the size of this project most of the code will be explained rather than directly listed. The code of this section is no exception since this project indexes data from three kinds of Uniswap contracts, one of which is a factory that deploys new contracts. Additionally, it uses a Multicall contract. For this project, you will need: The Uniswap V3 Factory smart contract address and ABI The ABI of any one of the Uniswap V3 Pool smart contracts deployed by the Factory The Uniswap V3 Positions smart contract address, and ABI The address of any deployed Maker DAO's Multicall smart contract. Luckily, Uniswap have their own The ABI of an ERC-20 token (can be compiled from OpenZeppelin repository , or downloaded, for example from here , or from the repository of this article's project ). Save it as abi/ERC20.json . info Note: The project also uses ERC20NameBytes and ERC20SymbolBytes ABIs, which OpenZeppelin defines in IERC20Metadata.sol and includes in ERC20.sol . Save these files to ./abi . info Note: you can find a pool and its ABI by inspecting the internal calls of one of the Create Pool transactions of the Factory contract . Generate TypeScript code for them: npx squid-evm-typegen ./src/abi ./abi/*.json npx squid-evm-typegen ./src/abi 0x1f98431c8ad98523631ae4a59f267346ea31f984 #factory npx squid-evm-typegen ./src/abi 0xc36442b4a4522e871399cd717abdd847ab11fe88 #NonfungiblePositionManager npx squid-evm-typegen ./src/abi 0x390a4d096ba2cc450e73b3113f562be949127ceb #pool --multicall This should create a few files in the src/abi folder for you. The last command also automatically generates a Typescript ABI for the Multicall contract due to the --multicall flag. Tables and Databases ​ The @subsquid/file-store library defines the Table and Database classes. The Database class gets its name from the interface that was originally developed to access an actual database . Here, the interface is used without modification in a class designed to access a filesystem. Table s play a similar role to that of tables of an actual database: they represent collections of rows, all of which share same set of fields/columns. Each such data structure requires one or more data files to store it in both CSV and Parquet, hence the mapping of Table s to files. To summarize, Table instances are used to define data files along with their schemas and hold file-specific settings. Database facilitates the interactions with the processor, coordinates writing to the files and maintains any state that facilitates that process (configuration, cloud connections and so on). There are two main differences from the CSV tutorial and the first one is that for this project we will be using a Table implementation from @subsquid/file-store-parquet . Let's install it: npm i @subsquid/file-store-parquet The other one is that this project is more involved and is, in fact, using ten different tables instead of one. It's advisable to define these tables in a separate file. The original project has them under src/tables.ts . The syntax is pretty much the same as for the CSV tables, except now the Table , Column , and Types classes are imported from the @subsquid/file-store-parquet library and the set of available types is different. We will also configure the parquet file compression using the new Compression class. Here's a snippet: import { Table , Column , Compression , Types } from '@subsquid/file-store-parquet' export const Tokens = new Table ( 'tokens.parquet' , { blockNumber : Column ( Types . Uint32 ( ) ) , timestamp : Column ( Types . Timestamp ( ) ) , contractAddress : Column ( Types . String ( ) ) , symbol : Column ( Types . String ( ) ) , name : Column ( Types . String ( ) ) , totalSupply : Column ( Types . Uint64 ( ) ) , decimals : Column ( Types . Uint16 ( ) ) , } , { compression : Compression . ZSTD , } ) The rest of the tables definitions can be found here . Similarly, a src/db.ts file should be created to configure the Database class. Here we specify the tables used, as well as the destination and the size of the chunks in which the data is going to be split. Here are the contents of this file in full: import assert from 'assert' import { Database , LocalDest , Store } from '@subsquid/file-store' import { FactoryPairCreated , PoolBurn , PoolInitialize , PoolMint , PoolSwap , PositionCollect , PositionDecreaseLiquidity , PositionIncreaseLiquidity , PositionTransfer , Tokens , } from './tables' import { PoolsRegistry } from './utils' import { S3Dest } from '@subsquid/file-store-s3' type Metadata = { height : number pools : string [ ] } export const db = new Database ( { tables : { Tokens , FactoryPairCreated , PoolInitialize , PoolMint , PoolBurn , PoolSwap , PositionCollect , PositionDecreaseLiquidity , PositionIncreaseLiquidity , PositionTransfer , } , dest : process . env . DEST === 'S3' ? new S3Dest ( './uniswap' , 'csv-store' ) : new LocalDest ( './data' ) , hooks : { async onConnect ( dest ) { if ( await dest . exists ( 'status.json' ) ) { let { height , pools } : Metadata = await dest . readFile ( 'status.json' ) . then ( JSON . parse ) assert ( Number . isSafeInteger ( height ) ) let registry = PoolsRegistry . getRegistry ( ) for ( let pool of pools ) { registry . add ( pool ) } return height } else { return - 1 } } , async onFlush ( dest , range ) { let metadata : Metadata = { height : range . to , pools : PoolsRegistry . getRegistry ( ) . values ( ) , } await dest . writeFile ( 'status.json' , JSON . stringify ( metadata ) ) } , } , chunkSizeMb : 50 , } ) export type Store_ = typeof db extends Database < infer R , any > ? Store < R > : never info Note: the chunkSizeMb option defines the size (in MB) of a parquet file before it's saved on disk, and a new one is created. Data indexing ​ The orchestration of the indexing logic is defined in the file named src/processor.ts : import { EvmBatchProcessor } from '@subsquid/evm-processor' import * as positionsAbi from './abi/NonfungiblePositionManager' import * as factoryAbi from './abi/factory' import * as poolAbi from './abi/pool' import { db } from './db' import { processFactory } from './mappings/factory' import { processPools } from './mappings/pools' import { FACTORY_ADDRESS , POSITIONS_ADDRESS } from './utils/constants' import { processPositions } from './mappings/positions' let processor = new EvmBatchProcessor ( ) . setBlockRange ( { from : 12369621 } ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( { url : process . env . ETH_CHAIN_NODE , rateLimit : 10 } ) . setFinalityConfirmation ( 75 ) . addLog ( { address : [ FACTORY_ADDRESS ] , topic0 : [ factoryAbi . events . PoolCreated . topic ] } ) . addLog ( { topic0 : [ poolAbi . events . Burn . topic , poolAbi . events . Mint . topic , poolAbi . events . Initialize . topic , poolAbi . events . Swap . topic , ] } ) . addLog ( { address : [ POSITIONS_ADDRESS ] , topic0 : [ positionsAbi . events . IncreaseLiquidity . topic , positionsAbi . events . DecreaseLiquidity . topic , positionsAbi . events . Collect . topic , positionsAbi . events . Transfer . topic , ] } ) processor . run ( db , async ( ctx ) => { await processFactory ( ctx ) await processPools ( ctx ) await processPositions ( ctx ) } ) Here's a brief explanation of the code above: The EvmBatchProcessor class is instantiated and set to connect to the Ethereum gateway of SQD Network, as well as a blockchain node, requesting data after a certain block ( make sure to add a node URL to ETH_CHAIN_NODE variable in the .env file ). Real-time consensus data will be considered final after 75 block confirmations / 15 minutes. It is also configured to request data for EVM logs generated by the Factory and Positions smart contracts, filtering for certain events ( PoolCreated and IncreaseLiquidity , DecreaseLiquidity , Collect , Transfer , respectively). Processor is configured to also request EVM logs from any address with topic0 matching one of the signatures of the following Pool smart contract events: Burn , Mint , Initialize , Swap . This will guarantee that events generated by all Pool contracts are captured regardless of when the contracts were deployed. Finally, the processor is launched and data is processed in batches, by functions defined in src/mappings . For a brief explanation of what processFactory , processPools and processPositions do, let's take the processPositions functions as an example: it needs to "unbundle" the batch of logs received for each EVM log found it checks that it belongs to one of the pool addresses generated by the factory compares the log's topic0 against the topics of the Events of the position NFT contract uses the corresponding Event TypeScript class to decode the log writes the decoded information to the corresponding table (parquet file) To better understand how data is transformed, and how the other functions are defined as well, it's advised to browse the repository and inspect the code. Be sure to check the utils folder as well, as there are some auxiliary files and functions used in the mapping logic. Launch the project ​ When the logic is fully implemented, to launch the project and start indexing, open a terminal and run these three commands: npm run build node -r dotenv/config lib/main.js The indexer should be able to catch up with the Ethereum blockchain, and reach the chain's head in a very short time . info Bear in mind that this may vary a lot, depending on the Ethereum node used and on your hardware, as well as the connection, or physical distance from the Ethereum node. It took ~45 minutes while testing for this article. A test on a connection with a much higher latency and the same configuration finished indexing in 5 hours (figures out of date). The process will generate a series of sub-folders in the data folder, labelled after the block ranges where the data is coming from, and in each one of these folders there should be one *.parquet file for each of the tables we defined. Data analysis with Python ​ If you want to learn how to analyze this data using Python and Pandas, refer to the Medium article dedicated to this demo project . Conclusions ​ The purpose of this tutorial was to demonstrate how to use the SQD indexing framework for data analytics on larger datasets. The parquet data format is one of the most successful tools used in this setting, supported by the most common data analysis libraries such as Pandas and Pyarrow. We tested the performance of our parquet format tools on Uniswap, indexing the data from its pools and positions held by investors. The project described here was able to index the entirety of Uniswap Pool events, across all the pools created by the Factory contract, as well as the Positions held by investors, in less than an hour (~45 minutes) (figure out of date). info Note: Indexing time may vary, depending on factors, such as the Ethereum node used, on the hardware, and quality of the connection. The simple Python script in the project's repository shows how to read multiple Parquet files, and perform some data analysis with Pandas. SQD Team seeks feedback on this new tool. If you want to share any thoughts or have any suggestions, feel free to reach out to us at the SquidDevs Telegram channel . Edit this page Previous Index to local CSV files Next Use with Ganache or Hardhat Objective Pre-requisites Setup ERC-20 token ABI Tables and Databases Data indexing Launch the project Data analysis with Python Conclusions
typeorm-store | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks typeorm-store On this page @subsquid/typeorm-store This page describes the interface of the classes from the @subsquid/typeorm-store NPM package. If you're looking for a guide on saving squid data to databases and the related workflows, check out the Saving to PostgreSQL page. TypeormDatabase constructor arguments ​ The argument of the TypeormDatabase class constructor may have the following fields: stateSchema: string : the name of the database schema that the processor uses to persist its status (hash + height of the highest reached block). Useful for making sure that each processor uses its own state schema when running multiple processors against the same database (e.g. in a multichain setting). Default: 'squid_processor' . isolationLevel: 'SERIALIZABLE' | 'READ COMMITTED' | 'REPEATABLE READ' : sets the transaction isolation level of processor transactions. Default: 'SERIALIZABLE' . supportHotBlocks: boolean : controls the support for hot blocks. Necessary in all squids that must be able to handle short-lived blockchain forks . That includes all squids that index chain data in near-real time using RPC endpoints. Default: true . projectDir: string : the folder where TypeormDatabase will look for the TypeORM model definition (at lib/model ) and for migrations (at db/migrations ). Default: process.cwd() . Store interface ​ Batch access methods ​ upsert(e: E | E[]) ​ Upsert a single or multiple entities to the database. Does not cascade the upsert to the relations. await ctx . store . upsert ( [ new User ( { id : 'Bob' } ) , new User ( { id : 'Alice' } ) ) ] ) save(e: E | E[]) ​ Deprecated alias for upsert() . insert(e: E | E[]) ​ Inserts a given entity or entities into the database. Does not check if the entity(s) exist in the database and will fail if a duplicate is inserted. Executes a primitive INSERT operation without cascading to the relations . await ctx . store . insert ( [ new User ( { id : 'Bob' } ) , new User ( { id : 'Alice' } ) ) ] ) remove(e: E | E[] | EntityClass<E>, id?: string | string[]) ​ Deletes a given entity or entities from the database. Accepts either an object or an entity ID(s). Does not cascade the deletion . await ctx . store . remove ( User , [ 'Alice' , 'Bob' ] ) TypeORM methods ​ For details see TypeORM EntityManager reference . get ​ Get an entity by ID. await ctx . store . get ( User , 'Bob' ) count ​ Count the number of entities matching a where filter. await ctx . store . count ( User , { where : { firstName : "Timber" , } , } ) countBy ​ Count the number of entities matching a filter. await ctx . store . countBy ( User , { firstName : "Timber" } ) find ​ Return a list matching a where filter. await ctx . store . find ( User , { where : { firstName : "Timber" , } , } ) findBy ​ Return a list matching a filter. let accounts = await ctx . store . findBy ( Account , { id : In ( [ ... accountIds ] ) } ) findOne ​ Return the first entity matching a where filter. const timber = await ctx . store . findOne ( User , { where : { firstName : "Timber" , } , } ) findOneBy ​ Return the first entity matching a filter. const timber = await ctx . store . findOneBy ( User , { firstName : "Timber" } ) findOneOrFail ​ Throws if nothing is found. const timber = await ctx . store . findOneOrFail ( User , { where : { firstName : "Timber" , } , } ) findOneByOrFail ​ Throws if nothing is found. const timber = await ctx . store . findOneByOrFail ( User , { firstName : "Timber" } ) Find Operators ​ find() and findXXX() methods support the following operators: In (contains in array) Not LessThan LessThanOrEqual MoreThan MoreThanOrEqual Like ILike Between Any IsNull Raw (raw SQL fragments) See the details and examples in the TypeORM FindOption docs . Example ​ let accounts = await ctx . store . findBy ( Account , { id : In ( [ ... accountIds ] ) } ) Joining relations ​ To load an entity with relations, use relations field on the find options and specify which relations should be joined: await ctx . store . find ( User , { relations : { project : true , } , where : { project : { name : "TypeORM" , initials : "TORM" , } , } , } ) See the TypeORM docs sections for details. Database connection parameters ​ Database credentials must be supplied via the environment variables: DB_HOST (default localhost ) DB_PORT (default 5432 ) DB_NAME (default postgres ) DB_USER (default postgres ) DB_PASS (default postgres ) DB_SSL (default false ) DB_SSL_REJECT_UNAUTHORIZED (default true ) DB_URL (default undefined , see the DB_URL section ) info When deploying to Cloud with the Postgres addon enabled in the manifest , any user-supplied values are overwritten for most of these variables. See Variable shadowing . typorm-store also supports the following variables for connecting to databases that require client-side SSL: DB_SSL_CA - the root certificate in plain text DB_SSL_CA_FILE - path to a root certificate file DB_SSL_CERT - client certificate in plain text DB_SSL_CERT_FILE - path to client certificate in plain text DB_SSL_KEY - client key in plain text DB_SSL_KEY_FILE - path to client key in plain text tip In case you're deploying to Cloud you can set secrets to the contents of any given file via stdin: sqd secrets set DB_SSL_CA < ca.crt DB_URL ​ When set, DB_URL takes precedence over all individual variables. Its format is as follows: postgres[ql]://[username[:password]@][host[:port]]/database[?parameter_list] where parameter_list is an & -separated list of assignments of SSL connection parameters: ssl=(0|1|true|false) sslmode=(disabled|no-verify|prefer|require|verify-ca|verify-full) sslcert=<path_to_cert_file> sslkey=<path_to_key_file> sslrootcert=<path_to_root_cert_file> When any value is omitted from the URL, the value of the corresponding individual DB_* variable will be used instead. If that is not set, the default will be used. Edit this page Previous Data sinks Next file-store extensions TypeormDatabase constructor arguments Store interface Batch access methods TypeORM methods Find Operators Joining relations Database connection parameters DB_URL
Inspect logs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Inspect logs On this page Logging SQD Cloud automatically collects the logs emitted by the squid processor, its API server and its database. Please use the built-in SDK logger throughout your code when developing for SQD Cloud. You can set the severity flags for squids running in the Cloud via SQD_DEBUG , SQD_TRACE or SQD_INFO - see Environment Variables . To inspect and follow the squid logs from all the squid services, use sqd logs : sqd logs -n < name > -s < slot > -f or sqd logs -n < name > -t < tag > -f For older version-based deployments... ...the slot string is v${version} , so use sqd logs -n < name > -s v < version > -f Check out the Slots and tags guide to learn more. There are additional flags to filter the logs: -f to follow the logs -c allows filtering by the container (can be processor , db , db-migrate and query-node ) -l allows filtering by the severity -p number of lines to fetch (default: 100 ) --since cut off by the time (default: 1d ). Accepts the notation of the ms library : 1d , 10h , 1m . Example â€‹ sqd logs squid-substrate-template@v1 -f -c processor -l info --since 1d Edit this page Previous Environment variables Next Monitoring Example
Development tools for type-safe decoding the raw data, streamlined persistence with TypeORM, and code generation. | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools Development tools for type-safe decoding the raw data, streamlined persistence with TypeORM, and code generation. üìÑÔ∏è TypeORM migration generation Generate TypeORM models from a schema file üìÑÔ∏è TypeORM model generation Generate TypeORM models from a schema file üóÉÔ∏è Type-safe decoding 3 items üìÑÔ∏è Squid generation tools Tools that generate squids from ABIs üìÑÔ∏è Hasura configuration tool Configure Hasura for your squid Previous Substrate types bundles Next TypeORM migration generation
Rewards | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Rewards On this page Rewards addReward(options) â€‹ This allows for tracking rewards. options has the following structure: { // data requests where ? : { pubkey ? : string [ ] } range ? : { from : number to ? : number } } pubkey here is the set public keys of reward receivers. Leave undefined to subscribe to all rewards. Selection of the exact fields to be retrieved for each reward item is done with the setFields() method documented on the Field selection page. Edit this page Previous Token balances Next Field selection
TypeORM migration generation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools TypeORM migration generation TypeORM migration generation The typeorm-migration tool is used to generate, apply and revert database migrations. It follows the conventions below. The migrations are generated in the db/migrations folder. The database connection is inferred from the DB_XXX environment variables. All entities should be exported from lib/model commonjs module, i.e. the entity classes must be compiled from TypeScript. Here are some useful commands: npx squid-typeorm-migration apply # apply pending migrations npx squid-typeorm-migration generate # generate the migration for the schema defined in schema.graphql rm -r db/migrations # clean the db/migrations folder Edit this page Previous Tools Next TypeORM model generation
Entity relations | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file Schema file and codegen Entities Indexes and constraints Entity relations Unions and typed JSON Interfaces OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Schema file Entity relations On this page Entity relations and inverse lookups The term "entity relation" refers to the situation when an entity instance contains an instance of another entity within one of its fields. Type-wise this means that some entity (called the owning entity ) has a field of a type that is some other, non-owning entity. Within the database, this is implemented as an (automatically indexed) foreign key column within the table mapped to the owning entity. A fieldName entity-typed field will map to a column named field_name_id . One-to-one and one-to-many relations are supported by Typeorm. The "many" side of the one-to-many relations is always the owning side. Many-to-many relations are modeled as two one-to-many relations with an explicit join table . An entity relation is always unidirectional, but it is possible to request the data on the owning entity from the non-owning one. To do so, define a field decorated @derivedFrom in the schema. Doing so will cause the Typeorm code generated by squid-typeorm-codegen and the GraphQL API served by squid-graphql-server to show a virtual (that is, not mapping to a database column ) field populated via inverse lookup queries. The following examples illustrate the concepts. One-to-one relations ​ type Account @entity { id : ID ! balance : BigInt ! user : User @derivedFrom ( field : "account" ) } type User @entity { id : ID ! account : Account ! @unique username : String ! creation : DateTime ! } The User entity references Account and owns the one-to-one relation. This is implemented as follows: On the database side: the account property of the User entity maps to the account_id foreign key column of the user table referencing the account table. On the TypeORM side: the account property of the User entity gets decorated with @OneToOne and @JoinColumn . On the GraphQL side: sub-selection of the account property is made available in user -related queries. Sub-selection of the user property is made available in account -related queries. info Unlike for the many-to-one case, the codegen will not add a virtual reverse lookup property to the TypeORM code for one-to-one relations. You can add it manually: src/model/generated/account.model.ts import { OneToOne as OneToOne_ } from "typeorm" @ Entity_ ( ) export class Account { // ... @ OneToOne_ ( ( ) => User , e => e . account ) user : User } If you are using this feature, please let us know at the SquidDevs Telegram channel . Many-to-one/One-to-many relations ​ type Account @entity { " Account address " id : ID ! transfersTo : [ Transfer ! ] @derivedFrom ( field : "to" ) transfersFrom : [ Transfer ! ] @derivedFrom ( field : "from" ) } type Transfer @entity { id : ID ! to : Account ! from : Account ! amount : BigInt ! } Here Transfer defines owns the two relations and Account defines the corresponding inverse lookup properties. This is implemented as follows: On the database side: the from and to properties of the Transfer entity map to from_id and to_id foreign key columns of the transfer table referencing the account table. On the TypeORM side: properties to and from of the Transfer entity class get decorated with @ManyToOne . Properties transfersTo and transfersFrom decorated with @OneToMany get added to the Account entity class. On the GraphQL side: sub-selection of all relation-defined properties is made available in the schema. Many-to-many relations ​ Many-to-many entity relations should be modeled as two one-to-many relations with an explicitly defined join table.
Here is an example: # an explicit join table type TradeToken @entity { id : ID ! # This is required, even if useless trade : Trade ! token : Token ! } type Token @entity { id : ID ! symbol : String ! trades : [ TradeToken ! ] ! @derivedFrom ( field : "token" ) } type Trade @entity { id : ID ! tokens : [ TradeToken ! ] ! @derivedFrom ( field : "trade" ) } Edit this page Previous Indexes and constraints Next Unions and typed JSON One-to-one relations Many-to-one/One-to-many relations Many-to-many relations
SQD Boost Program | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Boost Program On this page SQD Boost Program Unlock new opportunities with SQD and contribute to the future of blockchain indexing Overview ​ Welcome to the SQD Boost Program! We're excited to invite talented developers to collaborate with us in advancing the SQD ecosystem. This program offers a unique opportunity to work closely with our team, enhance your skills, and make a meaningful impact on the future of blockchain indexing. Level 1: Learn and Contribute ​ In Level 1, you'll engage in tasks designed to be both educational and impactful. These tasks typically require up to 10 hours of work and are perfect for developers looking to dive deeper into the SQD platform. Tasks Include: Developing Squids: Utilize the latest squid SDK to build simple indexers (squids) for trending Web3 projects. Gain hands-on experience in interacting with blockchain data and creating efficient data pipelines. Migrating Subgraphs to SQD: Assist in migrating existing subgraphs to the SQD platform. Enhance interoperability and expand the ecosystem by bringing valuable data sources into SQD. Testing Alpha Features: Get early access to new features in SQD Cloud. Provide valuable feedback on alpha releases, helping us refine and improve the platform. Why Join the SQD Boost Program? Enhance Your Skills: Work with cutting-edge blockchain technologies. Learn and apply best practices in indexing and data management. Collaborate and Network: Join a community of like-minded developers. Share knowledge and collaborate on innovative projects. Influence SQD's Direction: Your insights and feedback will directly impact the development of the SQD platform. Play a role in shaping tools and services used by developers worldwide. Earn Rewards: Receive SQD tokens as a token of appreciation for your contributions and completed tasks. How to Enroll
Ready to get started? We'd love to have you onboard!
Please fill out the enrollment form to join the SQD Boost Program: Enroll in the SQD Boost Program Note: Spaces are limited, so we encourage you to apply soon to secure your spot.We look forward to collaborating with you and building the future of blockchain indexing together!If you have any questions or need further information, feel free to reach out to us at [email protected] . Thank you for your interest in the SQD Boost Program! Edit this page Overview Level 1: Learn and Contribute
Overview | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Overview On this page Overview info OpenReader is no longer recommended for use in new squid projects relying on PostgreSQL . See Serving GraphQL to learn about the new options and the Limitations section to understand our motivation. OpenReader is a server that presents data of PostgreSQL-powered squids as a GraphQL API. It relies on the eponymous library lib of the Squid SDK for schema generation. Schema file is used as an input; the resulting API supports OpenCRUD queries for the entities defined in the schema. To start the API server based on schema.graphql install @subsquid/graphql-server and run the following in the squid project root: npx squid-graphql-server The squid-graphql-server executable supports multiple optional flags to enable caching , subscriptions , DoS protection etc. Its features are covered in the next sections. The API server listens at port defined by the GQL_PORT environment variable (defaults to 4350 ). The database connection is configured with the env variables DB_NAME , DB_USER , DB_PASS , DB_HOST , DB_PORT . In SQD Cloud , OpenReader is usually ran as the api: service in the deploy: section of the Deployment manifest . Supported queries ​ The details of the supported OpenReader queries can be found in a separate section Core API . Here is a brief overview of the queries generated by OpenReader for each entity defined in the schema file: the squid last processed block is available with squidStatus { height } query a "get one by ID" query with the name {entityName}ById for each entity defined in the schema file a "get one" query for @unique fields , with the name {entityName}ByUniqueInput Entity queries named {entityName}sConnection . Each query supports rich filtering support, including field-level filters , composite AND and OR filters , nested queries , cross-relation queries and Relay-compatible cursor-based pagination . Subsriptions via live queries (Deprecated in favor of Relay connections) Lookup queries with the name {entityName}s . Union and typed JSON types are mapped into GraphQL Union Types with a proper type resolution with __typename . Built-in custom scalar types ​ The OpenReader GraphQL API defines the following custom scalar types: DateTime entity field values are presented in the ISO format Bytes entity field values are presented as hex-encoded strings prefixed with 0x BigInt entity field values are presented as strings Limitations ​ RAM usage of subscriptions scales poorly under high load, making the feature unsuitable for most production uses. There are currently no plans to fix this issue. Setting up custom resolvers for subscriptions is unreasonably hard. @subsquid/graphql-server depends on the deprecated Apollo Server v3. Edit this page Previous OpenReader Next Configuration Supported queries Built-in custom scalar types Limitations
Cheatsheet | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start Indexing Orca Whirlpool Cheatsheet SDK Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing How to start Cheatsheet On this page CLI cheatsheet This quide provides a quick reference to the commands needed to launch the squid and manage the database. Install dependencies ​ npm i Compile the project ​ npx tsc Launch Postgres database to store the data ​ docker compose up -d Apply database migrations to create the target schema ​ npx squid-typeorm-migration apply Run indexer ​ node -r dotenv/config lib/main.js Check out the indexed swaps ​ docker exec "$(basename " $( pwd ) " ) -db-1 " psql -U postgres \ -c " SELECT slot, from_token, to_token, from_amount, to_amount FROM exchange ORDER BY id LIMIT 10 " Notice that this project doesn't utilize sqd commands. Edit this page Previous Indexing Orca Whirlpool Next SDK Install dependencies Compile the project Launch Postgres database to store the data Apply database migrations to create the target schema Run indexer Check out the indexed swaps
Squid generation tools | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools TypeORM migration generation TypeORM model generation Type-safe decoding Squid generation tools Hasura configuration tool Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Tools Squid generation tools On this page Squid generation tools warning As of 2025-01-01 we no longer maintain the old squid-gen tool. SQD provides tools for generating ready-to-use squids that index events and function calls of smart contracts. EVM/Solidity and WASM/ink! smart contracts are supported. The tools can be configured to make squids that save data to a PostgreSQL database or to a file-based dataset . All that is required is NodeJS, Squid CLI and, if your squid will be using a database, Docker. Squid generation procedure is very similar for both contract types. Here are the steps: Create a new blank squid with sqd init using a suitable template: # for EVM/Solidity contracts sqd init my-squid -t abi # OR # for WASM/ink! contracts sqd init my-squid -t https://github.com/subsquid-labs/squid-ink-abi-template Enter the squid folder and install the dependencies: cd my-squid npm ci Write the configuration of the future squid to squidgen.yaml . Retrieve any necessary contract ABIs and store them at ./abi . Alternatively, skip to the next step and specify the configuration via CLI. Note: some features will not be available. Generate and build the squid code: npx squid-gen config squidgen.yaml npm run build If you chose to configure the tool via CLI instead, do so now. Here's an example: npx squid-gen-abi \ --address 0x2E645469f354BB4F5c8a05B3b30A929361cf77eC \ --archive https://v2.archive.subsquid.io/network/ethereum-mainnet \ --event NewGravatar \ --event UpdatedGravatar \ --function '*' \ --from 6000000 See npx squid-gen-abi --help for all options. Prepare your squid for launching. If it is using a database, start a PostgreSQL container, then regenerate and apply migrations: docker compose up -d npx squid-typeorm-migration generate npx squid-typeorm-migration apply If it is storing its data to a dataset, strip the project folder of database-related facilities that are no longer needed. Test the complete squid by running it locally. Start a processor with node -r dotenv/config lib/main.js If your squid will be serving GraphQL also run npx squid-graphql-server in a separate terminal. Make sure that the squid saves the requested data to its target: if it is serving GraphQL, visit the local GraphiQL playground ; for PostgreSQL-based squids you can also connect to the database with PGPASSWORD=postgres psql -U postgres -p 23798 -h localhost squid and take a look at the contents; if it is storing data to a file-based dataset, wait for the first filesystem sync then verify that all the expected files are present and contain the expected data. If your squid produces data at a low rate, you may have to tweak the chunkSizeMb setting and/or add a ctx.store.setForceFlush() call to manually write dataset chunks at appropriate intervals. At this point your squid is ready. You can run it on your own infrastructure or deploy it to SQD Cloud . Configuration ​ A valid config for the squid-gen config is a YAML file with the following sections: archive is an endpoint URL of a SQD Network gateway. Find an appropriate gateway at the Supported networks page or with sqd gateways . target section describes how the scraped data should be stored. Set target : type : postgres to use a PostgreSQL database that can be presented to users as a GraphQL API or used as-is. Another option is to store the data to a file-based dataset . contracts is a list of contracts to be indexed. Define the following fields for each contract: name address range (optional): block range to be indexed. An object with from and to properties, each of which can be omitted. Defaults to indexing the whole chain. abi (optional on EVM): path to the contract JSON ABI. If omitted for an EVM contract, the tool will attempt to fetch the ABI by address from the Etherscan API or a compatible alternative set by the etherscanApi root option. proxy (EVM-only): when indexing a proxy contract for events or calls defined in the implementation, set this option to its address and the address option to the address of the implementation contract. That way the tool will retrieve the ABI of the implementation and use it to index the output of the proxy. events (optional): a list of events to be indexed or a boolean value. Set to true to index all events defined in the ABI. Defaults to false , meaning that no events are to be indexed. functions (EVM-only, optional): a list of functions the calls of which are to be indexed or a boolean value. Set to true to index calls of all functions defined in the ABI. Defaults to false , meaning that no function calls are to be indexed. etherscanApi (EVM-only, optional): Etherscan API-compatible endpoint to fetch contract ABI by a known address. Default: https://api.etherscan.io/ . file-store targets ​ Currently the only file-based data target type supported by squid-gen packages is parquet . When used, it requires that the path field is also set alongside type . A path can be a local path or an URL pointing to a folder within a bucket on an S3-compatible cloud service. Support for file-store is in alpha stage. Known caveats: If a S3 URL is used, then the S3 region, endpoint and user credentials will be taken from the default environment variables . Fill your .env file and/or set your SQD Cloud secrets accordingly. Unlike their PostgreSQL-powered equivalents, the squids that use file-store may not write their data often. You may have to configure the chunkSizeMb parameter of the Database class and/or call ctx.store.setForceFlush() when appropriate to strike an acceptable balance between the lag of the indexed data and the number of files in the resulting dataset. See Filesystem store overview for details. For parquet targets, the Decimal(38) column type is used by the code generator to represent uint256 . This is done for compatibility reasons: very few tools seem to support reading wider decimals from Parquet files. If you're getting a lot of errors containing value ... does not fit into Decimal(38, 0) , consider replacing the Decimal(38) column type with Decimal(78) or String() at src/table.ts . At the moment, squids generated with file-based data targets contain a lot of facilities for managing the database and have to be stripped of them before use. Strip the squid folder for file-store ​ Steps to convert a squid made with a database-enabled template for use with file-store : Remove unneeded files and packages. rm docker-compose.yml npm uninstall @subsquid/graphql-server @subsquid/typeorm-migration @subsquid/typeorm-store @subsquid/util-internal-json pg typeorm @subsquid/typeorm-codegen Replace commands.json with the one from the file-store-parquet-example repo . curl -o commands.json https://raw.githubusercontent.com/subsquid-labs/file-store-parquet-example/main/commands.json In squid.yaml , remove the deploy.addons section and replace the deploy.api section with api: cmd: [ "sleep" , "3600" ] Install any required file-store packages. # if target.type was `parquet` npm install @subsquid/file-store-parquet # if target.path was an S3 URL npm install @subsquid/file-store-s3 Configuration examples ​ EVM/Solidity ​ Index LiquidationCall events of the AAVE V2 Lending Pool contract , starting from block 11362579 when it was deployed. Save the results to PostgreSQL. Use the ABI located at ./abi/aave-v2-pool.json . archive : eth - mainnet target : type : postgres contracts : - name : aave - v2 - pool address : "0x7d2768dE32b0b80b7a3454c06BdAc94A69DDc7A9" abi : ./abi/aave - v2 - pool.json range : from : 11362579 events : - LiquidationCall Index events and function calls by the DPX contract (a proxy) on Arbitrum, based on the ABI of the implementation contract retrieved from Arbiscan API. Save the results to Parquet files at './data'. archive : arbitrum target : type : parquet path : ./data contracts : - name : dpx address : "0x3f770Ac673856F105b586bb393d122721265aD46" proxy : "0x6C2C06790b3E3E3c38e12Ee22F8183b37a13EE55" events : true functions : true etherscanApi : https : //api.arbiscan.io/ Note: this example is known to run into the integer length issue described in the file-store targets section. One way to make it work is to widen all Decimal column types from 38 to 78 symbols: sed -i -e 's/38/78/g' src/table.ts Index all events and function calls of the Positions NFT and Factory contracts of Uniswap, send the results to the uniswap-data folder of the subsquid-testing-data bucket. archive : eth - mainnet target : type : parquet path : s3 : //subsquid - testing - bucket/uniswap - data contracts : - name : positions address : "0xC36442b4a4522E871399CD717aBDD847Ab11FE88" events : true functions : true - name : factory address : "0x1F98431c8aD98523631AE4a59f267346ea31F984" events : true functions : true Notes: This example is also susceptible to the integer length issue and will drop at least two events if used as-is, without widening the column types. The generated squid requires some variables to be set to connect to S3. Here's an example of what .env may look like: S3_REGION = us-east-1 S3_ENDPOINT = https://s3.filebase.com S3_ACCESS_KEY_ID = myAccessKeyId S3_SECRET_ACCESS_KEY = mySecretAccessKey WASM/ink! ​ Index Transfer events emitted by a ERC20 contract on Shibuya, save results to PostgreSQL. Do not forget to use the ink-abi template ! archive : shibuya target : type : postgres contracts : - name : testToken abi : "./abi/erc20.json" address : "0x5207202c27b646ceeb294ce516d4334edafbd771f869215cb070ba51dd7e2c72" events : - Transfer Note: you can get the ABI from the squid-gen repository: curl -o abi/erc20.json https://raw.githubusercontent.com/subsquid/squid-gen/master/tests/ink-erc20/abi/erc20.json Edit this page Previous Direct RPC queries Next Hasura configuration tool Configuration file-store targets Strip the squid folder for file-store Configuration examples EVM/Solidity WASM/ink!
Procuring SQD | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Procuring SQD Delegate Run a worker Run a gateway Self-host a portal Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Participate Procuring SQD On this page Procuring SQD Participating in SQD Network usually requires some amount of SQD tokens. This page details all the ways in which you can get some of these. General considerations ​ The correct SQD token contracts are on Arbitrum: 0x1337420dED5ADb9980CFc35f8f2B054ea86f8aB1 on Base: 0xd4554BEa546EFa83C1E6B389ecac40EA999B3E78 on Ethereum: 0x1337420dED5ADb9980CFc35f8f2B054ea86f8aB1 To be accessible for the SQD Network contracts the tokens must be on the Arbitrum One network. The most convenient way to interact with the contracts is the SQD network app . You'll need a browser wallet to use it. Supported options: MetaMask Rainbow Coinbase Wallet A variety of wallets accessible via WalletConnect . Buy on Arbitrum ​ Buy SQD on Pancakeswap . The WETH/SQD pool here is the best funded SQD pool across all DEXes, so you'll likely get the best deal here. Buy on Base ​ Buy SQD on Aerodrome . Bridge tokens to Arbitrum One using: IceCreamSwap bridge for moving tokens around quickly Native Base->Eth and Eth->Arbitrum bridges if you want minimal fees and don't mind waiting for 7 days. Centralized exchanges ​ Bybit MEXC KuCoin Gate.io CoinEx Consult our CoinMarketCap entry for a complete list. Edit this page Previous Participate Next Delegate General considerations Buy on Arbitrum Buy on Base Centralized exchanges
SQD Network API documentation for Tron | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Cheatsheet Network API Tron API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing Network API SQD Network API documentation for Tron üìÑÔ∏è Tron API Access Tron data Previous Cheatsheet Next Tron API
SolanaDataSource reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource SolanaDataSource reference documentation 📄️ Block data for Solana Block data for Solana 📄️ General settings Data sourcing and metrics 📄️ Instructions Track instruction execution with addInstruction() 📄️ Transactions Subscribe to txn data with addTransaction() 📄️ Log messages Subscribe to log messages with addLog() 📄️ Balances Track balance changes with addBalance() 📄️ Token balances Track token balance changes with addTokenBalance() 📄️ Rewards Track rewards with addReward() 📄️ Field selection Fine-tuning data requests with setFields() Previous SDK Next Block data for Solana
Indexes and constraints | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file Schema file and codegen Entities Indexes and constraints Entity relations Unions and typed JSON Interfaces OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Schema file Indexes and constraints On this page Indexes and unique constraints warning The lack of indices is the most common cause of slow API queries It is crucial to add database indexes to the entity fields on which one expects filtering and ordering. To add an index to a column, the corresponding entity field must be decorated with @index . The corresponding entity field will be decorated with TypeORM @Index() . One can additionally decorate the field with @unique to enforce uniqueness. It corresponds to the @Index({ unique: true }) TypeORM decorator. Example ​ type Transfer @entity { id : ID ! to : Account ! amount : BigInt ! @index fee : BigInt ! @index @unique } Multi-column indices ​ Multi-column indices are defined on the entity level, with an optional unique constraint. Example ​ type Foo @entity @index ( fields : [ "foo" , "bar" ] ) @index ( fields : [ "bar" , "baz" ] ) { id : ID ! bar : Int ! baz : [ Enum ! ] foo : String ! type Extrinsic @entity @index ( fields : [ "hash" , "block" ] , unique : true ) { id : ID ! hash : String ! @unique block : String ! } Edit this page Previous Entities Next Entity relations Example Multi-column indices Example
Saving to filesystems | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data Store interface Saving to PostgreSQL Saving to filesystems Saving to BigQuery EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Persisting data Saving to filesystems On this page Filesystem store warning Make sure you understand and use the chunkSizeMb setting and the setForceFlush() store method. Failure to do so may cause the processor to output an empty dataset. Overview ​ Squid SDK provides append-only Store interface implementations for saving the data generated by squids to files. The store is designed primarily for offline analytics. It supports CSV and Parquet files, persisted either locally or to a S3 storage. See the list of Packages below. The core component of all file-based stores is the Database class from @subsquid/file-store . To use it, construct an instance and pass it to processor.run() . This results in ctx.store exposing table writers that accept rows of data to be stored. File-based stores always partition data sets along the "block height" dimension, even when it is not in the schema. The number of blocks per partition is variable: a new partition is written when either the internal buffer (size governed by the chunkSizeMb Database constructor option ) of the store fills up, or after any call to batch handler during which there was a call to setForceFlush() . Same failover guarantees as with the Postgres-based store are provided: the processor will roll back to the last successful state after a restart. Example ​ Save ERC20 Transfer events retrieved by EVM processor in transfers.csv files: import { EvmBatchProcessor } from '@subsquid/evm-processor' import * as erc20abi from './abi/erc20' import { Database , LocalDest } from '@subsquid/file-store' import { Column , Table , Types } from '@subsquid/file-store-csv' const processor = /* processor definition */ const dbOptions = { tables : { TransfersTable : new Table ( 'transfers.csv' , { from : Column ( Types . String ( ) ) , to : Column ( Types . String ( ) ) , value : Column ( Types . Numeric ( ) ) } ) } , dest : new LocalDest ( './data' ) , chunkSizeMb : 10 } processor . run ( new Database ( dbOptions ) , async ( ctx ) => { for ( let c of ctx . blocks ) { for ( let log of c . logs ) { if ( /* the log item is a Transfer we're interested in */ ) { let { from , to , value } = erc20abi . events . Transfer . decode ( log ) ctx . store . TransfersTable . write ( { from , to , value } ) } } } } ) The resulting ./data folder may look like this: ./data/ ├── 0000000000-0007688959 │   └── transfers.csv ├── 0007688960-0007861589 │   └── transfers.csv .. . ├── 0016753040-0016762029 │   └── transfers.csv └── status.txt Each of the folders here contains a little over 10 MBytes of data. status.txt contains the height of the last indexed block and its hash. Packages ​ @subsquid/file-store is the core package that contains the implementation of Database for filesystems. At least one file format add-on must be installed alongside it: CSV : Supported via @subsquid/file-store-csv . Parquet : An advanced format that works well for larger data sets. Supported via @subsquid/file-store-parquet . Data in either of these formats can be written to A local filesystem : Supported by @subsquid/file-store out of the box. A bucket in an Amazon S3-compatible cloud : Supported via @subsquid/file-store-s3 . Database Options ​ Constructor of the Database class from file-store accepts a configuration object as its only argument. Its format is as follows: DatabaseOptions { tables : Record < string , Table > dest : Dest chunkSizeMb ? : number hooks ? : DatabaseHooks < Dest > } Here, Table is an interface for classes that make table writers , objects that convert in-memory tabular data into format-specific file contents. An implementation of Table is available for every file format supported by file-store . Consult pages about specific output formats to find out how to define Table s. tables is a mapping from developer-defined string handles to Table instances. A table writer will be created for each Table in this mapping. It will be exposed at ctx.store.<tableHandle> . dest is an instance of Dest , an interface for objects that take the properly formatted file contents and write them onto a particular filesystem. An implementation of Dest is available for every filesystem supported by file-store . For local filesystems use the LocalDest class from the @subsquid/file-store package and supply new LocalDest(outputDirectoryName) here. For other targets consult documentation pages specific to your filesystem choice . chunkSizeMb governs the size of the internal data buffer. A dataset partition will be written as soon as this buffer fills up, or at the end of the batch if setForceFlush() was called. hooks are useful for persisting data between batches . Table Writer Interface ​ For each Table supplied via the tables field of the constructor argument, Database adds a table writer property to ctx.store . The writer is exposed at ctx.store.<tableHandle> , where <tableHandle> is the key of the Table instance in the tables mapping. It has the following methods: ctx.store.<tableHandle>.write(record: T) ctx.store.<tableHandle>.writeMany(records: T[]) Here, T is a Table implementation-specific data row type. See the documentation pages on specific file formats for details. These synchronous methods add rows of data to an in-memory buffer and perform no actual filesystem writes. Instead, the write happens automatically when the internal buffer reaches chunkSizeMb or at the end of the batch during which setForceFlush() was called. The methods return the table writer instance and can be chained. For example, with a Database defined like this: const db = new Database ( { tables : { TransfersTable : new Table ( /* table options */ ) } , // ...dest and dataset partitioning options } ) the following calls become available in the batch handler : processor . run ( db , async ctx => { let record = // row in a format specific to Table implementation ctx . store . TransfersTable . write ( record ) ctx . store . TransfersTable . writeMany ( [ record ] ) } ) setForceFlush() ​ Both of the following calls ctx.setForceFlush() ctx.setForceFlush(true) set the force flush flag within the store. If the flag is still set at the end of a batch , a dataset partition will be written regardless of how full the data buffer currently is. This is useful e.g. in ensuring that the partition size is not much greater than a constant value when writing data at a low rate: let blockCount = 0 processor . run ( db , async ctx => { // ...data is transformed and queued for writing blockCount += ctx . blocks . length if ( blockCount >= 500_000 ) { ctx . store . setForceFlush ( ) } } ) Unset the flag with ctx.setForceFlush(false) Hooks ​ By default, Database maintains a record of the syncing progress in the status.txt file. When the processor with a Database instance starts, it calls the onStateRead() function that reads the highest reached block from status.txt on the target filesystem and returns its hash and height. If the file does not exist, the function returns -1 for height and the syncing resumes starting at the next (zeroth/genesis) block. Syncing status record is updated every time a new partition is written to the dataset: the processor calls onFlush() which overwrites status.txt with the new highest reached block. As a result, the integrity of the data set is guaranteed given the blockchain history up to the point recorded in status.txt . The functions onStateRead() and onStateUpdate() can be overridden using the hooks constructor argument field. To do that, set that field to DatabaseHooks < Dest > { onStateRead ( dest : Dest ) : Promise < HashAndHeight | undefined > onStateUpdate ( dest : Dest , info : HashAndHeight ) : Promise < void > } Parameters: dest : the Dest object used by Database . Use it to access the filesystem. info : a {height: number, hash: string} object. Overriding these functions can be useful for transferring some processor state between batches reliably. A basic example of using hooks can be found here . Edit this page Previous Saving to PostgreSQL Next Saving to BigQuery Overview Packages Database Options Table Writer Interface setForceFlush() Hooks
Glossary | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Glossary On this page Glossary Archives ​ Deprecated term used for SQD Network and for the data sourcing service of the deprecated FireSquid SDK version. Occasionally refers to a chain-specific endpoint available from either source (e.g. "an Ethereum archive"). The new terminology is: "Archives" as an abstract collection of services for some networks is replaced by " SQD Network " (when referring to data location) or "SQD Network gateway" (when referring to the service) "public Archives" are replaced by the open private version of SQD Network "an archive" for a particular network is replaced by "a SQD Network gateway" List of gateways for the open private SQD Network is available in these docs and via sqd gateways . Not to be confused with archive blockchain nodes . archive-registry ​ The deprecated NPM package @subsquid/archive-registry that was used to look up squid data sources by network aliases (with lookupArchive() and a small CLI). We now recommend using raw gateway URLs instead of lookupArchive() calls in processor configuration. The exploratory CLI is replaced by sqd gateways ; list of available network-specific gateways is also available in these docs . Block ​ An atomic state transition of a blockchain. Typically an ordered collection of transactions. Call ​ On Substrate , a call is a sub-routine changing the runtime state. An extrinsic consists of a root call which in turn may have sub-calls, thus calls executed by an extrinsic have parent-child relationship. For example, util.batch extrinsic has a single root call and multiple child calls. SQD processor is call-based rather than extrinsic based, as normally one is interested in specific calls changing the substrate state, no matter if it was part of a batch extrinsic, or it was wrapped in a sudo or proxy call. Cloud (former Aquarium) ​ A cloud service for deploying squids in a serverless fashion maintained by Subsquid Labs GmbH. Contracts pallet ​ Contracts is a pallet developed by Parity to execute WASM-based smart contracts. ETL ​ Stands for Extract-Transform-Load. A pipeline to extract data from the source, enrich, transform, normalize and load into the target data store. Event ​ An operation performed during a blockchain state transition that results in emission of a structured log message. Subsequently, the message can be retrieved from blockchain nodes. EVM ​ Stands for the Ethereum Virtual Machine. An instruction set and the runtime specification originally developed for the Ethereum network which was later adopted by many other chains (e.g. BSC, Polygon). Extrinsic ​ On Substrate , a generalized transaction externally submitted to a Substrate runtime for execution. There are technical nuances differentiating transactions and extrinsics. FRAME pallets ​ A collection of standard Substrate pallets maintained by Parity. Currently kept in the Substrate Repo . Frontier pallet ​ Frontier is a Substrate pallet implementing the Ethereum Virtual Machine . In particular, Substrate chains with Frontier pallet support EVM-based smart contracts. GraphQL ​ A declarative query language and an API specification language developed by Facebook as an alternative to REST. See the official GraphQL docs for more details. Hot blocks ​ An alternative term for unfinalized blocks that emphasizes the fact that the data extracted from these blocks is volatile. Sometimes used in Squid SDK code and comments. ink! ​ An SDK (software development kit) and a smart-contract language for developing WASM-based smart contracts, maintained by Parity. The contracts developed with ink! are compiled into a WASM blob compatible with the API exposed by the Contracts pallet . More details here . OpenReader ​ SQD's own open source GraphQL server , built in-house. No longer recommended for new projects running PostgreSQL due to its limitations . See Serving GraphQL to learn more. The GraphQL schema generation library at the heart of 1. Implements OpenCRUD . Pallet ​ A portable module that can be added to a Substrate runtime. Typically, contains a self-contained implementation of a business logic that can be re-used across multiple chains. Schema file ​ A file describing the target data schema for a squid, normally called schema.graphql . The file uses a GraphQL dialect to define entities, properties and relations. See details here . State ​ A key-value map defining the internal worldview of an EVM contract of a Substrate runtime at a specific point in time. The consensus algorithm ensures that the honest majority of the nodes agree on the runtime state. Storage ​ On Substrate , a persistent key-value database kept by the chain nodes. It is used to access the current and historical state . See details on the Substrate docs page Squid ​ A project consisting of an ETL for extracting and transforming on-chain data (squid processor), and optionally an API to present the data (squid API). Squid processor ​ The ETL part of the squid. Extracts on-chain data from an SQD Network gateway and/or directly from chain RPC, then transforms and optionally enriches it with external data. Saves the result into a target data sink . Squid API ​ The data presentation part of the squid. Typically, it's an OpenReader GraphQL API auto-generated from a schema file. Substrate ​ Substrate is a framework for developing blockchain runtimes. Used to develop the Polkadot, Kusama chains and all the parachains. Substrate Runtime ​ The code that defines the state transition logic of a blockchain, and by extention its business logic. See details on the Substrate docs page . Squid SDK ​ Squid SDK is a collection of open-source libraries for building squids . Typegen ​ A tool generating strongly typed data access classes from a metadata in some format. Squid SDK includes typegen tools: for accessing EVM smart contract data based on the contract ABI for accessing event, extrinsics and storage data based on Substrate metadata for accessing ink! smart contract data based on the contract metadata See this section for documentation on these tools. WASM ​ A portable binary code format and an execution environment specification. WASM programs enjoy deterministic outputs and near-native execution speeds, which makes WASM an attractive alternative to EVM. Edit this page Previous External tools Archives archive-registry Block Call Cloud (former Aquarium) Contracts pallet ETL Event EVM Extrinsic FRAME pallets Frontier pallet GraphQL Hot blocks ink! OpenReader Pallet Schema file State Storage Squid Squid processor Squid API Substrate Substrate Runtime Squid SDK Typegen WASM
On the FireSquid release | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions On the FireSquid release On the FireSquid release Previous major release of SQD was called FireSquid. It featured GraphQL-based archives ( EVM , Substrate ) that were replaced by SQD Network . Interfaces for data requests on the SDK side were changed without keeping backwards compatibility. Actions are needed if: You're relying on a squid that's using an older SDK version. One way to know that is to look at the signatures of the data requesting methods ( .addEvent() , .addLog() etc): if call signatures are different from what you see in the docs ( EVM , Substrate ), then you need to migrate to the modern ArrowSquid SDK . You're relying on a GraphQL API of an older archive. Your options are: To rely on SQD Network instead. See its reference documentation for info on available datasets and the API used to access them. To fork an older archive setup and maintain it yourself. For EVM you can just fork the repo . If you would like to do the same for the Substrate archives, that'd require pulling some old code out of the repos' history. Ping us in the SquidDevs TG chat . Edit this page
Block data for Substrate | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Substrate Block data for Substrate General settings Data requests Fields selection Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors Substrate Block data for Substrate On this page Block data for Substrate SubstrateBatchProcessor follows the common squid processor architecture , in which data processing happens within the batch handler , a function repeatedly called on batches of on-chain data. The function takes a single argument called "batch context". Its structure follows the common batch context layout , with ctx.blocks being an array of Block objects containing the data to be processed, aligned at the block level. For SubstrateBatchProcessor the Block interface is defined as follows: export type Block < F extends FieldSelection = { } > = { header : BlockHeader < F > extrinsics : Extrinsic < F > [ ] calls : Call < F > [ ] events : Event < F > [ ] } F here is the type of the argument of the setFields() processor method. Block.header contains the block header data. The rest of the fields are iterables containing four kinds of blockchain data. The canonical ordering within each iterable is the same as it is within the blocks. The exact fields available in each data item type are inferred from the setFields() call argument. They are documented on the Field selection page: extrinsics section ; calls section ; events section ; block header section . Example â€‹ The handler below simply outputs all the Balances.transfer_all calls on Kusama in real time : import { SubstrateBatchProcessor } from '@subsquid/substrate-processor' import { TypeormDatabase } from '@subsquid/typeorm-store' const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/kusama' ) . setRpcEndpoint ( 'https://kusama-rpc.polkadot.io' ) . setBlockRange ( { from : 19_600_000 } ) . addCall ( { name : [ 'Balances.transfer_all' ] , } ) . setFields ( { call : { origin : true , success : true } } ) processor . run ( new TypeormDatabase ( ) , async ctx => { for ( let block of ctx . blocks ) { for ( let call of block . calls ) { ctx . log . info ( call , ` Call: ` ) } } } ) One can experiment with the setFields() argument and see how the output changes. Edit this page Previous Substrate Next General settings Example
Query optimization | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Query optimization Query optimization Before going to production, ensure that none of the queries that your app generates are executed suboptimally. Failure to do so can cause severe performance issues. The optimization procedure involves executing a representative sample of your app's queries, analyzing the slow queries and iteratively adding any helpful indexes. Here is the procedure: Create a development deployment of your squid. Use the squid name that you intend to be using in production, a new slot and (optionally) a dev tag. See the Slots and tags guide . Wait for you dev deployment to sync. Configure your dev deployment to log mildly slow SQL queries. Set the log_min_duration_statement to some low value in your manifest: deploy : addons : postgres : config : log_min_duration_statement : 50 ... I'm using 50 ms = 0.05 s here. All queries that take longer than that to execute will now be logged by the db container. Update your dev deployment, e.g. with sqd deploy -s < dev-slot > . Send a representative set of queries to your dev deployment. For this step you can use automated or manual tests of your API consumer app. View the slow queries using sqd logs , e.g. sqd logs -f -n < my-squid-name > -s < dev-slot > -c db Connect to your dev deployment's database with psql . You can find the connection command by going to the squids view of the network app > your dev deployment's page > "DB access" tab. Execute the slow queries with EXPLAIN ANALYZE and view their execution plans. Hypothesize about which indexes that could be used to speed up your queries. Aim to use as few indexes as possible: having more than necessary can hurt the performance. One possible strategy is to use ChatGPT. Just give it the execution plan and ask which indexes should be added. If it struggles give it your database schema as well (e.g. in form of your schema.graphql file listing). It is also useful to ask it to explain its suggestion. Update the indexes of your dev deployment's database: Ensure that your local database container schema is the same as the schema of your dev deployment's database. Assuming that the codebases are the same, docker compose up -d npm run build npx squid-typeorm-migration apply should do the trick. If you encounter problems, wipe the database with docker compose down and repeat the step. Add or remove some @index statements to your squid's schema file. Regenerate the TypeORM model classes. npx squid-typeorm-codegen Build your code. npm run build Generate an incremental migration file. npx squid-typeorm-migration generate Update your dev deployment in the Cloud without resetting its database, e.g. sqd deploy -s < dev-slot > . Main guide is in this section . Repeat steps 4-9 until there are no more slow queries. If your queries are complicated, you can skip step 9 and add indexes manually in psql , e.g. like this CREATE INDEX "my_idx" ON my_table ( column ) ; This can drastically reduce the time you spend per iteration, but then you'll have to manually add the combination of indexes that worked to schema.graphql and redo the whole optimization procedure once more to ensure consistency. Otherwise, you may run into unexpected performance issues due to lost/changed indexes as you redeploy or modify your squid. Set a less strict rule for logging slow queries, e.g. as follows deploy : addons : postgres : config : log_min_duration_statement : 1000 # 1 second ... Update the deployment: sqd deploy -s < dev-slot > . Mark your development deployment for production use. If you used a development tag, remove it with sqd tags remove Add a production tag to the deployment with sqd tags add See also the Zero-downtime updates section . Edit this page Previous Slots and tags Next RPC addon
Union type resolution | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Union type resolution Union type resolution Use cases for Union types have been discussed in the schema reference . Here, we discuss how to query union types. Let's take this modified schema from the Substrate tutorial : schema.graphql type Account @entity { id : ID ! #Account address events : [ Event ] } type WorkReport { id : ID ! #event id addedFiles : [ [ String ] ] deletedFiles : [ [ String ] ] extrinsicId : String blockHash : String ! } type JoinGroup { id : ID ! owner : String ! extrinsicId : String blockHash : String ! } type StorageOrder { id : ID ! fileCid : String ! extrinsicId : String blockHash : String ! } union Event = WorkReport | JoinGroup | StorageOrder Here, an Event will have different fields depending on the underlying type. This query demonstrates how to request different fields for each of these types: query MyQuery { accounts { events { __typename ... on WorkReport { id blockHash extrinsicId deletedFiles } ... on JoinGroup { id blockHash extrinsicId } ... on StorageOrder { id blockHash extrinsicId } } id } } The special __typename field allows users to discern the returned object type without relying on comparing the sets of regular fields. For example, in the output of the query above JoinGroup and StorageOrder events can only be distingushed by looking at the __typename field. Here is a possible output to illustrate: { "data" : { "accounts" : [ { "events" : [ { "__typename" : "WorkReport" , "id" : "0000584321-000001-01cdb" , "blockHash" : "0x01cdb3cb6fa00f62fd20220104f1d740a53518b63517419da8a89325d065562b" , "extrinsicId" : "0000584321-000001-01cdb" , "deletedFiles" : [ ] } ] , "id" : "cTKmzHG3RHa1yhujyZpPnNL17p8a48Av3JFwDjpttLcxeSo26" } , { "events" : [ { "__typename" : "JoinGroup" , "id" : "0000584598-000010-d06ec" , "blockHash" : "0xd06ec6716e96108e24987ef03d23c857ef3b467dd057d7a32c4e123fe5a8df36" , "extrinsicId" : "0000584598-000004-d06ec" } ] , "id" : "cTKqevWRdvbNNAQ3hLxhsNYhQ8pf5YGkYnnVjgjLNiVr4kd7a" } , { "events" : [ { "__typename" : "StorageOrder" , "id" : "0000584627-000013-1fa19" , "blockHash" : "0x1fa19ae98731afad853ffd491fcbc0c3dcda6b8b7f5a2d56ac6c4c1eb9e4f95e" , "extrinsicId" : "0000584627-000005-1fa19" } ] , "id" : "cTGYF8jvcpnRmgNopqT4nVs5rWHEviAAdRdfNrZE8NFz2Av7B" } ] } } Edit this page Previous Sorting Next The frontier package
External APIs and IPFS | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides External APIs and IPFS On this page External APIs and IPFS Squid processor is a usual node.js process, so one can fetch data from external APIs or IPFS gateways within the processor.run() method. In squids deployed to SQD Cloud one normally uses API calls in combination with API keys set via secrets . Example ​ For example, one can enrich the indexed transaction with historical price data using the Coingecko API. // ... processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { const burns : Burn [ ] = [ ] for ( let c of ctx . blocks ) { for ( let txn of c . transactions ) { burns . push ( new Burn ( { id : formatID ( c . header . height , txn . hash ) , block : c . header . height , address : txn . from , value : txn . value , txHash : txn . hash , price : await getETHPriceByDate ( c . header . timestamp ) } ) ) } } } ) async function getETHPriceByDate ( timestamp : number ) : Promise < bigint > { const formatted = moment ( new Date ( timestamp ) . toISOString ( ) ) . format ( "DD-MM-yyyy" ) const res = await axios . get ( ` https://api.coingecko.com/api/v3/coins/ethereum/history?date= ${ formatted } &localization=false ` ) return res . data . market_data . current_price . usd } IPFS fetching ​ For reliable indexing of content stored on IPFS (e.g. NFT metadata) we recommend fetching from dedicated IPFS gateways, e.g. provided by Filebase . For a more elaborate example of with IPFS gateway and external API calls, inspect the Step 3 of the BAYC tutorial . Example ​ // FIXME: replace with a dedicated gateway export const BASE_URL = 'https://cloudflare-ipfs.com/ipfs/' export const api = Axios . create ( { baseURL : BASE_URL , headers : { 'Content-Type' : 'application/json' , } , withCredentials : false , timeout : 5000 , httpsAgent : new https . Agent ( { keepAlive : true } ) , } ) export const fetchMetadata = async ( ctx : DataHandlerHandlerContext < Store , typeof fieldSelection > , cid : string ) : Promise < any | null > => { try { const { status , data } = await api . get ( ` ${ BASE_URL } / ${ cid } ` ) ctx . log . info ( ` [IPFS] ${ status } CID: ${ cid } ` ) if ( status < 400 ) { return data } } catch ( e ) { ctx . log . warn ( ` [IPFS] ERROR CID: ${ cid } TRY ${ ( e as Error ) . message } ` ) } return null } processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( let c of ctx . blocks ) { for ( let log of c . logs ) { // track and decode NFT events to get CID // use fetchMetadata() to fetch metadata } } } ) Edit this page Previous RPC ingestion and reorgs Next Multichain Example IPFS fetching Example
TronBatchProcessor reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Block data for Tron General settings Transactions Logs Internal transactions Field selection Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing TronBatchProcessor TronBatchProcessor reference documentation 📄️ Block data for Tron Block data for Tron 📄️ General settings Data sourcing and metrics 📄️ Transactions Methods to subscribe to txn data 📄️ Logs Subscribe to event data with addLog() 📄️ Internal transactions Subscribe to internal txn data 📄️ Field selection Fine-tuning data requests with setFields() Previous Indexing USDT on Tron Next Block data for Tron
Guide into building a squid that indexes transfers, tokens and token owners of Bored Ape Yach Club NFTs. Each step ends with a working squid implementing a part of the final functionality. | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Step 1: Transfer events Step 2: Owners & tokens Step 3: External data Step 4: Optimization Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Indexing BAYC Guide into building a squid that indexes transfers, tokens and token owners of Bored Ape Yach Club NFTs. Each step ends with a working squid implementing a part of the final functionality. üìÑÔ∏è Step 1: Transfer events Indexing events emitted by the BAYC contract üìÑÔ∏è Step 2: Owners & tokens Deriving entities for NFTs and their owners üìÑÔ∏è Step 3: External data Retrieving data from state calls, IPFS and HTTP üìÑÔ∏è Step 4: Optimization Syncing faster while tracking changing data Previous Tutorials Next Step 1: Transfer events
Logs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Block data for Tron General settings Transactions Logs Internal transactions Field selection Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing TronBatchProcessor Logs On this page Logs addLog(options) ​ Get event logs emitted by some or all contracts in the network. options has the following structure: { where ? : { address ? : string [ ] topic0 ? : string [ ] topic1 ? : string [ ] topic2 ? : string [ ] topic3 ? : string [ ] } include ? : { transaction ? : boolean } range ? : { from : number to ? : number } } Data requests are located in the where field: address : the set of addresses of contracts emitting the logs. Omit to subscribe to events from all contracts in the network. topicN : the set of values of topicN. Omit the where field to subscribe to all logs network-wide. Related data can be requested via the include field: transaction = true : will retrieve the parent transacton for each selected log. The data will be added to the .transactions iterable within block data and made available via the .transaction field of each log item. Note that logs can also be requested by the other TronBatchProcessor methods as related data. Selection of the exact fields to be retrieved for each log item and the optional parent transactions is done with the setFields() method documented on the Field selection page. Example ​ Request all Transfer(address,address,uint256) event logs emitted by the USDT smart contract, include parent txs. const USDT_ADDRESS = 'a614f803b6fd780986a42c78ec9c7f77e6ded13c' const TRANSFER_TOPIC = 'ddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' processor . addLog ( { where : { address : [ USDT_ADDRESS ] , topic0 : [ TRANSFER_TOPIC ] } , include : { transaction : true } } ) Edit this page Previous Transactions Next Internal transactions
Examples | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Examples Examples Loading... Edit this page Previous Troubleshooting Next FAQ
Index Solana with SQD | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK Index Solana with SQD üóÉÔ∏è SolanaDataSource 9 items üìÑÔ∏è Typegen Typegen tool Previous Cheatsheet Next SolanaDataSource
Indexing Fuel Network data | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing Indexing Fuel Network data On this page Indexing transaction receipts In this step-by-step tutorial we will look into a squid that indexes Fuel Network data. Pre-requisites: Node.js v20 or newer, Git, Docker. Download the project ​ Begin by retrieving the template and installing the dependencies: git clone https://github.com/subsquid-labs/fuel-example cd fuel-example npm i Configuring the data source ​ "Data source" is a component that defines what data should be retrieved and where to get it. To configure the data source to retrieve the data produced by the receipt field of the fuel transaction, we initialize it like this: src/main.ts const dataSource = new DataSourceBuilder ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/fuel-mainnet' ) . setGraphql ( { url : 'https://mainnet.fuel.network/v1/graphql' , strideConcurrency : 3 , strideSize : 50 } ) . setFields ( { receipt : { contract : true , receiptType : true } } ) . addReceipt ( { type : [ 'LOG_DATA' ] } ) . build ( ) Here, https://v2.archive.subsquid.io/network/fuel-mainnet is the address for the public SQD Network gateway for Fuel. Check out the exhaustive public gateways list . The argument of addReceipt() is a set of filters that tells the processor to retrieve all receipts of type LOG . The argument of setFields() specifies the exact fields we need for every data item type. In this case we request contract and receiptType for receipt data items. See also FuelDataSource reference and the comments in main.ts of the fuel-example repo. With a data source it becomes possible to retrieve filtered blockchain data from SQD Network , transform it and save the result to a destination of choice. Decoding the event data ​ The other part of the squid processor (the ingester process of the indexer) is the callback function used to process batches of the filtered data, the batch handler . In Fuel Squid SDK it is typically defined within a run() call, like this: import { run } from '@subsquid/batch-processor' run ( dataSource , database , async ctx => { // data transformation and persistence code here } ) Here, dataSource is the data source object described in the previous section database is a Database implementation specific to the target data sink. We want to store the data in a PostgreSQL database and present with a GraphQL API, so we provide a TypeormDatabase object here. ctx is a batch context object that exposes a batch of data (at ctx.blocks ) and any data persistence facilities derived from db (at ctx.store ). See Block data for Fuel for details on how the data batches are presented. Batch handler is where the raw on-chain data is decoded, transformed and persisted. This is the part we'll be concerned with for the rest of the tutorial. We begin by defining a database and starting the data processing: src/main.ts run ( dataSource , database , async ctx => { // Block items that we get from `ctx.blocks` are flat JS objects. // // We can use `augmentBlock()` function from `@subsquid/fuel-objects` // to enrich block items with references to related objects. let contracts : Map < String , Contract > = new Map ( ) let blocks = ctx . blocks . map ( augmentBlock ) for ( let block of blocks ) { for ( let receipt of block . receipts ) { if ( receipt . receiptType == 'LOG_DATA' && receipt . contract != null ) { let contract = contracts . get ( receipt . contract ) if ( ! contract ) { contract = await ctx . store . findOne ( Contract , { where : { id : receipt . contract } } ) if ( ! contract ) { contract = new Contract ( { id : receipt . contract , logsCount : 0 , foundAt : block . header . height } ) } } contract . logsCount += 1 contracts . set ( contract . id , contract ) } } } ctx . store . upsert ( [ ... contracts . values ( ) ] ) } ) This goes through all the receipts in the block, verifies that they have type LOG_DATA" , reads the contract field from the receipt and saves it to the database. At this point the squid is ready for its first test run. Execute npx tsc docker compose up -d npx squid-typeorm-migration apply node -r dotenv/config lib/main.js You can verify that the data is being stored in the database by running docker exec "$(basename " $( pwd ) " ) -db-1 " psql -U postgres -c " SELECT * FROM contract" Full code can be found here . Edit this page Previous Fuel indexing Next FuelDataSource Download the project Configuring the data source Decoding the event data
Conduit integration | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Conduit integration Conduit integration Edit this page
Guides to migrations from other frameworks and older SQD versions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Migrate from The Graph ArrowSquid for EVM ArrowSquid for Substrate hasura-configuration tool v2 Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Migration guides Guides to migrations from other frameworks and older SQD versions üìÑÔ∏è Migrate from The Graph Migrate a subgraph to SQD üìÑÔ∏è ArrowSquid for EVM A step-by-step migration guide for EVM üìÑÔ∏è ArrowSquid for Substrate A step-by-step migration guide for Substrate üìÑÔ∏è hasura-configuration tool v2 Breaking change in the Hasura configuration tool Previous Hasura configuration tool Next Migrate from The Graph
.squidignore file(s) | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference .squidignore file(s) .squidignore file(s) Since @subsquid/ [email protected] If a .squidignore file is present in any of the squid folders (including the project root), sqd deploy will read filename patterns from it and omit the matching files from the bundle to be sent to the Cloud. Filename patterns follow the gitignore pattern format . Patterns read from .squidignore files from higher level folders (the project root being the highest) are overridden by patterns read from lower level folders. When no .squidignore files are supplied, sqd deploy will omit the following files and folders: node_modules builds lib Dockerfile .git .github .idea Edit this page Previous RPC service networks Next Changelog: slots and tags
Entity queries | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Entity queries On this page Entity Queries Introduction ​ OpenReader auto-generates queries from the schema.graphql file. All entities defined in the schema can be queried over the GraphQL endpoint. Exploring queries ​ Let’s take a look at the different queries you can run using the GraphQL server. We’ll use examples based on a typical channel/video schema. Simple entity queries ​ Fetch list of entities ​ Fetch a list of channels: query { channels { id handle } } or, using a newer and more advanced {entityName}sConnection query query { channelsConnection ( orderBy : id_ASC ) { edges { node { id handle } } } } Fetch an entity using its unique fields ​ Fetch a channel by a unique id or handle: query Query1 { channelByUniqueInput ( where : { id : "1" } ) { id handle } } query Query2 { channelByUniqueInput ( where : { handle : "Joy Channel" } ) { id handle } } Filter query results / search queries ​ The where argument ​ You can use the where argument in your queries to filter results based on some field’s values. You can even use multiple filters in the same where clause using the AND or the OR operators. For example, to fetch data for a channel named Joy Channel : query { channels ( where : { handle_eq : "Joy Channel" } ) { id handle } } Note that {entityName}sConnection queries support exactly the same format of the where argument: query { channelsConnection ( orderBy : id_ASC , where : { handle_eq : "Joy Channel" } ) { edges { node { id handle } } } } Supported Scalar Types ​ SQD supports the following scalar types: String Int Float BigInt Boolean Bytes DateTime Equality Operators ( _eq ) ​ _eq is supported by all the scalar types. The following are examples of using this operator on different types: Fetch a list of videos where title is "Bitcoin" Fetch a list of videos where isExplicit is "true" Fetch a list of videos publishedOn is "2021-01-05" query Query1 { videos ( where : { title_eq : "Bitcoin" } ) { id title } } query Query2 { videos ( where : { isExplicit_eq : true } ) { id title } } query Query3 { videos ( where : { publishedOn_eq : "2021-01-05" } ) { id title } } Greater than or less than operators ( gt , lt , gte , lte ) ​ The _gt (greater than), _lt (less than), _gte (greater than or equal to), _lte (less than or equal to) operators are available on Int, BigInt, Float, DateTime types. The following are examples of using these operators on different types: Fetch a list of videos published before "2021-01-05" Fetch a list of channels before block "999" query Query1 { videos ( where : { publishedOn_gte : "2021-01-05" } ) { id title } } query Query2 { channels ( where : { block_lte : "999" } ) { id handle } } Text search or pattern matching operators ( _contains , _startsWith , _endsWith ) ​ The _contains , _startsWith , _endsWith operators are used for pattern matching on string fields. Example: query Query1 { videos ( where : { title_contains : "Bitcoin" } ) { id title } } query Query2 { videos ( where : { title_endsWith : "cryptocurrency" } ) { id title } } Edit this page Previous Intro Next AND/OR filters Introduction Exploring queries Simple entity queries Filter query results / search queries
Run a node - a worker or a gateway - or simply delegate your tokens | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Procuring SQD Delegate Run a worker Run a gateway Self-host a portal Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Participate Run a node - a worker or a gateway - or simply delegate your tokens 📄️ Procuring SQD Get SQD tokens 📄️ Delegate Delegate your tokens to workers 📄️ Run a worker Serve SQD Network data 📄️ Run a gateway Operate a node that routes data requests 📄️ Self-host a portal Operate a client node Previous Tokenomics Next Procuring SQD
Processor in action | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Processor in action On this page Processor in action EVM Substrate An end-to-end idiomatic usage of EvmBatchProcessor can be inspected in the gravatar template repository and also learned from more elaborate examples . In order to illustrate the concepts covered in the development guide , here we highlight the key steps, put together a processor configuration and a data handling definition. Pre-requisites: NodeJS, Git, Docker, Squid CLI , any of the EVM templates . 1. Model the target schema and generate entity classes ​ Create or edit schema.graphql to define the target entities and relations. Consult the schema reference . Update the entity classes, start a fresh database and regenerate migrations: npx squid-typeorm-codegen docker compose down docker compose up -d rm -r db/migrations npx squid-typeorm-migration generate Apply the migrations with npx squid-typeorm-migration apply 2. Generate Typescript ABI modules ​ Use evm-typegen to generate the facade classes, for example like this: npx squid-evm-typegen src/abi 0x2E645469f354BB4F5c8a05B3b30A929361cf77eC #Gravity --clean 3. Configuration ​ See Configuration section for more details. const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setFinalityConfirmation ( 75 ) . setBlockRange ( { from : 6175243 } ) // fetch logs emitted by '0x2E645469f354BB4F5c8a05B3b30A929361cf77eC' // matching either `NewGravatar` or `UpdatedGravatar` . addLog ( { address : [ '0x2E645469f354BB4F5c8a05B3b30A929361cf77eC' ] , topic0 : [ events . NewGravatar . topic , events . UpdatedGravatar . topic , ] } ) 4. Iterate over the batch items and group events ​ The following code snippet illustrates a typical data transformation in a batch. The strategy is to Iterate over ctx.blocks and block.logs Decode each log using a suitable facade class Enrich and transform the data Upsert arrays of entities in batches using ctx.save() The processor.run() method the looks as follows: processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { // storing the new/updated entities in // an in-memory identity map const gravatars : Map < string , Gravatar > = new Map ( ) // iterate over the data batch stored in ctx.blocks for ( const c of ctx . blocks ) { for ( const log of c . logs ) { // decode the log data const { id , owner , displayName , imageUrl } = extractData ( log ) // transform and normalize to match the target entity (Gravatar) gravatars . set ( id . toHexString ( ) , new Gravatar ( { id : id . toHexString ( ) , owner : decodeHex ( owner ) , displayName , imageUrl } ) ) } } // Upsert the entities that were updated. // Note that store.save() automatically updates // the existing entities and creates new ones. // It splits the data into suitable chunks to // guarantee an adequate performance. await ctx . store . save ( [ ... gravatars . values ( ) ] ) } ) ; In the snippet above, we decode both NewGravatar and UpdatedGravatar with a single helper function that uses the generated events facade class: function extractData ( evmLog : any ) : { id : ethers . BigNumber , owner : string , displayName : string , imageUrl : string } { if ( evmLog . topics [ 0 ] === events . NewGravatar . topic ) { return events . NewGravatar . decode ( evmLog ) } if ( evmLog . topics [ 0 ] === events . UpdatedGravatar . topic ) { return events . UpdatedGravatar . decode ( evmLog ) } throw new Error ( 'Unsupported topic' ) } 5. Run the processor and store the transformed data into the target database ​ Run the processor with node -r dotenv/config lib/main.js The script will build the code automatically before running. In a separate terminal window, run npx squid-graphql-server Inspect the GraphQL API at http://localhost:4350/graphql . An end-to-end idiomatic usage of SubstrateBatchProcessor can be inspected in the squid-substrate-template and also learned from more elaborate examples . Here we highlight the key steps and put together a configuration and a data handling definition to illustrate the concepts covered in the squid development guide . Pre-requisites: NodeJS, Git, Docker, Squid CLI , any of the Substrate templates . 1. Set up the processor ​ See Setup section for more details. src/processor.ts import { SubstrateBatchProcessor } from '@subsquid/substrate-processor' export const processor = new SubstrateBatchProcessor ( ) // set the data source . setGateway ( 'https://v2.archive.subsquid.io/network/kusama' ) . setRpcEndpoint ( 'https://kusama-rpc.dwellir.com' ) // add data requests . addEvent ( { name : [ 'Balances.Transfer' ] , extrinsic : true } ) . addCall ( { name : [ 'Balances.transfer_keep_alive' ] , extrinsic : true } ) // define the data // to be fetched for each data item . setFields ( { extrinsic : { hash : true , fee : true } , call : { success : true } , block : { timestamp : true } } ) 2. Define a custom data facade and extract normalized data from BatchContext ​ The following code snippet illustrates how the data is extracted and normalized into some user-specific facade interface TransferData . Note how the data items in each of the block data iterables ( events , calls , extrinsics ) are filtered in the batch handler to make sure they match the data requests. Type-safe access and decoding of the call and event data is done with the help of the classes generated by a suitable typegen tool , in this case squid-substrate-typegen . src/main.ts import { Store } from '@subsquid/typeorm-store' import * as ss58 from '@subsquid/ss58' import { events } from './types' import { ProcessorContext } from './processor' // some normalized user-define data interface TransferData { id : string timestamp : Date | undefined extrinsicHash : string | undefined from : string to : string amount : bigint fee ? : bigint } // extract and normalize function getTransfers ( ctx : ProcessorContext < Store > ) : TransferData [ ] { let transfers : TransferData [ ] = [ ] for ( let block of ctx . blocks ) { for ( let event of block . events ) { if ( event . name === 'Balances.Transfer' ) { // decode the event data with runtime-versioned // classes generated by typegen let rec : { from : string , to : string , amount : bigint } if ( events . balances . transfer . v1020 . is ( event ) ) { let [ from , to , amount , ] = events . balances . transfer . v1020 . decode ( event ) rec = { from , to , amount } } else if ( events . balances . transfer . v1050 . is ( event ) ) { let [ from , to , amount ] = events . balances . transfer . v1050 . decode ( event ) rec = { from , to , amount } } else { rec = events . balances . transfer . v9130 . decode ( event ) } // extract and normalize the `item.event` data let timestamp = block . header . timestamp == null ? undefined : new Date ( block . header . timestamp ) transfers . push ( { id : event . id , timestamp , extrinsicHash : event . extrinsic ?. hash , from : ss58 . codec ( 'kusama' ) . encode ( rec . from ) , to : ss58 . codec ( 'kusama' ) . encode ( rec . to ) , amount : rec . amount , fee : event . extrinsic ?. fee || 0n } ) } } for ( let call of block . calls ) { if ( call . name === 'Balances.transfer_keep_alive' ) { // extranct and normalize the call data transfer . push ( { /* some data manipulation on the call data */ } ) } } } return transfers } 3. Run the processor and store the transformed data into the target database ​ The snippet below assumes that we are using TypeormDatabase and that the database schema together with the model entity types has already been prepared. Consult the Schema file section for more details. src/main.ts import { TypeormDatabase } from '@subsquid/typeorm-store' // an entity type generated from the schema file import { Transfer } from './model' import { processor } from './processor' // ... `getTransfers(Ctx)` definition processor . run ( new TypeormDatabase ( ) , async ctx => { // get the normalized data from the context let transfersData = getTransfers ( ctx ) // an array of entity class instances let transfers : Transfer [ ] = [ ] for ( let t of transfersData ) { let { id , blockNumber , timestamp , extrinsicHash , amount , fee } = t // join some extra data let from = getAccount ( accounts , t . from ) let to = getAccount ( accounts , t . to ) // populate entity class instances // with the extracted data transfers . push ( new Transfer ( { id , blockNumber , timestamp , extrinsicHash , from , to , amount , fee } ) ) } // persist an array of entities // in a single statement await ctx . store . insert ( transfers ) } ) Edit this page Previous Frontier EVM-indexing squid Next Case studies 1. Model the target schema and generate entity classes 2. Generate Typescript ABI modules 3. Configuration 4. Iterate over the batch items and group events 5. Run the processor and store the transformed data into the target database 1. Set up the processor 2. Define a custom data facade and extract normalized data from BatchContext 3. Run the processor and store the transformed data into the target database
Event logs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM Event logs On this page Event logs addLog(options) ​ Get event logs emitted by some or all contracts in the network. options has the following structure: { // data requests address ? : string [ ] topic0 ? : string [ ] topic1 ? : string [ ] topic2 ? : string [ ] topic3 ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval transaction ? : boolean transactionLogs ? : boolean transactionTraces ? : boolean } Data requests: address : the set of addresses of contracts emitting the logs. Omit to subscribe to events from all contracts in the network. topicN : the set of values of topicN. range : the range of blocks to consider. Related data retrieval: transaction = true : the processor will retrieve all parent transactions and add them to the transactions iterable within the block data . Additionally it will expose them via the .transaction field of each log item. transactionLogs = true : the processor will retrieve all "sibling" logs, that is, all logs emitted by transactions that emitted at least one matching log. The logs will be exposed through the regular logs block data iterable and via .transaction.logs for matching logs. transactionTraces = true : the processor will retrieve the traces for all transactions that emitted at least one matching log. The traces will be exposed through the regular traces block data iterable and via .transaction.traces . Note that logs can also be requested by addTransaction() and addTrace() method as related data. Selection of the exact data to be retrieved for each log and its optional parent transaction is done with the setFields() method documented on the Field selection page. Some examples are available below. Examples ​ Fetch NewGravatar(uint256,address,string,string) and UpdateGravatar(uint256,address,string,string) event logs emitted by 0x2E645469f354BB4F5c8a05B3b30A929361cf77eC . For each log, fetch topic set, log data. Fetch parent transactions with their inputs. const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setFinalityConfirmation ( 75 ) . addLog ( { address : [ '0x2e645469f354bb4f5c8a05b3b30a929361cf77ec' ] , topic0 : [ // topic: 'NewGravatar(uint256,address,string,string)' '0x9ab3aefb2ba6dc12910ac1bce4692cf5c3c0d06cff16327c64a3ef78228b130b' , // topic: 'UpdatedGravatar(uint256,address,string,string)' '0x76571b7a897a1509c641587568218a290018fbdc8b9a724f17b77ff0eec22c0c' , ] , transaction : true } ) . setFields ( { log : { topics : true , data : true } , transaction : { input : true } } ) tip Typescript ABI modules generated by squid-evm-typegen provide event signatures/topic0 values as constants, e.g. import * as gravatarAbi from './abi/gravatar' // ... topic0 : [ gravatarAbi . events . NewGravatar . topic , gravatarAbi . events . UpdatedGravatar . topic , ] , // ... Fetch every Transfer(address,address,uint256) event on Ethereum mainnet where topic2 is set to the destination address (a common but non-standard practice) and the destination is vitalik.eth a.k.a. 0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045 . For each log, fetch transaction hash. const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setFinalityConfirmation ( 75 ) . addLog ( { topic0 : [ // topic0: 'Transfer(address,address,uint256)' '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' ] , topic2 : [ // vitalik.eth '0x000000000000000000000000d8da6bf26964af9d7eed9e03e53415d37aa96045' ] } ) . setFields ( { log : { transactionHash : true } } ) tip As you may observe, the address in the topic2 is a bit longer than usual ( 0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045 , 42 chars).
This is caused by the fact that Squid SDK expects Bytes32[] ; therefore, the length has to be 66 chars long.
The possible quick fix is to pad the original address with zeros and prepend 0x . const address = '0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045' const topic = '0x' + address . replace ( 'x' , '0' ) . padStart ( 64 , '0' ) . toLowerCase ( ) Edit this page Previous General settings Next Transactions Examples
sqd run | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd run On this page sqd run Run a squid locally according to the deployment manifest . sqd run PATH Notes: The command is especially useful for running multichain squids , as it runs all services in the same terminal and handles failures gracefully. sqd run PATH â€‹ Run a squid project locally USAGE $ sqd run PATH [--interactive] [-m <value>] [-f <value>] [-i <value>... | -e <value>...] [-r <value>] FLAGS -e, --exclude=<value>...  Do not run specified services -f, --envFile=<value>     [default: .env] Relative path to an additional environment file -i, --include=<value>...  Run only specified services -m, --manifest=<value>    [default: squid.yaml] Relative path to a squid manifest file -r, --retries=<value>     [default: 5] Attempts to restart failed or stopped services --[no-]interactive    Disable interactive mode See code: src/commands/run.ts Edit this page Previous sqd restart Next sqd secrets sqd run PATH
Cheatsheet | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing Cheatsheet On this page CLI cheatsheet This guide provides a quick reference to the commands needed to launch the squid and manage the database. Install dependencies ​ npm i Compile the project ​ npx tsc Launch Postgres database to store the data ​ docker compose up -d Apply database migrations to create the target schema ​ npx squid-typeorm-migration apply Run indexer ​ node -r dotenv/config lib/main.js Check out the indexed swaps ​ docker exec "$(basename " $( pwd ) " ) -db-1 " psql -U postgres \ -c " SELECT id, logs_count, found_at FROM contract ORDER BY logs_count desc LIMIT 10 " You can use the sqd utility to shorten these commands and manage their interrelations. Learn more here . Edit this page Previous Field selection Next Network API Install dependencies Compile the project Launch Postgres database to store the data Apply database migrations to create the target schema Run indexer Check out the indexed swaps
Data requests | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Substrate Block data for Substrate General settings Data requests Fields selection Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors Substrate Data requests On this page Data requests warning Setting a data request field to [] selects no data . Pass undefined for a wildcard selection: . addEvent ( { name : [ ] } ) // selects no events . addEvent ( { } ) // selects all events tip All array-valued data request fields (e.g. name?: string[] of addEvent() are arrays of acceptable values. That means that you can request e.g. multiple events with a single addEvent() call. Requesting data that way is more efficient than requesting data with many individual calls. Events ​ addEvent(options) : Subscribe to Substrate runtime events. options has the following structure: { // data requests name ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval call ? : boolean stack ? : boolean extrinsic ? : boolean } name s must follow the convention ${Pallet}.${NameWithinPallet} with both parts usually capitalized, e.g. Balances.Transfer . You may also request the data related to the events: with call = true the processor will retrieve the parent call and add it to the calls iterable within the block data ; with stack = true it will do that with all calls in the entire call stack; with extrinsic = true it will add the parent extrinsic to the extrinsics block data iterable. Note than events can also be requested by the addCall() method as related data. Selection of the exact data to be retrieved for each log and its optional parent transaction is done with the setFields() method documented on the Fields selection page. Calls ​ addCall(options) : Subscribe to runtime calls (even if wrapped into a system.sudo or util.batch extrinsic). options has the following structure: { // data requests name ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval events ? : boolean stack ? : boolean extrinsic ? : boolean } The name must follow the convention ${Pallet}.${call_name} . The pallet name is normally capitalized, and the call name is in the snake_case format, as in Balances.transfer_keep_alive . By default, both successful and failed calls are fetched. Select the call.success field and later check it within the batch handler if you need to disambiguate. You may also request the data related to the calls: with events = true the processor will retrieve all the events that the call emitted and add them to the events iterable within the block data ; with stack = true it will add all calls in the stack of each matching call, including itself, to the calls iterable; with extrinsic = true it will add the parent extrinsic to the extrinsics block data iterable. Note than calls can also be requested by the addEvent() method as related data. Selection of the exact data to be retrieved for each log and its optional parent transaction is done with the setFields() method documented on the Fields selection page. Specialized setters ​ addEvmLog(options) ​ A SubstrateBatchProcessor configuration setter that subscribes it to EVM.Log events by contract address(es) and/or EVM log topics. options have the following structure: { // data requests address ? : string [ ] topic0 ? : string [ ] topic1 ? : string [ ] topic2 ? : string [ ] topic3 ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval call ? : boolean stack ? : boolean extrinsic ? : true } Related data retrieval and field selection are identical to addEvent() . addEthereumTransaction(options) ​ A SubstrateBatchProcessor configuration setter that subscribes it to Ethereum.transact calls by contract address(es) and/or function sighashes . options have the following structure: { // data requests to ? : string [ ] // contract addresses sighash ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval events : boolean stack : boolean extrinsic : boolean } Related data retrieval and field selection are identical to that addCall() . The processor with fetch both successful and failed transactions. Further, there's a difference between the success of a Substrate call and the internal EVM transaction. The transaction may fail even if the enclosing Substrate call has succeeded. Use events = true to retrieve Ethereum.Executed events that can be used to figure out the EVM transaction status (see getTransactionResult() ). addContractsContractEmitted(options) ​ Subscribe to the ink! events ( Contracts.ContractEmitted ) of the WASM runtime. options has the following structure: { // data requests contractAddress ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval call ? : boolean stack ? : boolean extrinsic ? : boolean } Contract addresses must be specified as hex strings, so make sure to decode them if yours are encoded with ss58. Related data retrieval and field selection are identical to addEvent() . addGearMessageQueued(options) ​ addGearUserMessageSent(options) ​ Structure of options is identical for both methods: { // data requests programId ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval call ? : boolean stack ? : boolean extrinsic ? : boolean } The methods above subscribe to the events Gear.MessageQueued and Gear.UserMessageSent emitted by the specified Gear program. Related data retrieval and field selection are identical to addEvent() . Edit this page Previous General settings Next Fields selection Events Calls Specialized setters
Topics specific to EVM | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Factory contracts Proxy contracts Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides EVM-specific Topics specific to EVM üìÑÔ∏è Factory contracts Indexing a dynamic set of contracts üìÑÔ∏è Proxy contracts Indexing proxy contracts Previous Saving to BigQuery Next Factory contracts
sqd tags | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd tags On this page sqd tags Manage squid deployements' tags. See the slots and tags guide . sqd tags add sqd tags remove sqd tags add ​ Add a tag to a squid deployment / slot USAGE $ sqd tags add TAG [--interactive] [--allow-tag-reassign] [-r [<org>/]<name>(@<slot>|:<tag>) | -o <code> | [-s <slot> -n <name>] | [-t <tag> ]] ARGUMENTS TAG  New tag to assign FLAGS --allow-tag-reassign  Allow reassigning an existing tag --[no-]interactive    Disable interactive mode SQUID FLAGS -n, --name=<name>                               Name of the squid -r, --reference=[<org>/]<name>(@<slot>|:<tag>)  Fully qualified reference of the squid. It can include the organization, name, slot, or tag -s, --slot=<slot>                               Slot of the squid -t, --tag=<tag>                                 Tag of the squid ORG FLAGS -o, --org=<code>  Code of the organization See code: src/commands/tags/add.ts sqd tags remove ​ Remove a tag from a squid deployment / slot USAGE $ sqd tags remove TAG [--interactive] [-r [<org>/]<name>(@<slot>|:<tag>) | -o <code> | [-s <slot> -n <name>] | [-t <tag> ]] ARGUMENTS TAG  New tag to assign FLAGS --[no-]interactive  Disable interactive mode SQUID FLAGS -n, --name=<name>                               Name of the squid -r, --reference=[<org>/]<name>(@<slot>|:<tag>)  Fully qualified reference of the squid. It can include the organization, name, slot, or tag -s, --slot=<slot>                               Slot of the squid -t, --tag=<tag>                                 Tag of the squid ORG FLAGS -o, --org=<code>  Code of the organization See code: src/commands/tags/remove.ts Edit this page Previous sqd secrets Next sqd whoami sqd tags add sqd tags remove
DoS protection | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Caching Custom API extensions Subscriptions DoS protection Access control Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Configuration DoS protection On this page DoS protection The squid GraphQL API server accepts the following optional start arguments to fend off heavy queries. To enable the protection, add the corresponding flags to serve and serve:prod command definitions at commands.json : ... "serve" : { "description" : "Start the GraphQL API server" , "cmd" : [ "squid-graphql-server" , "--max-root-fields" , "10" , "--max-response-size" , "1000" ] } , "serve:prod" : { "description" : "Start the GraphQL API server in prod" , "cmd" : [ "squid-graphql-server" , "--max-root-fields" , "10" , "--max-response-size" , "1000" , "--dumb-cache" , "in-memory" ] } , ... When deploying to SQD Cloud , make sure to use serve:prod to start the GraphQL api service in the deployment manifest : squid.yaml # ... deploy : # other services ... api : cmd : [ "sqd" , "serve:prod" ] --max-request-size <kb> The argument limits the size of a request in kilobytes. It is set to 256kb by default. --max-root-fields <count> The maximal allowed number of root-level queries in a single GraphQL request. --max-response-size <nodes> This option limits the estimated query response size and makes server return an error if it exceeds the provided value. Note that the estimated size depends only on the decorators in schema.graphql and the requested fields. The estimate is the product of the cardinality of the entity list and the response item weight. The cardinality is estimated as the minimum of the limit argument of the query ( Infinity if not provided) @cardinality value defined in schema.graphql (if the requested entity type is decorated in the schema file, Infinity otherwise) the size of the argument list of the _eq and id_in filters in the where clause (if applicable) In particular, if there are no @cardinality decorators in schema.graphql , the client queries must explicitly provide limits or where filters to pass through. The response item weight is calculated recursively: byteWeight for each scalar field or 1 if it's not decorated for non-scalar fields, the estimated weight times the estimated cardinality (if it's a list) each non-leaf node in the query AST tree adds a weight of 1 In a nutshell, assuming that the schema file is properly decorated with @cardinality and @byteWeight , the estimated response size should roughly be at the same scale as the byte size of the query result. --subscription-max-response-size <nodes> Same as --max-response-size but for live query subscriptions . Example â€‹ Assume the schema is defined as follows, and the server is launched with --max-response-size 1000 . schema.graphql type Foo @ entity { id : ID ! // a large string bigField : String ! @ byteWeight ( value : 1000.0 ) bar : Bar ! } type Bar @ entity { id : ID ! // bar.foos typically contain about 100 items foos : [ Foo ! ] ! @ derivedFrom ( field : "bar" ) @ cardinality ( value : 100 ) bazs : [ Baz ! ] ! @ derivedFrom ( field : "bar" ) } // there are around 100 entities of type Baz type Baz @ entity @ cardinality ( value : 100 ) { id : ID ! bar : Bar ! } The following queries will be bounced: query A { bars { id } } query B { bars ( limit : 1001 ) { id } } query C { bars ( limit : 100 ) { id foos ( limit : 10 ) { id } } } query D { bars ( limit : 10 ) { id foos { id } } } query E { foos ( id_eq : "1" ) { id bigField } } The estimated cardinality of query A is Infinity The estimated cardinality of query B 1001 and so the expected size exceeds the limit The estimated cardinality of query C is 100 while the item size is 13 , so the size is estimated to 1300 . The estimated cardinality of query D is 10 while the item size is 103 , so the size is estimated to 1030 . The estimated cardinality of query E is 1 while the item size is 1001 (due to bigField having weight 1000 ). At the same time, the following queries will go through: query A { bars ( limit : 100 ) { id } } query B { bars ( limit : 3 ) { id foos { id } bazs { id } } } query C { bars { id bar { foos ( where : { id_in [ "1" , "2" ] } ) { id } id } } } Edit this page Previous Subscriptions Next Access control
Balances | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Balances On this page Balances addBalance(options) â€‹ This allows for tracking SOL account balances. options has the following structure: { // data requests where ? : { account ? : string [ ] } // related data retrieval include ? : { transaction ? : boolean transactionInstructions ? : boolean } range ? : { from : number to ? : number } } The data requests here are: account : the set of accounts to track. Leave undefined to subscribe to balance updates of all accounts in the whole network. Related data retrieval flags: transaction = true : retrieve the transaction that gave rise to the balance update transactionInstructions = true : retrieve all instructions executed by the parent transaction The related data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/solana-objects on the block data to populate the convenience reference fields like instruction.inner . Selection of the exact fields to be retrieved for each balance item and the related data is done with the setFields() method documented on the Field selection page. Edit this page Previous Log messages Next Token balances
Traces | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM Traces On this page Traces tip Traces for historical blocks are currently available from SQD Network on the same basis as all other data stored there: for free. If you deploy a squid that indexes traces in real-time to SQD Cloud and use our RPC addon , the necessary trace_ or debug_ RPC calls made will be counted alongside all other calls and the price will be computed for the total count. There are no surcharges for traces or state diffs. addTrace(options) ​ Subscribe to execution traces . This allows for tracking internal calls. The options object has the following structure: { // data requests callTo ? : string [ ] callFrom ? : string [ ] callSighash ? : string [ ] createFrom ? : string [ ] rewardAuthor ? : string [ ] suicideRefundAddress ? : string [ ] type ? : string [ ] range ? : { from : number , to ? : number } // related data retrieval transaction ? : boolean transactionLogs ? : boolean subtraces ? : boolean parents ? : boolean } The data requests here are: type : get traces of types from this set. Allowed types are 'create' | 'call' | 'suicide' | 'reward' . callTo : get call traces to the addresses in this set. callFrom : get call traces from the addresses in this set. callSighash : get call traces with signature hashes in this set. createFrom : get create traces from the addresses in this set. rewardAuthor : get reward traces where block authors are in this set. suicideRefundAddress : get suicide traces where refund addresses in this set. range : get traces from transactions from this range of blocks. Related data retrieval: transaction = true will cause the processor to retrieve transactions that the traces belong to. transactionLogs = true will cause the processor to retrieve all logs emitted by transactions that the traces belong to. subtraces = true will cause the processor to retrieve downstream traces in addition to those that matched the data requests. parents = true will cause the processor to retrieve upstream traces in addition to those that matched the data requests. These extra data items will be added to the appropriate iterables within the block data . Note that traces can also be requested by addTransaction() and addLog() method as related data. Selection of the exact data to be retrieved for each trace item is done with the setFields() method documented on the Field selection page. Be aware that field selectors for traces do not share their names with the fields of trace data items, unlike field selectors for other data item types. This is due to traces varying their structure depending on the value of the type field. Examples ​ Exploring internal calls of a given transaction ​ For a mint call to Uniswap V3 Positions NFT . import { EvmBatchProcessor } from '@subsquid/evm-processor' import { TypeormDatabase } from '@subsquid/typeorm-store' const TARGET_TRANSACTION = '0xf178718219151463aa773deaf7d9367b8408e35a624550af975e089ca6e015ca' const TO_CONTRACT = '0xc36442b4a4522e871399cd717abdd847ab11fe88' // Uniswap v3 Positions NFT const METHOD_SIGHASH = '0x88316456' // mint const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setFinalityConfirmation ( 75 ) . setBlockRange ( { from : 16962349 , to : 16962349 } ) . addTransaction ( { to : [ TO_CONTRACT ] , sighash : [ METHOD_SIGHASH ] , traces : true } ) . setFields ( { trace : { callTo : true } } ) processor . run ( new TypeormDatabase ( ) , async ctx => { let involvedContracts = new Set < string > ( ) let traceCount = 0 for ( let block of ctx . blocks ) { for ( let trc of block . traces ) { if ( trc . type === 'call' && trc . transaction ?. hash === TARGET_TRANSACTION ) { involvedContracts . add ( trc . action . to ) traceCount += 1 } } } console . log ( ` txn ${ TARGET_TRANSACTION } had ${ traceCount - 1 } internal transactions ` ) console . log ( ` ${ involvedContracts . size } contracts were involved in txn ${ TARGET_TRANSACTION } : ` ) involvedContracts . forEach ( c => { console . log ( c ) } ) } ) Grabbing addresses of all contracts ever created on Ethereum ​ Full code is available in this branch . WARNING: will contain addresses of some contracts that failed to deploy. import { EvmBatchProcessor } from '@subsquid/evm-processor' import { TypeormDatabase } from '@subsquid/typeorm-store' import { CreatedContract } from './model' const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setFields ( { trace : { createResultAddress : true , } , } ) . addTrace ( { type : [ 'create' ] , transaction : true , } ) processor . run ( new TypeormDatabase ( { supportHotBlocks : false } ) , async ( ctx ) => { const contracts : Map < string , CreatedContract > = new Map ( ) const addresses : Set < string > = new Set ( ) for ( let c of ctx . blocks ) { for ( let trc of c . traces ) { if ( trc . type === 'create' && trc . result ?. address != null && trc . transaction ?. hash !== undefined ) { contracts . set ( trc . result . address , new CreatedContract ( { id : trc . result . address } ) ) } } } await ctx . store . upsert ( [ ... contracts . values ( ) ] ) } ) Currently there is no convenient way to check whether a trace had effect on the chain state, but this feature will be added in future releases. Edit this page Previous Storage state diffs Next Field selection Examples Exploring internal calls of a given transaction Grabbing addresses of all contracts ever created on Ethereum
Saving to PostgreSQL | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data Store interface Saving to PostgreSQL Saving to filesystems Saving to BigQuery EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Persisting data Saving to PostgreSQL On this page Saving to PostgreSQL TypeormDatabase context store provides a wrapper over the TypeORM EntityManager optimized for batch saving. It currently supports only Postgres-compatible databases and seamlessly integrates with entity classes generated from the schema file . Check out this section of the reference page to learn how to specify the database connection parameters. Usage ​ import { Store , TypeormDatabase } from '@subsquid/typeorm-store' import { FooEntity } from './model' const dbOptions = { /* ... constructor options ... */ } processor . run ( new TypeormDatabase ( dbOptions ) , async ctx => { // ... await ctx . store . upsert ( [ new FooEntity ( { id : '1' } ) , new FooEntity ( { id : '2' } ) ] ) } ) Here, FooEntity represents a TypeORM entity class. In squids, these are typically generated from schema files with squid-typeorm-codegen . TypeormDatabase constructor options govern the behavior of the class at the highest level. They are described in this section of the reference . ctx.store is of type Store . See the Store interface section of the reference page for more details. Database migrations ​ The Squid SDK manages the database schema with TypeORM-based database migrations by means of the squid-typeorm-migration(1) tool.
The tool auto-generates the schema migrations from the TypeORM entities created by codegen , so that custom migration scripts are rarely needed. Here are some useful commands: npx squid-typeorm-migration apply # Apply the DB migrations npx squid-typeorm-migration generate # Generate a DB migration matching the TypeORM entities rm -r db/migrations # Clean the migrations folder Inspect the full list of available options with npx squid-typeorm-migration --help To generate or update the migrations after a schema change, follow the steps below. Updating after schema changes ​ In most cases the simplest way to update the schema is to drop the database and regenerate the migrations from scratch. 1. Update schema.graphql 2. Regenerate the TypeORM entity classes # generate entites code from the schema file npx squid-typeorm-codegen 3. Recreate the database # drop the database docker compose down # start a blank database docker compose up -d Note that without dropping the database the next step will generate a migration only for the schema difference. 4. Build the squid code npm run build 5. Recreate the database migration rm -r db/migrations npx squid-typeorm-migration generate 6. Apply the database migration npx squid-typeorm-migration apply Updating a deployed squid schema ​ In some rare cases it is possible to update the schema without dropping the database and restarting the squid from a blank state. The most important case is adding an index to an entity field. More complex changes are usually not feasible. Updating a running squid requires that you add an incremental migration. 1. Ensure that your local database is running and has the same schema as the database of your Cloud squid In most situations re-creating the database container and applying existing migrations should be enough: docker compose down docker compose up -d npx squid-typeorm-migration apply 2. Update schema.graphql For example, add an index 3. Regenerate the model classes and build the code npx squid-typeorm-codegen npm run build 4. Add a new database migration npx squid-typeorm-migration generate This will create a new file in db/migrations . You may want to examine it before proceeding to the next step. 5. Update the squid in Cloud If the squid is deployed to SQD Cloud, update the deployed version . If you're self-hosting it, update your remote codebase and run npx squid-typeorm-migration apply SQD Cloud deployment ​ By default, the TypeORM migrations are automatically applied by Cloud with the command npx squid-typeorm-migration apply before the squid services are started. For custom behavior, one can override the migration script using the optional migrate: section of squid.yaml . info To force Cloud to reset the database and start with a blank state after a schema change, use the --hard-reset flag of sqd deploy . Edit this page Previous Store interface Next Saving to filesystems Usage Database migrations Updating after schema changes Updating a deployed squid schema SQD Cloud deployment
RPC addon | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources RPC addon On this page RPC addon List of available networks and their aliases is available on the reference page. SQD Cloud provides a built-in RPC service. The service is available as an addon to all squids deployed to the Cloud, although playground squids only have access to a limited number of calls. Professional organizations do not have that limitation and can access their RPC endpoints from outside of the Cloud. Enable the RPC add-on in the manifest like this: deploy : addons : rpc : - eth - goerli.http Processor configuration ​ With the add-on successfully enabled, your squid will get a unique proxied endpoint to the requested network. SQD Cloud will make its URL available to the deployed squid at the RPC_${Upper(network)}_${Upper(protocol)} environment variable. Assert it to avoid compilation errors. We also recommend rate limiting RPC addon requests on the processor side to the same rate as was used in the manifest: import { assertNotNull } from '@subsquid/util-internal' processor . setRpcEndpoint ( { // dash in "eth-goerli" becomes an underscore url : assertNotNull ( process . env . RPC_ETH_GOERLI_HTTP ) , rateLimit : 10 } ) External access ​ Professional organizations can access their RPC proxies from outside of the SQD Cloud. Among other things, this enables a development process that uses the service for both local and Cloud runs transparently. Here an example configuration for Ethereum Mainnet: Here is an example configuration for Ethereum Mainnet Add the following variable to .env : # get the url from https://app.subsquid.io/rpc/chains/eth RPC_ETH_HTTP = < your_rpc_url_for_external_use > Enable the RPC addon in squid.yaml: deploy : addons : rpc : - eth.http This will add and populate the RPC_ETH_HTTP variable for Cloud deployments. Configure the processor to use the URL from from RPC_ETH_HTTP : import { EvmBatchProcessor } from '@subsquid/evm-processor' import { assertNotNull } from '@subsquid/util-internal' export const processor = new EvmBatchProcessor ( ) . setRpcEndpoint ( assertNotNull ( process . env . RPC_ETH_HTTP ) ) // ...the rest of the processor configuration Pricing ​ RPC addon requests are priced at a flat rate , with substantial packages included for free for all organization types . Pricing does not depend on the call method. Edit this page Previous Query optimization Next Portal for EVM+Substrate Processor configuration External access Pricing
sqd deploy | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd deploy On this page sqd deploy Deploy new or update an existing squid deployment in the Cloud. Squid name and also optionally slot and/or tag are taken from the provided deployment manifest. sqd deploy SOURCE sqd deploy SOURCE â€‹ Deploy new or update an existing squid in the Cloud USAGE $ sqd deploy SOURCE [--interactive] [-r [<org>/]<name>(@<slot>|:<tag>) | -o <code> | -n <name> | [-s <slot>] | [-t <tag>]] [-m <manifest_path>] [--hard-reset] [--stream-logs] [--add-tag <value>] [--allow-update] [--allow-tag-reassign] [--allow-manifest-override] ARGUMENTS SOURCE  [default: .] Squid source. Could be: - a relative or absolute path to a local folder (e.g. ".") - a URL to a .tar.gz archive - a github URL to a git repo with a branch or commit tag FLAGS -m, --manifest=<manifest_path>  [default: squid.yaml] Specify the relative local path to a squid manifest file in the squid working directory --add-tag=<value>           Add a tag to the deployed squid --allow-manifest-override   Allow overriding the manifest during deployment --allow-tag-reassign        Allow reassigning an existing tag --allow-update              Allow updating an existing squid --hard-reset                Perform a hard reset before deploying. This will drop and re-create all squid resources, including the database, causing a short API downtime --[no-]interactive          Disable interactive mode --[no-]stream-logs          Attach and stream squid logs after the deployment SQUID FLAGS -n, --name=<name>                               Name of the squid -r, --reference=[<org>/]<name>(@<slot>|:<tag>)  Fully qualified reference of the squid. It can include the organization, name, slot, or tag -s, --slot=<slot>                               Slot of the squid -t, --tag=<tag>                                 Tag of the squid ORG FLAGS -o, --org=<code>  Code of the organization DESCRIPTION Deploy new or update an existing squid in the Cloud EXAMPLES // Create a new squid with name provided in the manifest file $ sqd deploy . // Create a new squid deployment and override it's name to "my-squid-override" $ sqd deploy . -n my-squid-override // Update the "my-squid" squid with slot "asmzf5" $ sqd deploy . -n my-squid -s asmzf5 // Use a manifest file located in ./path-to-the-squid/squid.prod.yaml $ sqd deploy ./path-to-the-squid -m squid.prod.yaml // Full paths are also fine $ sqd deploy /Users/dev/path-to-the-squid -m /Users/dev/path-to-the-squid/squid.prod.yaml See code: src/commands/deploy.ts Edit this page Previous sqd autocomplete Next sqd explorer sqd deploy SOURCE
sqd logs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd logs On this page sqd logs Fetch logs from a squid deployed to the Cloud sqd logs sqd logs â€‹ Fetch logs from a squid deployed to the Cloud USAGE $ sqd logs [--interactive] [--since <value>] [--search <value>] [-f | -p <value>] [-r [<org>/]<name>(@<slot>|:<tag>) | -o <code> | [-s <slot> -n <name>] | [-t <tag> ]] [-c processor|query-node|api|db-migrate|db...] [-l error|debug|info|warning...] [--since <value>] FLAGS -c, --container=<option>...  Container name <options: processor|query-node|api|db-migrate|db> -f, --follow                 Follow -l, --level=<option>...      Log level <options: error|debug|info|warning> -p, --pageSize=<value>       [default: 100] Logs page size --[no-]interactive       Disable interactive mode --search=<value>         Filter by content --since=<value>          [default: 1d] Filter by date/interval SQUID FLAGS -n, --name=<name>                               Name of the squid -r, --reference=[<org>/]<name>(@<slot>|:<tag>)  Fully qualified reference of the squid. It can include the organization, name, slot, or tag -s, --slot=<slot>                               Slot of the squid -t, --tag=<tag>                                 Tag of the squid ORG FLAGS -o, --org=<code>  Code of the organization Notes: --since accepts the notation of the ms library : 1d, 10h, 1m. With --level=error sqd will fetch logs emitted by three different calls to the SDK Logger , namely ctx.log.error ctx.log.fatal ctx.log.trace See code: src/commands/logs.ts Edit this page Previous sqd list Next prod sqd logs
ArrowSquid for EVM | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Migrate from The Graph ArrowSquid for EVM ArrowSquid for Substrate hasura-configuration tool v2 Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Migration guides ArrowSquid for EVM On this page Migrate to ArrowSquid (EVM) This is a EVM guide. Substrate guide is available here . ArrowSquid refers to the versions @subsquid/ [email protected] and @subsquid/ [email protected] . ArrowSquid is not compatible with the FireSquid archive endpoints. Instead, it uses SQD Network gateways (see the Supported EVM networks page). The main feature introduced by the ArrowSquid update on EVM is the new ability of the processor to ingest unfinalized blocks directly from a network node, instead of waiting for the archive to ingest and serve it first. The processor can now handle forks and rewrite the contents of its database if it happens to have indexed orphaned blocks. This allows SQD-based APIs to become near real-time and respond to the on-chain activity with subsecond latency. Another major feature introduced by ArrowSquid is the support for transaction execution receipts, traces and state diffs . It enables a significantly more fine-grained control over the smart contract states, especially in the situations when the EVM log data is insufficient. For example, one can: Reliably index transaction data, taking into account the transaction status Keep track of internal calls Observe smart contract state changes even if they are caused by internal transactions Track smart contract creation and destruction The EvmBatchProcessor configuration and data selection interfaces has been simplified and the way in which the data is fetched has been made more efficient. End-to-end ArrowSquid examples can be found in the SDK repo and in the EVM examples section. Here is a step-by-step guide for migrating a squid built with an older SDK version to the post-ArrowSquid tooling. Step 1 ​ Update all packages affected by the update: npm i @subsquid/evm-processor@next npm i @subsquid/typeorm-store@next If your squid uses file-store , please update any related packages to the @next version, too. Step 2 ​ Replace the old setDataSource() processor configuration call with a combination of setGateway() and setRpcEndpoint() . Use a public SQD Network gateway URL for your network . If your squid did not use an RPC endpoint before, find one for your network and set it with setRpcEndpoint() . Also configure the network-specific number of transaction confirmations sufficient for finality. For Ethereum mainnet your edit might look like this: processor - .setDataSource({ - archive: lookupArchive('eth-mainnet', {release: 'FireSquid'}) - }) + .setGateway('https://v2.archive.subsquid.io/network/ethereum-mainnet') + .setRpcEndpoint({ + url: '<my_eth_rpc_url>', + rateLimit: 10 + }) + .setFinalityConfirmation(75) We recommend using a private RPC endpoint for the best performance, e.g. from BlastAPI . For squids deployed to SQD Cloud you may also consider using our RPC addon . Your squid will work without an RPC endpoint, but with a significantly increased chain latency (a few hours for most chains, roughly a day for BSC). If that works for you, you can use just the SQD Network gateway without setting an RPC here and skip Step 7 altogether. Step 3 ​ Next, we have to account for the changes in signatures of addLog() and addTransaction() processor methods. Previously, each call of these methods supplied its own fine-grained field selectors. In the new interface, these calls can only enable or disable the retrieval of related data (with boolean flags transaction for addLog() and logs and a few others for addTransaction() ). Field selection is now done by the new setFields() method on a per-item-type basis: once for all log s, once for all transaction s etc. The setting is processor-wide: for example, all transaction s returned by the processor will have the same set of available fields, regardless of whether they are taken from the batch context directly or are accessed from within a log item. Begin migrating to the new interface by finding all calls to addLog() and combining all the evmLog field selectors into a single processor-wide field selector that requests all fields previously requested by individual selectors. Remove the id , logIndex (previously index ) and transactionIndex fields: now they are always available and cannot be requested explicitly. When done, make a call to setFields() and supply the new field selector at the log field of its argument. For example, suppose the processor was initialized with the following three calls: const processor = new EvmBatchProcessor ( ) . addLog ( CONTRACT_ADDRESS , { filter : [ [ abi . events . Transfer . topic ] ] , data : { evmLog : { id : true } } as const } ) . addLog ( CONTRACT_ADDRESS , { filter : [ [ abi . events . Approval . topic ] ] , data : { evmLog : { id : true , data : true } } as const } ) . addLog ( CONTRACT_ADDRESS , { filter : [ [ abi . events . ApprovalForAll . topic ] ] , data : { evmLog : { topics : true } } as const } ) then the new global selector should be added like this: const processor = new EvmBatchProcessor ( ) // ... addLog() calls ... . setFields ( { log : { data : true , topics : true } } ) Be aware that this operation will not increase the amount of data retrieved from SQD Network, since previously such coalescence was done under the hood and all fields were retrieved by the processor anyway. In fact, the amount of data should decrease due to a more efficient transfer mechanism employed by ArrowSquid. See the Field selection page for full documentation on field selectors. Step 4 ​ Repeat step 3 for the transaction field selector. Make sure to check any transaction field selections by addLog() calls in addition to these made by addTransaction() . Remove the default fields id and transactionIndex (previously index ) and add the final field selector to the .setFields() call. For example, suppose the processor was initialized like this: const processor = new EvmBatchProcessor ( ) . addLog ( CONTRACT_ADDRESS , { filter : [ [ abi . events . Transfer . topic ] ] , data : { evmLog : { /* ...some log field selections... */ } , transaction : { hash : true } } as const } ) . addTransaction ( CONTRACT_ADDRESS , { sighash : [ abi . functions . approve . sighash ] , data : { transaction : { gas : true , gasPrice : true } } as const } ) then the new global selector should be added like this: const processor = new EvmBatchProcessor() // ... addLog() and addTransaction() calls ... .setFields({ log: { /* ...some log field selections... */ }, + transaction: { + hash: true, + gas: true, + gasPrice: true + } }) See the Field selection page for full documentation on field selectors. Step 5 ​ Replace the old calls to addLog() and addTransaction() with calls using the new signatures . warning The meaning of passing [] as a set of parameter values has been changed in the ArrowSquid release: now it selects no data . Pass undefined for a wildcard selection: . addLog ( { address : [ ] } ) // selects no logs . addLog ( { } ) // selects all logs . addTransaction ( { from : [ ] } ) // selects no transactions . addTransaction ( { } ) // selects all transactions Old data requests will be erased during the process. Make sure to request the appropriate data with the boolean flags ( transaction for addLog() and logs for addTransaction() ) while doing that. For example, a processor originally initialized like this: const processor = new EvmBatchProcessor ( ) . addLog ( CONTRACT_ADDRESS , { filter : [ [ abi . events . Transfer . topic ] ] , data : { evmLog : { topics : true , data : true } , transaction : { hash : true } } as const } ) . addTransaction ( CONTRACT_ADDRESS , { sighash : [ abi . functions . approve . sighash ] , data : { transaction : { gas : true , gasPrice : true } } as const } ) should now be made with const processor = new EvmBatchProcessor ( ) . addLog ( { address : [ CONTRACT_ADDRESS ] , topic0 : [ abi . events . Transfer . topic ] , transaction : true // IMPORTANT: set this to true whenever the old call defined options.data.transaction } ) . addTransaction ( { to : [ CONTRACT_ADDRESS ] , sighash : [ abi . functions . approve . sighash ] } ) . setFields ( { log : { topics : true , data : true } , transaction : { hash : true , gas : true , gasPrice : true } } ) Step 6 ​ Finally, update the batch handler to use the new batch context . There are two ways to do that: If you're in a hurry, use the transformContext.ts module. Download it with curl -o src/transformContext.ts https://gist.githubusercontent.com/belopash/aa6b67dc374add44b9bdff1c9c1eee17/raw/441d43a932591624822b5bfd51a23147b5cecac2/transformContext.ts then transform the new context to the old format at the beginning of the batch handler: src/processor.ts import { transformContext } from './transformContext' // ... processor . run ( db , async ( newCtx : DataHandlerContext < Store , any > ) => { let ctx = transformContext ( newCtx ) // the rest of the batch handler should work unchanged } ) Alternatively, rewrite your batch handler using the new batch context interface . Consult the block data page for EVM-specific details on the new context format. Step 7 ​ Update your transactions/events decoding code. The big change here is that now decoders generated by @subsquid/evm-typegen return bigint where ethers.BigNumber was used before. Regenerate all TypeScript ABI wrappers as described in the EVM typegen section, then find all places where ethers.BigNumber s returned by old decoders were handled in your code and rewrite it to use bigint s. Step 8 ​ Iteratively reconcile any type errors arising when building your squid (e.g. with npm run build ). In case you're using tranformContext.ts you may find the types it exports helpful. If you need to specify the field selection generic argument explicitly, get it as a typeof of the setFields argument value: import { OldBlockData } from './transformContext' const fieldSelection = { log : { data : true } , transaction : { hash : true , } } as const let processor = new EvmBatchProcessor ( ) . setFields ( fieldSelection ) /* the rest of the processor configuration */ type MyBlockData = OldBlockData < typeof fieldSelection > // ... At this point your squid should be able to work with the ArrowSquid tooling. If it doesn't, read on. Troubleshooting ​ If these instructions did not work for you, please let us know at the SquidDevs Telegram chat . Edit this page Previous Migrate from The Graph Next ArrowSquid for Substrate Step 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step 8 Troubleshooting
Step-by-step tutorials | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Step-by-step tutorials 🗃️ Indexing BAYC 4 items 📄️ Index to local CSV files Storing data in files for analysis 📄️ Index to Parquet files Storing data in files for analysis 📄️ Use with Ganache or Hardhat Use SQD to index an Ethereum dev node 📄️ Simple Substrate squid Build a starter squid for Substrate 📄️ ink! contract indexing Build a squid indexing an ink! smart contract 📄️ Frontier EVM-indexing squid Build a squid indexing NFTs on Astar 📄️ Processor in action Batch processors in action 📄️ Case studies Deep dives into larger projects built with SQD Previous hasura-configuration tool v2 Next Indexing BAYC
Installation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI Installation On this page Installation Squid CLI is a command line tool for scaffolding new squids from templates running SDK tools and scripts defined in commands.json in a cross-platform way managing squid deployments in SQD Cloud (former Aquarium) The CLI is distributed as a npm package . To install Squid CLI, follow the steps below. 0. Install and setup Squid CLI ​ First, install the latest version of Squid CLI as a global npm package: npm i -g @subsquid/cli@latest Check the version: sqd --version Make sure the output looks like @subsquid/cli@<version> . info The next steps are optional for building and running squids. A key is required to enable the CLI commands managing the SQD Cloud deployments. 1. Obtain a SQD Cloud deployment key ​ Sign in to Cloud , and obtain (or refresh) the deployment key page by clicking at the profile picture > "Deployment key": 2. Authenticate Squid CLI ​ Open a terminal window and run sqd auth -k < DEPLOYMENT_KEY > 3. Explore with --help ​ Use sqd --help to get a list of the available command and sqd <command> --help to get help on the available options for a specific command, e.g. sqd deploy --help Edit this page Previous Squid CLI Next commands.json 0. Install and setup Squid CLI 1. Obtain a SQD Cloud deployment key 2. Authenticate Squid CLI 3. Explore with --help
Reference manual for the sqd utility | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI Reference manual for the sqd utility 📄️ Installation Setup Squid CLI 📄️ commands.json Dynamic sqd commands 📄️ sqd auth Log in to the Cloud 📄️ sqd autocomplete Display autocomplete installation instructions. 📄️ sqd deploy Deploy new or update an existing squid deployment in the Cloud. Squid name and also optionally slot and/or tag are taken from the provided deployment manifest. 📄️ sqd explorer sqd explorer is disabled in @subsquid/cli>=3.0.0. If you've been using it, please let us know in the SquidDevs Telegram channel. 📄️ sqd gateways Explore data sources for a squid 📄️ sqd init Setup a new squid project from a template or github repo 📄️ sqd list List squids deployed to the Cloud 📄️ sqd logs Fetch logs from a squid deployed to the Cloud 📄️ prod sqd prod and the production aliases system are deprecated starting from @subsquid/ [email protected] . Zero downtime updates are now handled by the slots and tags system. See also the changelog for a comparison between production aliases and the new system. 📄️ sqd remove Remove a squid deployed to the Cloud 📄️ sqd restart Restart a squid deployed to the Cloud 📄️ sqd run Run a squid locally according to the deployment manifest. 📄️ sqd secrets Manage account secrets 📄️ sqd tags Manage squid deployements' tags. See the slots and tags guide. 📄️ sqd whoami Show the user details for the current Cloud account Previous ApeWorx plugin Next Installation
Migrate to the Cloud portal | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Migrate to the Cloud portal On this page Migrate to the Cloud portal For users on Solana ​ info SQD Network portals are currently in beta. Please report any bugs or suggestions to the SQD Portal Beta chat or to Squid Devs . The newest version of SQD Network portal serves real time data. It can replace both gateways of the open private version of SQD Network and RPC endpoints. Currently it only supports Solana ( more on Solana indexing in SQD ). info We're currently experimenting with tightening the data request complexity limits. If you see an HTTP 400 error with a message like this: Couldn't parse query: query contains X item requests, but only 50 is allowed where X is some number above 50, or any other HTTP 400 response, please let us know. Here are the steps to migrate: Step 0: Procur a portal endpoint URL ​ You want https://portal.sqd.dev/datasets/solana-beta Dedicated Cloud Solana portal with real time support is TBA. Step 1: Install @portal-api packages ​ A. Enter your squid's folder. B. Remove both your lock file and the node_modules folder: NPM Yarn PNPM rm -r node_modules package-lock.json rm -r node_modules yarn.lock rm -r node_modules pnpm-lock.yaml C. Upgrade all SQD packages that have a @portal-api version to it: npx --yes npm-check-updates --filter "@subsquid/*" --target "@portal-api" --upgrade D. Freeze the versions of @portal-api packages by removing any version range specifiers ( ^ , ~ , < , > , >= , <= ) preceding the package versions. Here's a script: sed -i -e 's/[\^~=<>]*\([0-9\.]*-portal-api\.[0-9a-f]\{6\}\)/\1/g' package.json You can also do it manually Here's an example edit: "dependencies": { - "@subsquid/batch-processor": "^1.0.0-portal-api.18ef40", + "@subsquid/batch-processor": "1.0.0-portal-api.18ef40", "@subsquid/borsh": "^0.2.0", - "@subsquid/solana-objects": ">=0.0.3-portal-api.18ef40", - "@subsquid/solana-stream": "<1.0.0-portal-api.18ef40", + "@subsquid/solana-objects": "0.0.3-portal-api.18ef40", + "@subsquid/solana-stream": "1.0.0-portal-api.18ef40", "@subsquid/typeorm-migration": "^1.3.0", - "@subsquid/typeorm-store": "~1.6.0-portal-api.18ef40", + "@subsquid/typeorm-store": "1.6.0-portal-api.18ef40", "dotenv": "^16.4.7", "pg": "^8.13.1", "typeorm": "^0.3.20" }, "devDependencies": { - "@subsquid/solana-typegen": "^0.4.1-portal-api.18ef40", + "@subsquid/solana-typegen": "0.4.1-portal-api.18ef40", "@types/node": "^22.13.1", "typescript": "~5.7.3" } E. Install the dependencies: NPM Yarn PNPM npm install yarn install pnpm install Step 2: Update your code ​ A. Replace all exising data sources with the portal: + .setPortal('https://portal.tethys.sqd.dev/datasets/solana-beta') - .setGateway('https://v2.archive.subsquid.io/network/solana-mainnet') - .setRpc({ - client: new SolanaRpcClient({ - url: process.env.SOLANA_NODE - }) - }) Also, please remove any mentions of SolanaRpcClient , for example: - import {DataSourceBuilder, SolanaRpcClient} from '@subsquid/solana-stream' + import {DataSourceBuilder} from '@subsquid/solana-stream' B. Replace any block height literals with slot number literals. + .setBlockRange({from: 325000000}) - .setBlockRange({from: 303262650}) TBA add a convenient converter C. If you used the slot field of block headers anywhere in your code, replace it with .number : - slot: block.header.slot, + slot: block.header.number, D. If you need the block height (for example to stay compatible with your old code) request it in the .setFields call: .setFields({ block: { // block header fields timestamp: true, + height: true }, Your squid is ready to use. Step 3: testing ​ We highly recommend that all Portal Beta users test their migrated squids by re-syncing them. This will allow you to make sure that everything works as expected for the whole length of the chain and catch any bugs early. A workaround that allows continuous operation without a resync (not recommended) Assuming your squid is version-controlled and has one processor: Commit your updated squid code. Stop your Cloud squid deployment. Reset your repo to the state before the updates. Add the to field to the argument of the .setBlockRange DataSourceBuilder call (likely in ./src/main.ts ). Set it to the current slot number. Update your deployment Take a look at your squid's logs. It should be repeatedly terminating (due to having nothing to do) and restarting. Connect to the squid's database and update the height field of the status schema to contain slot instead of the block height of the block mentioned there. Verify that the height field updated successfully by re-reading the status schema. Reset your codebase to its updated version and redeploy your squid again. To resync your squid, follow the zero-downtime update procedure: Deploy your squid into a new slot. Wait for it to sync, observing the improved data fetching. Assign your production tag to the new deployment to redirect the GraphQL requests there. See this section for details. What's next? ​ Take a look at the Soldexer project for a peek at the new client architecture and the most detailed and up-to-date documentation on the updated Portal API. Edit this page For users on Solana Step 0: Procur a portal endpoint URL Step 1: Install @portal-api packages Step 2: Update your code Step 3: testing What's next?
Quickstart | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Quickstart On this page Quickstart This is a 5 min quickstart on how to build an indexer using Squid SDK. The indexer (squid) will: Fetch all historical USDC transfers on Ethereum from the SQD Network Decode it Save it to a local Postgres database Start a GraphQL server with a rich API to query the historical USDC transfers. Prerequisites ​ (On Windows) WSL Node.js v18+ Git Docker (for running Postgres) Step 1 ​ Install Squid CLI: npm i -g @subsquid/cli Squid CLI is a multi-purpose utility tool for scaffolding and managing the indexers, both locally and in SQD Cloud. Step 2 ​ Scaffold the indexer project (squid) from an example repo using Squid CLI: sqd init hello-squid -t https://github.com/subsquid-labs/showcase01-all-usdc-transfers cd hello-squid The example is just a public GitHub repo with a squid project pre-configured to index USDC. Step 3 ​ Inspect the ./src folder: src/ ├── abi │   └── usdc.ts ├── main.ts └── model ├── generated │   ├── index.ts │   ├── marshal.ts │   └── usdcTransfer.model.ts └── index.ts Here, src/abi/usdc is a utility module generated from the JSON ABI of the USDC contract. It contains methods for event decoding, direct RPC queries and some useful constants. src/model contains TypeORM model classes autogenerated from schema.graphql . Squids use them to populate Postgres. main.ts is the main executable. In this example, it also contains all the data retrieval configuration: const processor = new EvmBatchProcessor ( ) // SQD Network gateways are the primary source of blockchain data in // squids, providing pre-filtered data in chunks of roughly 1-10k blocks. // Set this for a fast sync. . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) // Another data source squid processors can use is chain RPC. // In this particular squid it is used to retrieve the very latest chain data // (including unfinalized blocks) in real time. It can also be used to //   - make direct RPC queries to get extra data during indexing //   - sync a squid without a gateway (slow) . setRpcEndpoint ( 'https://rpc.ankr.com/eth' ) // The processor needs to know how many newest blocks it should mark as "hot". // If it detects a blockchain fork, it will roll back any changes to the // database made due to orphaned blocks, then re-run the processing for the // main chain blocks. . setFinalityConfirmation ( 75 ) // .addXXX() methods request data items. In this case we're asking for // Transfer(address,address,uint256) event logs emitted by the USDC contract. // // We could have omitted the "address" filter to get Transfer events from // all contracts, or the "topic0" filter to get all events from the USDC // contract, or both to get all event logs chainwide. We also could have // requested some related data, such as the parent transaction or its traces. // // Other .addXXX() methods (.addTransaction(), .addTrace(), .addStateDiff() // on EVM) are similarly feature-rich. . addLog ( { range : { from : 6_082_465 } , address : [ USDC_CONTRACT_ADDRESS ] , topic0 : [ usdcAbi . events . Transfer . topic ] , } ) // .setFields() is for choosing data fields for the selected data items. // Here we're requesting hashes of parent transaction for all event logs. . setFields ( { log : { transactionHash : true , } , } ) The rest of the file is about data processing and storage: // TypeormDatabase objects store the data to Postgres. They are capable of // handling the rollbacks that occur due to blockchain forks. // // There are also Database classes for storing data to files and BigQuery // datasets. const db = new TypeormDatabase ( { supportHotBlocks : true } ) // The processor.run() call executes the data processing. Its second argument is // the handler function that is executed once on each batch of data. Processor // object provides the data via "ctx.blocks". However, the handler can contain // arbitrary TypeScript code, so it's OK to bring in extra data from IPFS, // direct RPC calls, external APIs etc. processor . run ( db , async ( ctx ) => { // Making the container to hold that which will become the rows of the // usdc_transfer database table while processing the batch. We'll insert them // all at once at the end, massively saving IO bandwidth. const transfers : UsdcTransfer [ ] = [ ] // The data retrieved from the SQD Network gatewat and/or the RPC endpoint // is supplied via ctx.blocks for ( let block of ctx . blocks ) { // On EVM, each block has four iterables - logs, transactions, traces, // stateDiffs for ( let log of block . logs ) { if ( log . address === USDC_CONTRACT_ADDRESS && log . topics [ 0 ] === usdcAbi . events . Transfer . topic ) { // SQD's very own EVM codec at work - about 20 times faster than ethers let { from , to , value } = usdcAbi . events . Transfer . decode ( log ) transfers . push ( new UsdcTransfer ( { id : log . id , block : block . header . height , from , to , value , txnHash : log . transactionHash } ) ) } } } // Just one insert per batch! await ctx . store . insert ( transfers ) } ) Step 4 ​ Install the dependencies and build npm i npm run build Step 5 ​ The processor is a background process that continously fetches the data, decodes it and stores it in a local Postgres. All the logic is defined in main.ts and is fully customizable. To run the processor, we first start a local Postgres where the decoded data is persisted (the template comes with a Docker compose file): docker compose up -d Processor will connect to Postgres using the connection parameters from .env . Apply database migrations with npx squid-typeorm-migration apply then start the processor with node -r dotenv/config lib/main.js The indexer is now running. Step 6 ​ Start the GraphQL API serving the transfers data from Postgres: npx squid-graphql-server The server comes with a GraphQL playground available at localhost:4350/graphql . Step 7 ​ Query the data! Edit this page Previous Indexing SDK Next Overview Prerequisites Step 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7
Field selection | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource Field selection On this page Field selection setFields(options) ​ Set the fields to be retrieved for data items of each supported type. The options object has the following structure: { block ? : // field selector for block headers transaction ? : // field selector for transactions receipt ? : // field selector for receipts input ? : // field selector for inputs output ? : // field selector for output } Every field selector is a collection of boolean fields that map to the fields of data items within the batch context iterables . Defining a field of a field selector of a given type and setting it to true will cause the processor to populate the corresponding field of all data items of that type. Here is a definition of a processor that requests hash and status fields for transactions and the contract field for receipts: const dataSource = new DataSourceBuilder ( ) . setFields ( { transaction : { hash : true , status : true , } , receipt : { contract : true , } , } ) ; Same fields will be available for all data items of any given type, including the items accessed via nested references. Suppose we used the processor defined above to subscribe to some transactions as well as some receipts, and for each receipt we requested its parent transaction: dataSource . addTransaction ( { // some transaction data requests } ) . addReceipt ( { // some receipt data requests transaction : true } ) . build ( ) ; After populating the convenience reference fields with augmentBlock() from @subsquid/fuel-objects , the contract field will be available both within the data items of the transactions iterable of block data and within the transaction items that provide parent transaction information for the receipts matching the addReceipt() data request: run ( dataSource , database , async ( ctx ) => { let blocks = ctx . blocks . map ( augmentBlock ) for ( let block of blocks ) { for ( let txn of block . transactions ) { let contract = txn . contract ; // OK } for ( let rec of block . receipts ) { if ( /* rec matches the data request */ ) { let recContract = rec . transaction . contract ; // also OK! } } } } ) Some data fields, like hash for transactions, are enabled by default but can be disabled by setting a field of a field selector to false . For example, this code will not compile: const dataSource = new DataSourceBuilder ( ) . setFields ( { transaction : { hash : false , } , } ) . build ( ) ; run ( dataSource , database , async ( ctx ) => { for ( let block of ctx . blocks ) { for ( let txn of block . transactions ) { let hash = txn . hash ; // ERROR: no such field } } } ) ; Disabling unused fields will improve sync performance, as the fields' data will not be fetched from the SQD Network gateway. Data item types and field selectors ​ tip Most IDEs support smart suggestions to show the possible field selectors. For VS Code, press Ctrl+Space . Here we describe the data item types as functions of the field selectors. Unless otherwise mentioned, each data item type field maps to the eponymous field of its corresponding field selector. Item fields are divided into three categories: Fields that are always added regardless of the setFields() call. Fields that are enabled by default and can be disabled by setFields() . E.g. a hash field will be fetched for transactions by default, but can be disabled by setting hash: false within the transaction field selector. Fields that can be requested by setFields() . Transaction ​ Fields of Transaction data items may be requested by the eponymous fields of the field selector. Composite fields like inputContract are requested in their entirety by a single selector. Here's a detailed description of possible Transaction fields: Transaction { // independent of field selectors index : number // can be disabled with field selectors hash : string type : TransactionType status : Status // can be enabled with field selectors inputAssetIds ? : string [ ] inputContracts ? : string [ ] inputContract ? : { utxoId : string balanceRoot : string stateRoot : string txPointer : string contractId : string } policies ? : Policies scriptGasLimit ? : bigint maturity ? : number mintAmount ? : bigint mintAssetId ? : string mintGasPrice ? : bigint txPointer ? : string isScript : boolean isCreate : boolean isMint : boolean isUpgrade : boolean isUpload : boolean outputContract ? : { inputIndex : number balanceRoot : string stateRoot : string } witnesses ? : string [ ] receiptsRoot ? : string script ? : string scriptData ? : string salt ? : string storageSlots ? : string [ ] rawPayload ? : string bytecodeWitnessIndex ? : number bytecodeRoot ? : string subsectionIndex ? : number subsectionsNumber ? : number proofSet ? : string [ ] upgradePurpose ? : UpgradePurpose } transactionType field has the following type: type TransactionType = "Script" | "Create" | "Mint" | "Upgrade" | "Upload" status field has the following type: type Status = SubmittedStatus | SuccessStatus | SqueezedOutStatus | FailureStatus Status can be one of the following: interface SubmittedStatus { type : "SubmittedStatus" time : bigint } interface SuccessStatus { type : "SuccessStatus" transactionId : string time : bigint programState ? : ProgramState } interface SqueezedOutStatus { type : "SqueezedOutStatus" reason : string } interface FailureStatus { type : "FailureStatus" transactionId : string time : bigint reason : string programState ? : ProgramState totalGas : bigint totalFee : bigint } ProgramState is defined as follows: interface ProgramState { returnType : "RETURN" | "RETURN_DATA" | "REVERT" ; data : string ; } policies field has the following interface: interface Policies { gasPrice ? : bigint witnessLimit ? : bigint maturity ? : number maxFee ? : bigint } upgradePurpose field is of type ConsensusParametersPurpose | StateTransitionPurpose , where the types are defined as: interface ConsensusParametersPurpose { type : 'ConsensusParametersPurpose' witnessIndex : number checksum : string } interface StateTransitionPurpose { type : 'StateTransitionPurpose' root : string } Receipt ​ Fields of Receipt data items may be requested by the eponymous fields of the field selector. Here's a detailed description of possible Receipt fields: Receipt { // independent of field selectors index : number transactionIndex : number // can be disabled with field selectors receiptType : ReceiptType // can be enabled with field selectors contract ? : string pc ? : bigint is ? : bigint to ? : string toAddress ? : string amount ? : bigint assetId ? : string gas ? : bigint param1 ? : bigint param2 ? : bigint val ? : bigint ptr ? : bigint digest ? : string reason ? : bigint ra ? : bigint rb ? : bigint rc ? : bigint rd ? : bigint len ? : bigint result ? : bigint gasUsed ? : bigint data ? : string sender ? : string recipient ? : string nonce ? : string contractId ? : string subId ? : string } receiptType has the following type: type ReceiptType = | "CALL" | "RETURN" | "RETURN_DATA" | "PANIC" | "REVERT" | "LOG" | "LOG_DATA" | "TRANSFER" | "TRANSFER_OUT" | "SCRIPT_RESULT" | "MESSAGE_OUT" | "MINT" | "BURN" ; Input ​ Input data items can be of types InputCoin , InputContract or InputMessage . Each type has its own set of fields. To access the type-specific fields, narrow down the type by asserting the value of the type field, e.g. if ( input . type === "InputCoin" ) { // use InputCoin-specific fields here } To get a name of a field selector field, apply a type prefix to a capitalized name of the data item field, e.g. amount field of InputCoin input items is requested by coinAmount: true ; witnessIndex field of InputMessage input items is requested by messageWitnessIndex: true etc. All Input* data types have transactionIndex , index and type fields. These cannot be disabled. There aren't any fields that are enabled by default and can be disabled. InputCoin data items may have the following fields: interface InputCoin { type : "InputCoin" ; index : number ; transactionIndex : number ; utxoId : string ; owner : string ; amount : bigint ; assetId : string ; txPointer : string ; witnessIndex : number ; predicateGasUsed : bigint ; predicate : string ; predicateData : string ; } InputContract data items may have the following fields: interface InputContract { type : "InputContract" ; index : number ; transactionIndex : number ; utxoId : string ; balanceRoot : string ; stateRoot : string ; txPointer : string ; contractId : string ; } InputMessage data items may have the following fields: InputMessage { type : "InputMessage" ; index : number ; transactionIndex : number ; sender : string ; recipient : string ; amount : bigint ; nonce : string ; witnessIndex : number ; predicateGasUsed : bigint ; data : string ; predicate : string ; predicateData : string ; } It is possible to request more than one type of input in the same data request. For example, to request all inputs of type InputCoin and InputContract : dataSource . addInput ( { type : [ "InputCoin" , "InputContract" ] , } ) . build ( ) ; Output ​ Output data items can be of types CoinOutput , ContractOutput , ChangeOutput , VariableOutput or 'ContractCreated' . Each type has its own set of fields. To access the type-specific fields, narrow down the type by asserting the value of the type field, e.g. if ( output . type === "CoinOutput" ) { // use CoinOutput-specific fields here } To get a name of a field selector field, apply a type prefix to a capitalized name of the data item field, e.g. to field of CoinOutput items is requested by coinTo: true ; inputIndex field of ContractOutput items is requested by contractInputIndex: true ; amount field of ChangeOutput items is requested by changeAmount: true ; assetId field of VariableOutput items is requested by variableAssetId: true ; stateRoot field of ContractCreated items is requested by contractCreatedStateRoot: true etc. All output data types have transactionIndex , index and type fields. These cannot be disabled. There aren't any fields that are enabled by default and can be disabled. CoinOutput data items may have the following fields: CoinOutput { type : 'CoinOutput' index : number transactionIndex : number to : string amount : bigint assetId : string } ContractOutput data items may have the following fields: ContractOutput { type : "ContractOutput" ; index : number ; transactionIndex : number ; inputIndex : number ; balanceRoot : string ; stateRoot : string ; } ChangeOutput data items may have the following fields: ChangeOutput { type : 'ChangeOutput' index : number transactionIndex : number to : string amount : bigint assetId : string } VariableOutput data items may have the following fields: VariableOutput { type : 'VariableOutput' index : number transactionIndex : number to : string amount : bigint assetId : string } ContractCreated data items may have the following fields: ContractCreated { type : 'ContractCreated' index : number transactionIndex : number contract : string stateRoot : string } Block header ​ BlockHeader data items may have the following fields: BlockHeader { // independent of field selectors hash : string height : number // can be disabled with field selectors time : bigint // can be enabled with field selectors daHeight : bigint transactionsRoot : string transactionsCount : number messageReceiptCount : number prevRoot : string applicationHash : string eventInboxRoot : string consensusParametersVersion : number stateTransitionBytecodeVersion : number messageOutboxRoot : string } Request the fields with eponymous field request flags. A complete example ​ import { run } from "@subsquid/batch-processor" ; import { augmentBlock } from "@subsquid/fuel-objects" ; import { DataSourceBuilder } from "@subsquid/fuel-stream" ; import { TypeormDatabase } from "@subsquid/typeorm-store" ; import { Contract } from "./model" ; const dataSource = new DataSourceBuilder ( ) . setGateway ( "https://v2.archive.subsquid.io/network/fuel-mainnet" ) . setGraphql ( { url : "https://mainnet.fuel.network/v1/graphql" , } ) . setFields ( { transaction : { hash : false , isMint : true , } , receipt : { amount : true , gas : true , } , input : { coinOwner : true , contractStateRoot : true , } , output : { coinTo : true , contractCreatedStateRoot : true , } , block : { daHeight : true } } ) . addTransaction ( { type : [ 'Mint' , 'Script' ] , inputs : true , range : { from : 1_000_000 } } ) . addReceipt ( { type : [ 'LOG_DATA' ] } ) . addInput ( { type : [ 'InputCoin' , 'InputContract' ] } ) . addOutput ( { type : [ 'CoinOutput' , 'ContractCreated' ] , transaction : true } ) . build ( ) Edit this page Previous Transactions Next Cheatsheet Data item types and field selectors Transaction Receipt Input Output Block header A complete example
Transactions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Block data for Tron General settings Transactions Logs Internal transactions Field selection Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing TronBatchProcessor Transactions On this page Transactions Tron Network has a diverse and dynamic set of transaction types. TronBatchProcessor has methods for subscribing to some common transaction types, all with fine-grained filters: addTransferTransaction() addTransferAssetTransaction() addTriggerSmartContractTransaction() For other transaction types you can use the general method addTransaction() that does not offer much filtering. If your application would benefit from fine-grained request methods for other transaction types, please let us know in the SquidDevs Telegram channel . addTransferTransaction(options) ​ Get some or all TransferContract transactions on the network. options has the following structure: { where ? : { owner ? : string [ ] to ? : string [ ] } include ? : { logs ? : boolean internalTransactions ? : boolean } range ? : { from : number to ? : number } } Data requests are located in the where field: owner : the set of addresses of owners of the token being transferred. to : the set of addresses of tokens' receivers. Omit the where field to subscribe to all TransferContract txs network-wide. Related data can be requested via the include field: logs = true : will retrieve event logs emitted by the selected transactions. internalTransactions = true : will retrieve internal txs that occurred due to the selected transactions. The data will be added to the appropriate iterables within block data and made available via the .logs and .internalTransactions fields of each transaction item. Note that transactions can also be requested by the other TronBatchProcessor methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Example ​ This requests TransferContract txs where owner is 41da9964294c8689bfee2778606e485221625496bf , plus their logs, on block 64000675. processor . addTransferTransaction ( { where : { owner : [ '41da9964294c8689bfee2778606e485221625496bf' ] } , include : { logs : true } , range : { from : 64000675 , to : 64000675 } } ) addTransferAssetTransaction(options) ​ Get some or all TransferAssetContract transactions on the network. options has the following structure: { where ? : { asset ? : string [ ] owner ? : string [ ] to ? : string [ ] } include ? : { logs ? : boolean internalTransactions ? : boolean } range ? : { from : number to ? : number } } Data requests are located in the where field: asset : the set of token names / ids. owner : the set of addresses of owners of the token being transferred. to : the set of addresses of tokens' receivers. Omit the where field to subscribe to all TransferAssetContract txs network-wide. Related data can be requested via the include field: logs = true : will retrieve event logs emitted by the selected transactions. internalTransactions = true : will retrieve internal txs that occurred due to the selected transactions. The data will be added to the appropriate iterables within block data and made available via the .logs and .internalTransactions fields of each transaction item. Note that transactions can also be requested by the other TronBatchProcessor methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Example ​ This requests TransferAssetContract txs where owner is 4170d66a855afef04753f06c4d6210d6b77708cdb0 , plus their internal txs, on block 64000000. processor . addTransferTransaction ( { where : { owner : [ '4170d66a855afef04753f06c4d6210d6b77708cdb0' ] } , include : { internalTransactions : true } , range : { from : 64000000 , to : 64000000 } } ) addTriggerSmartContractTransaction(options) ​ Get some or all TriggerSmartContract transactions on the network. options has the following structure: { where ? : { contract ? : string [ ] sighash ? : string [ ] owner ? : string [ ] } include ? : { logs ? : boolean internalTransactions ? : boolean } range ? : { from : number to ? : number } } Data requests are located in the where field: contract : the set of addresses of contracts being called. sighash : the set of 4-byte signature hashes to filter the calls by. owner : the set of addresses of call owners. Omit the where field to subscribe to all TriggerSmartContract txs network-wide. Related data can be requested via the include field: logs = true : will retrieve event logs emitted by the selected transactions. internalTransactions = true : will retrieve internal txs that occurred due to the selected transactions. The data will be added to the appropriate iterables within block data and made available via the .logs and .internalTransactions fields of each transaction item. Note that transactions can also be requested by the other TronBatchProcessor methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Example ​ This requests TriggerSmartContract txs made directly to the USDT contract ( 41a614f803b6fd780986a42c78ec9c7f77e6ded13c ). processor . addTriggerSmartContractTransaction ( { where : { contract : [ '41a614f803b6fd780986a42c78ec9c7f77e6ded13c' ] } } ) addTransaction(options) ​ Get some or all transactions on the network. options has the following structure: { where ? : { type ? : string [ ] } include ? : { logs ? : boolean internalTransactions ? : boolean } range ? : { from : number to ? : number } } Data requests are located in the where field: type sets the allowed transaction types. Leave it undefined to subscribe to transactions of all types. Omit the where field to subscribe to all txs network-wide. Related data can be requested via the include field: logs = true : will retrieve event logs emitted by the selected transactions. internalTransactions = true : will retrieve internal txs that occurred due to the selected transactions. The data will be added to the appropriate iterables within block data and made available via the .logs and .internalTransactions fields of each transaction item. Note that transactions can also be requested by the other TronBatchProcessor methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Example ​ This requests all FreezeBalanceV2Contract txs network-wide, along with all the logs they emitted: processor . addTransaction ( { where : { type : [ 'FreezeBalanceV2Contract' ] } , include : { logs : true } } ) Edit this page Previous General settings Next Logs addTransferTransaction(options) addTransferAssetTransaction(options) addTriggerSmartContractTransaction(options) addTransaction(options)
Indexing Orca Whirlpool | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start Indexing Orca Whirlpool Cheatsheet SDK Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing How to start Indexing Orca Whirlpool On this page Indexing the Orca DEX In this step-by-step tutorial we will look into a squid that gets data about the Orca Exchange NFTs, their transfers and owners from the Solana blockchain . Pre-requisites: Node.js v20 or newer , Git, Docker. Download the project ​ Begin by retrieving the template and installing the dependencies: git clone https://github.com/subsquid-labs/solana-example cd solana-example npm i Interfacing with the Whirlpool program ​ First, we inspect the data available for indexing. In Solana, most programs use the Anchor framework . Anchor makes the metadata describing the shape of the instructions, transactions and contract variables available as an Interface Definition Language (IDL) JSON file. For many popular programs (including Whirlpool) IDL files are published on-chain. SQD provides a tool for retrieving program IDLs and generating boilerplate ABI code for data decoding. This can be done with npx squid-solana-typegen src/abi whirLbMiicVdio4qvUfM5KAg6Ct8VwpYzGff3uctyCc #whirlpool Here, src/abi is the destination folder and the whirlpool suffix sets the base name for the generated file. Checking out the generated src/abi/whirlpool/instructions.ts file. It exports an instruction instance for every instruction in the ABI. Here's how it is initialized for swap : export const swap = instruction ( { d8 : '0xf8c69e91e17587c8' , } , { tokenProgram : 0 , tokenAuthority : 1 , whirlpool : 2 , tokenOwnerAccountA : 3 , tokenVaultA : 4 , tokenOwnerAccountB : 5 , tokenVaultB : 6 , tickArray0 : 7 , tickArray1 : 8 , tickArray2 : 9 , oracle : 10 , } , struct ( { amount : u64 , otherAmountThreshold : u64 , sqrtPriceLimit : u128 , amountSpecifiedIsInput : bool , aToB : bool , } ) , ) Here, d8 are the eight bytes that the relevant instruction data starts with. Configuring the data source ​ "Data source" is a component that defines what data should be retrieved and where to get it. To configure the data source to retrieve the data produced by the swap instruction of the Whirlpool program, we initialize it like this: src/main.ts // ... import { run } from '@subsquid/batch-processor' import { augmentBlock } from '@subsquid/solana-objects' import { DataSourceBuilder , SolanaRpcClient } from '@subsquid/solana-stream' import { TypeormDatabase } from '@subsquid/typeorm-store' import assert from 'assert' import * as tokenProgram from './abi/token-program' import * as whirlpool from './abi/whirpool' import { Exchange } from './model' const dataSource = new DataSourceBuilder ( ) // Provide SQD Network Gateway URL. . setGateway ( 'https://v2.archive.subsquid.io/network/solana-mainnet' ) . setRpc ( process . env . SOLANA_NODE == null ? undefined : { client : new SolanaRpcClient ( { url : process . env . SOLANA_NODE , // rateLimit: 100 // requests per sec } ) , strideConcurrency : 10 // max concurrent RPC connections } ) . setBlockRange ( { from : 259_984_950 } ) . setFields ( { block : { timestamp : true } , transaction : { signatures : true } , instruction : { programId : true , accounts : true , data : true } , tokenBalance : { preAmount : true , postAmount : true , preOwner : true , postOwner : true } } ) . addInstruction ( { // select instructions that: where : { // were executed by the Whirlpool program, and programId : [ whirlpool . programId ] , // have the first eight bytes of .data equal to swap descriptor, and d8 : [ whirlpool . instructions . swap . d8 ] , // limiting to USDC-SOL pair only ... whirlpool . instructions . swap . accountSelection ( { whirlpool : [ '7qbRF6YsyGuLUVs6Y1q64bdVrfe4ZcUUz1JRdoVNUJnm' ] } ) , // were successfully committed isCommitted : true } , // for each instruction data item selected above // make sure to also include: include : { // inner instructions innerInstructions : true , // transaction that executed the instruction transaction : true , // all token balance records of that transaction transactionTokenBalances : true , } } ) . build ( ) Here, 'https://v2.archive.subsquid.io/network/solana-mainnet' is the address for the public SQD Network gateway for Solana mainnet. The only other Solana-compatible dataset currently available is Eclipse Testnet, with the gateway at 'https://v2.archive.subsquid.io/network/eclipse-testnet' . Many other networks are available on EVM and Substrate - see the exhaustive public networks list . 'process.env.SOLANA_NODE' is an environment variable pointing at a public RPC endpoint we chose to use in this example. When an endpoint is available, the processor will begin ingesting data from it once it reaches the highest block available within SQD Network. 259_984_950 is first Solana block currently indexed by SQD. The argument of addInstruction() is a set of filters that tells the processor to retrieve all data on the swap instruction of the Whirlpool program with discriminator matching the hash of the <namespace>:<instruction> of the swap instruction. The argument of setFields() specifies the exact fields we need for every data item type. See SolanaDataSource reference for more options. With a data source it becomes possible to retrieve filtered blockchain data from SQD Network , transform it and save the result to a destination of choice. Decoding the event data ​ The other part the squid processor (the ingester process of the indexer) is the callback function used to process batches of the filtered data, the batch handler . In Solana Squid SDK it is typically defined within a run() call, like this: import { run } from '@subsquid/batch-processor' run ( dataSource , database , async ctx => { // data transformation and persistence code here } ) Here, dataSource is the data source object described in the previous section database is a Database implementation specific to the target data sink. We want to store the data in a PostgreSQL database and present with a GraphQL API, so we provide a TypeormDatabase object here. ctx is a batch context object that exposes a batch of data (at ctx.blocks ) and any data persistence facilities derived from db (at ctx.store ). See Block data for Solana for details on how the data batches are presented. Batch handler is where the raw on-chain data is decoded, transformed and persisted. This is the part we'll be concerned with for the rest of the tutorial. We begin by defining a database and starting the data processing: src/main.ts const database = new TypeormDatabase ( ) // Now we are ready to start data processing run ( dataSource , database , async ctx => { // Block items that we get from `ctx.blocks` are flat JS objects. // // We can use `augmentBlock()` function from `@subsquid/solana-objects` // to enrich block items with references to related objects and // with convenient getters for derived data (e.g. `Instruction.d8`). let blocks = ctx . blocks . map ( augmentBlock ) let exchanges : Exchange [ ] = [ ] for ( let block of blocks ) { for ( let ins of block . instructions ) { // https://read.cryptodatabytes.com/p/starter-guide-to-solana-data-analysis if ( ins . programId === whirlpool . programId && ins . d8 === whirlpool . instructions . swap . d8 ) { let exchange = new Exchange ( { id : ins . id , slot : block . header . slot , tx : ins . getTransaction ( ) . signatures [ 0 ] , timestamp : new Date ( block . header . timestamp * 1000 ) } ) assert ( ins . inner . length == 2 ) let srcTransfer = tokenProgram . instructions . transfer . decode ( ins . inner [ 0 ] ) let destTransfer = tokenProgram . instructions . transfer . decode ( ins . inner [ 1 ] ) let srcBalance = ins . getTransaction ( ) . tokenBalances . find ( tb => tb . account == srcTransfer . accounts . source ) let destBalance = ins . getTransaction ( ) . tokenBalances . find ( tb => tb . account === destTransfer . accounts . destination ) let srcMint = ins . getTransaction ( ) . tokenBalances . find ( tb => tb . account === srcTransfer . accounts . destination ) ?. preMint let destMint = ins . getTransaction ( ) . tokenBalances . find ( tb => tb . account === destTransfer . accounts . source ) ?. preMint assert ( srcMint ) assert ( destMint ) exchange . fromToken = srcMint exchange . fromOwner = srcBalance ?. preOwner || srcTransfer . accounts . source exchange . fromAmount = srcTransfer . data . amount exchange . toToken = destMint exchange . toOwner = destBalance ?. postOwner || destBalance ?. preOwner || destTransfer . accounts . destination exchange . toAmount = destTransfer . data . amount exchanges . push ( exchange ) } } } await ctx . store . insert ( exchanges ) } ) This goes through all the instructions in the block, verifies that they indeed are swap instruction from the Whirlpool program and decodes the data of each inner instruction.
Then it retrieves the transaction from the decoded inner instruction and source and destination accounts.
The decoding is done with the tokenProgram.instructions.transfer.decode function from the Typescript ABI provided in the project. At this point the squid is ready for its first test run. Execute npx tsc docker compose up -d npx squid-typeorm-migration apply node -r dotenv/config lib/main.js You can verify that the data is being stored in the database by running docker exec "$(basename " $( pwd ) " ) -db-1 " psql -U postgres -c " SELECT * FROM exchange" The full code can be found here . Edit this page Previous How to start Next Cheatsheet Download the project Interfacing with the Whirlpool program Configuring the data source Decoding the event data
Configuring and extending the server | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Caching Custom API extensions Subscriptions DoS protection Access control Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Configuration Configuring and extending the server üìÑÔ∏è Caching Enable caching for faster queries üìÑÔ∏è Custom API extensions Extend the API with custom resolvers üìÑÔ∏è Subscriptions Subscribe to updates over a websocket üìÑÔ∏è DoS protection Enforce limits in the queries üìÑÔ∏è Access control Authentication and authorization Previous Overview Next Caching
Block data for Solana | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Block data for Solana Block data for Solana In Solana Squid SDK, the data is processed by repeatedly calling the user-defined batch handler function on batches of on-chain data. The sole argument of the batch handler is its context ctx , and ctx.blocks is an array of Block objects containing the data to be processed, aligned at the block level. For SolanaBatchProcessor the Block interface is defined as follows: export interface Block < F extends FieldSelection = { } > { header : BlockHeader < F > ; instructions : Instruction < F > [ ] ; transactions : Transaction < F > [ ] ; logs : LogMessage < F > [ ] ; balances : Balance < F > [ ] ; tokenBalances : TokenBalance < F > [ ] ; rewards : Reward < F > [ ] ; } F here is the type of the argument of the setFields() processor method. BlockData.header contains the block header data. The rest of the fields are iterables containing the six kinds of blockchain data. Canonical ordering within each iterable depends on the data kind: transactions are ordered in the same way as they are within blocks; instructions follow the order of transactions that gave rise to them; tokenBalances are ordered in a deterministic but otherwise unspecified way. The exact fields available in each data item type are inferred from the setFields() call argument. They are documented on the field selection page: Instruction section ; Transaction section ; LogMessage section ; Balance section ; TokenBalance section ; Reward section ; BlockHeader section . Edit this page Previous SolanaDataSource Next General settings
SQD Network API documentation for Tron | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Cheatsheet Network API Fuel Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing Network API SQD Network API documentation for Tron üìÑÔ∏è Fuel Network API Access the data of Fuel Network Previous Cheatsheet Next Fuel Network API
Tron API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Cheatsheet Network API Tron API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing Network API Tron API On this page Tron SQD Network API warning The Tron API of SQD Network is currently in beta. Breaking changes may be introduced in the future releases. Open private SQD Network offers access to Tron data. The gateway is at https://v2.archive.subsquid.io/network/tron-mainnet SQD Network API distributes the requests over a ( potentially decentralized ) network of workers . The main gateway URL points at a router that provides URLs of workers that do the heavy lifting. Each worker has its own range of blocks. Suppose you want to retrieve an output of some query on a block range starting at firstBlock (can be the genesis block) and ending at the highest available block. Proceed as follows: Retrieve the dataset height from the router with GET /height and make sure it's above firstBlock . Save the value of firstBlock to some variable, say currentBlock . Query the router for an URL of a worker that has the data for currentBlock with GET /${currentBlock}/worker . Retrieve the data from the worker by posting the query ( POST / ), setting the "fromBlock" query field to ${currentBlock} . Parse the retrieved data to get a batch of query data plus the height of the last block available from the current worker. Take the header.number field of the last element of the retrieved JSON array - it is the height you want. Even if your query returns no data, you'll still get the block data for the last block in the range, so this procedure is safe. Set currentBlock to the height from the previous step plus one . Repeat steps 3-6 until all the required data is retrieved. Implementation examples: Manually with cURL Suppose we want data on token burns from block 58_000_000. We have to: Verify that the dataset has reached the required height: curl https://v2.archive.subsquid.io/network/tron-mainnet/height Output 61889299 Remember that your current height is 58000000. Get a worker URL for the current height curl https://v2.archive.subsquid.io/network/tron-mainnet/58000000/worker Output https://rb03.sqd-archive.net/worker/query/czM6Ly90cm9uLW1haW5uZXQ Retrieve the data from the worker curl https://rb03.sqd-archive.net/worker/query/czM6Ly90cm9uLW1haW5uZXQ \ -X 'POST' -H 'content-type: application/json' -H 'accept: application/json' \ -d '{ "fromBlock":58000000, "toBlock":59286799, "fields":{"transaction":{"hash":true}}, "transactions":[{ "to":[ "0x0000000000000000000000000000000000000000" ] }] }' | jq Output: [ { "header" : { "number" : 16000000 , "hash" : "0x3dc4ef568ae2635db1419c5fec55c4a9322c05302ae527cd40bff380c1d465dd" , "parentHash" : "0x6f377dc6bd1f3e38b9ceb8c946a88c13211fa3f084622df3ee5cfcd98cc6bb16" } , "transactions" : [ ] } , // ... { "header" : { "number" : 16027977 , "hash" : "0x4b332878deb33e963b68c8bbbea60cbca72a88c297b6800eafa82baab497c166" , "parentHash" : "0x2b979d67d9b03394da336938ee0bcf5aedfdf87e1b5bd574d985aee749eb8b76" } , "transactions" : [ { "transactionIndex" : 96 , "hash" : "0xbaede248ec6fce28e9d874f69ea70359bea0107ce9144d6838898674d9d10c8c" } ] } , // ... { "header" : { "number" : 16031419 , "hash" : "0x9cc48c9b4ad8dddb1de86a15e30a62ffd48cf9b72930930cfa5167c4e1685d0a" , "parentHash" : "0x4ec7b4562739032f51e70d26fe5129e571e2bf0348a744c1509f8205f4381696" } , "transactions" : [ ] } ] Observe that we received the transactions up to and including block 16031419. To get the rest of the data, we find a worker who has blocks from 16031420 on: curl https://v2.archive.subsquid.io/network/ethereum-mainnet/16031420/worker Output: https://rb02.sqd-archive.net/worker/query/czM6Ly9ldGhlcmV1bS1tYWlubmV0 We can see that this part of the dataset is located on another host. Retrieve the data from the new worker curl https://rb02.sqd-archive.net/worker/query/czM6Ly9ldGhlcmV1bS1tYWlubmV0 \ -X 'POST' -H 'content-type: application/json' -H 'accept: application/json' \ -d '{ "fromBlock":16031420, "toBlock":18593441, "fields":{"transaction":{"hash":true}}, "transactions":[{"to":["0xd8da6bf26964af9d7eed9e03e53415d37aa96045"]}] }' | jq Output is similar to that of step 3. Repeat steps 4 and 5 until the dataset height of 18593441 reached. In Python def get_text ( url : str ) - > str : res = requests . get ( url ) res . raise_for_status ( ) return res . text def dump ( dataset_url : str , query : Query , first_block : int , last_block : int ) - > None : assert 0 <= first_block <= last_block query = dict ( query ) # copy query to mess with it later dataset_height = int ( get_text ( f' { dataset_url } /height' ) ) next_block = first_block last_block = min ( last_block , dataset_height ) while next_block <= last_block : worker_url = get_text ( f' { dataset_url } / { next_block } /worker' ) query [ 'fromBlock' ] = next_block query [ 'toBlock' ] = last_block res = requests . post ( worker_url , json = query ) res . raise_for_status ( ) blocks = res . json ( ) last_processed_block = blocks [ - 1 ] [ 'header' ] [ 'number' ] next_block = last_processed_block + 1 for block in blocks : print ( json . dumps ( block ) ) Full code here . Router API ​ GET /height (get height of the dataset) Example response: 65900614 . GET ${firstBlock}/worker (get a suitable worker URL) The returned worker will be capable of processing POST / requests in which the "fromBlock" field is equal to ${firstBlock} . Example response: https://rb06.sqd-archive.net/worker/query/czM6Ly90cm9uLW1haW5uZXQ . Worker API ​ POST / (query logs and transactions) Query Fields ​ fromBlock : Block number to start from (inclusive). toBlock : (optional) Block number to end on (inclusive). If this is not given, the query will go on for a fixed amount of time or until it reaches the height of the dataset. includeAllBlocks : (optional) If true, the Network will include blocks that contain no data selected by data requests into its response. fields : (optional) A selector of data fields to retrieve. Common for all data items. logs : (optional) A list of log requests . An empty list requests no data. transactions : (optional) A list of general transaction requests . An empty list requests no data. transferTransactions : (optional) A list of "transfer" transaction requests . An empty list requests no data. transferAssetTransactions : (optional) A list of "transfer asset" transaction requests . An empty list requests no data. triggerSmartContractTransactions : (optional) A list of "trigger smart contract" transaction requests . An empty list requests no data. internalTransactions : (optional) A list of internal transaction requests . An empty list requests no data. Example Request ​ { "logs" : [ { "address" : [ "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48" ] , "topic0" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" ] , "transaction" : true } ] , "fields" : { "block" : { "gasUsed" : true } , "log" : { "topics" : true , "data" : true } } , "fromBlock" : 16000000 , "toBlock" : 16000000 } Example Response ​ [ { "header" : { "number" : 16000000 , "hash" : "0x3dc4ef568ae2635db1419c5fec55c4a9322c05302ae527cd40bff380c1d465dd" , "parentHash" : "0x6f377dc6bd1f3e38b9ceb8c946a88c13211fa3f084622df3ee5cfcd98cc6bb16" , "gasUsed" : "0x121cdff" } , "transactions" : [ { "transactionIndex" : 0 } , { "transactionIndex" : 124 } , { "transactionIndex" : 131 } , { "transactionIndex" : 140 } , { "transactionIndex" : 188 } , { "transactionIndex" : 205 } ] , "logs" : [ { "logIndex" : 0 , "transactionIndex" : 0 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x000000000000000000000000ffec0067f5a79cff07527f63d83dd5462ccf8ba4" , "0x000000000000000000000000e47872c80e3af63bd237b82c065e441fa75c4dea" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000007270e00" } , { "logIndex" : 30 , "transactionIndex" : 124 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x000000000000000000000000f42ed7184f3bdd07b0456952f67695683afd9044" , "0x0000000000000000000000009bbcfc016adcc21d8f86b30cda5e9f100ff9f108" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000032430d8b" } , { "logIndex" : 34 , "transactionIndex" : 131 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x0000000000000000000000001d76271fb3d5a61184ba00052caa636e666d11ec" , "0x00000000000000000000000074de5d4fcbf63e00296fd95d33236b9794016631" ] , "data" : "0x000000000000000000000000000000000000000000000000000000000fa56ea0" } , { "logIndex" : 35 , "transactionIndex" : 131 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x00000000000000000000000074de5d4fcbf63e00296fd95d33236b9794016631" , "0x000000000000000000000000af0b0000f0210d0f421f0009c72406703b50506b" ] , "data" : "0x000000000000000000000000000000000000000000000000000000000fa56ea0" } , { "logIndex" : 58 , "transactionIndex" : 140 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x00000000000000000000000048c04ed5691981c42154c6167398f95e8f38a7ff" , "0x000000000000000000000000f41d156a9bbc1fa6172a50002060cbc757035385" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000026273075" } , { "logIndex" : 230 , "transactionIndex" : 188 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x000000000000000000000000ba12222222228d8ba445958a75a0704d566bf2c8" , "0x00000000000000000000000053222470cdcfb8081c0e3a50fd106f0d69e63f20" ] , "data" : "0x00000000000000000000000000000000000000000000000000000002536916b7" } , { "logIndex" : 232 , "transactionIndex" : 188 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x00000000000000000000000053222470cdcfb8081c0e3a50fd106f0d69e63f20" , "0x00000000000000000000000088e6a0c2ddd26feeb64f039a2c41296fcb3f5640" ] , "data" : "0x00000000000000000000000000000000000000000000000000000002536916b7" } , { "logIndex" : 372 , "transactionIndex" : 205 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x0000000000000000000000001116898dda4015ed8ddefb84b6e8bc24528af2d8" , "0x0000000000000000000000002796317b0ff8538f253012862c06787adfb8ceb6" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000018307e19" } , { "logIndex" : 374 , "transactionIndex" : 205 , "topics" : [ "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef" , "0x0000000000000000000000002796317b0ff8538f253012862c06787adfb8ceb6" , "0x000000000000000000000000735b75559ebb9cd7fed7cec2372b16c3871d2031" ] , "data" : "0x0000000000000000000000000000000000000000000000000000000018307e19" } ] } ] Data requests ​ warning Addresses in all data requests must be in hex without 0x and in lowercase. Example: a614f803b6fd780986a42c78ec9c7f77e6ded13c All addresses in the responses will be in this format, too. Logs ​ { // data requests address : string [ ] , topic0 : string [ ] , topic1 : string [ ] , topic2 : string [ ] , topic3 : string [ ] , // related data retrieval transaction : boolean } address : the set of addresses of contracts emitting the logs. Omit to subscribe to events from all contracts in the network. topicN : the set of values of topicN. A log will be included in the response if it matches all the requests. An empty array matches no logs; omitted or null request matches any log. With transaction: true all parent transactions will be included into the response. Transactions ​ General ​ { // data requests type : string [ ] , // related data retrieval logs : boolean , internalTransactions : boolean } type : the set of acceptable transaction types. A transaction will be included in the response if it matches all the requests (just the type request in this case). An empty array matches no transactions; omitted or null request matches any transaction. With logs: true all logs emitted by the transactions will be included into the response. With internalTransactions: true all the internal transactions induced by the selected transactions will be included into the response. "transfer" ​ { // data requests owner : string [ ] , to : string [ ] , // related data retrieval logs : boolean , internalTransactions : boolean } owner : the set of owner addresses for which the transfer transactions should be retrieved. to : the set of destination addresses. A transaction will be included in the response if it matches all the requests. An empty array matches no transactions; omitted or null request matches any transaction. With logs: true all logs emitted by the transactions will be included into the response. With internalTransactions: true all the internal transactions induced by the selected transactions will be included into the response. "transfer asset" ​ { // data requests owner : string [ ] , to : string [ ] , asset : string [ ] , // related data retrieval logs : boolean , internalTransactions : boolean } owner : the set of owner addresses for which the transfer transactions should be retrieved. to : the set of destination addresses. asset : the set of asset contract addresses. A transaction will be included in the response if it matches all the requests. An empty array matches no transactions; omitted or null request matches any transaction. With logs: true all logs emitted by the transactions will be included into the response. With internalTransactions: true all the internal transactions induced by the selected transactions will be included into the response. "trigger smart contract" ​ { // data requests owner : string [ ] , to : string [ ] , // related data retrieval logs : boolean , internalTransactions : boolean } owner : the set of owner addresses for which the transfer transactions should be retrieved. to : the set of destination addresses. A transaction will be included in the response if it matches all the requests. An empty array matches no transactions; omitted or null request matches any transaction. With logs: true all logs emitted by the transactions will be included into the response. With internalTransactions: true all the internal transactions induced by the selected transactions will be included into the response. Internal transactions ​ { // data requests caller : string [ ] , transferTo : string [ ] , // related data retrieval transaction : boolean } caller : the set of addresses of caller contracts. transferTo : the set of addresses of receivers of TRX or TRC10 tokens. A transaction will be included in the response if it matches all the requests. An empty array matches no transactions; omitted or null request matches any transaction. With transaction: true all parent transactions for the selected internal transactions will be included into the response. Data fields selector ​ A selector of fields for the returned data items. Its structure is as follows: { block:               // field selector for blocks log:                 // field selector for logs transaction:         // field selector for transactions internalTransaction: // field selector for internal transactions } The transaction field selector is common for all transaction types except for the internal transactions. Block fields ​ { number hash parentHash timestamp txTrieRoot version timestamp witness_address witness_signature } A valid field selector for blocks is a JSON that has a subset of these fields as keys and true as values, e.g. {"hash": true, "timestamp": true} . Log fields ​ { logIndex transactionHash address data topics } A valid field selector for logs is a JSON that has a subset of these fields as keys and true as values, e.g. {"address": true, "transactionHash": true} . Transaction fields ​ { hash ret signature type parameter permissionId refBlockBytes refBlockHash feeLimit expiration timestamp rawDataHex fee contractResult contractAddress resMessage withdrawAmount unfreezeAmount withdrawExpireAmount cancelUnfreezeV2Amount result energyFee energyUsage energyUsageTotal netUsage netFee originEnergyUsage energyPenaltyTotal } A valid field selector for transactions is a JSON that has a subset of these fields as keys and true as values, e.g. {"hash": true, "type": true, "contractResult": true} . Internal transaction fields ​ { transactionHash hash callerAddress transferToAddress callValueInfo note rejected extra } A valid field selector for internal transactions is a JSON that has a subset of these fields as keys and true as values, e.g. {"hash": true, "callerAddress": true, "callValueInfo": true} . Edit this page Previous Network API Next SQD Firehose Router API Worker API Data requests Logs Transactions Internal transactions Data fields selector Block fields Log fields Transaction fields Internal transaction fields
Classes for blockchain data extraction and transformation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors Classes for blockchain data extraction and transformation üìÑÔ∏è Processor architecture Makeup of squid processors üóÉÔ∏è EVM 7 items üóÉÔ∏è Substrate 4 items Previous Reference Next Processor architecture
SQD Portal, now in closed beta. | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Portal, now in closed beta. On this page SQD Portal, now in closed beta. After five months of rigorous load testing and countless iterations, we’re thrilled to announce the first phase of the SQD Portal rollout, our decentralized, streaming-based data retrieval solution. What is the SQD Portal? ​ Portal reimagines blockchain data access, leaving behind the limitations of centralized RPC nodes. It’s faster, smarter, and fully decentralized—how data retrieval was always meant to be. Key Differences ​ Before Portal ​ Optimized but centralized : Data hosted on 32 large workers managed by Subsquid Labs. Limited replication : Minimal redundancy, increasing the risk of failures. With Portal ​ Fully decentralized : 1,400+ worker nodes power the SQD Mainnet independently. 30x replication on average : Data is redundantly stored across the network for maximum reliability. Faster than ever : The query engine, previously written in Python, has been fully rewritten in Rust. It now leverages parallelized queries and incorporates numerous query execution performance optimizations, delivering an overall 10-50x performance boost compared to the centralized Archives. What’s in It for You? ​ Portal is Live : Lock SQD tokens, run your own Portal, and take full control of your data pipeline. Cloud Migration : Available by invite-only, but you can request early access here . Increased Stability : Benefit from 30x replication and enhanced resilience. Unrivaled performance : Handle even the heaviest workloads with lightning-fast ingestion rates, capable of querying up to 20 million blocks per second . Why Portal? ​ Built for your needs: Speed : Optimized for even the most demanding workloads. Simplicity : Easy for newcomers, powerful for experts, with seamless migration from centralized Archives. Control : Run your own Portal for full autonomy. Data Sovereignty : Token-gated, anonymous access with full control over your bandwidth and API endpoints. Apply ​ Explore : Join our closed beta program to access the Public Portal where you can query data from 100+ EVM networks— apply here . Migrate : SQD Cloud Migration is invite-only —our team will reach out if you’re eligible. Want to migrate your Squids today? Reach out to us here to fast-track your migration. Self-Host : Want full autonomy? Request access through the form , and we’ll provide all the resources you need to set up your own Portal. What’s Next? ​ Roadmap Update : A detailed roadmap will be published very soon. January 2025 : Public Portal release and the debut of “Hot Blocks” for real-time, decentralized streaming. FAQs ​ Q: I’m used to querying via gateways. Will this disrupt my workflow? Not at all. Your experience will remain seamless—you can migrate to Portal and the decentralized network via changing a couple lines of configuration. Q: Are there any limitations? None. Everything you use today, plus added speed, reliability, and autonomy. Q: What will happen to the centralized version of the network ("Archives") that my Squids use now? Archives remain live at least until August 2025, with migration guides to help you switch. Retiring centralized archives ensures alignment with our vision of full decentralization. Q: Why should I switch to Portal? Faster queries with multi-node parallelism. Full autonomy through self-hosting. We’re excited to help you get started. – The SQD Team Edit this page What is the SQD Portal? Key Differences Before Portal With Portal What’s in It for You? Why Portal? Apply What’s Next? FAQs
Delegate | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Procuring SQD Delegate Run a worker Run a gateway Self-host a portal Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Participate Delegate On this page Delegate The easiest way to participate in SQD Network is to delegate your SQD tokens to one of the workers. Here's how to do it. Go to network.subsquid.io . Go to the "Dashboard" tab if you aren't there already. You should see something like this: Pick a worker and press the "Delegate" button. You should see a form like this: Enter the amount of SQD s you'd like to delegate and press the button. Confirm the transaction in your wallet and wait for it to go through. Go to the "Delegate" tab to view your delegations. Note that the "Undelegate" button is inactive - you cannot undelegate until the current epoch ends. Undelegating ​ Go to network.subsquid.io . Go to the "Delegate" tab. You should see your delegation there. If you delegated recently, the "Undelegate" button might be inactive. This indicates that your delegation is not yet be available for withdrawal. Wait until the current epoch ends (should take up to ~20 minutes). Once the "Undelegate" button is active, click it. You should see a form like this one: Enter the SQD amount you want to withdraw, click the large new "Undelegate" button and confirm the transaction. Maximizing the rewards ​ You'll get most rewards if you delegate to high quality but obscure workers. Look for workers with high uptime percentage and a lot of free delegation capacity. Currently, both the worker and its delegators are rewarded the most when the total delegated SQD count is around 30_000 . Anything more or less means less rewards. Edit this page Previous Procuring SQD Next Run a worker Undelegating Maximizing the rewards
Portal for EVM+Substrate | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Portal for EVM+Substrate On this page Migrate to the Cloud portal For users on EVM and Substrate networks ​ info SQD Network portals are currently in beta. Please report any bugs or suggestions to the SQD Portal Beta chat or to Squid Devs . This guide walks you through replacing a gateway of the open private version of SQD Network with a portal of the permissionless SQD Network as the primary source of data for your Cloud squids. SQD team operates two independent portals that serve the needs of Cloud users: The dedicated Cloud Portal is only visible from within the Cloud, ensuring stable performance for squids deployed there. The Public Portal can be accessed from anywhere for easy experimentation and local development. info We're currently experimenting with tightening the data request complexity limits. If you see an HTTP 400 error with a message like this: Couldn't parse query: query contains X item requests, but only 50 is allowed where X is some number above 50, or any other HTTP 400 response, please let us know. Here are the steps to migrate: Step 1: update to @latest packages ​ A. Enter your squid's folder. B. Remove both your lock file and the node_modules folder: NPM Yarn PNPM rm -r node_modules package-lock.json rm -r node_modules yarn.lock rm -r node_modules pnpm-lock.yaml C. Upgrade all SQD packages to the @latest major version: npx --yes npm-check-updates --filter "@subsquid/*" --upgrade D. Install the dependencies: NPM Yarn PNPM npm install yarn install pnpm install Step 2: tell your code to use the Portal ​ Follow the network-specific instructions from the Cloud: navigate to the Portal's page click on the tile of your network to see the instructions Once you're done, your squid will use the Cloud Portal when deployed and the public portal for local runs. Step 3 (recommended): testing ​ If you just want to update your squid code to use the Portal-enabled SDK version you can simply redeploy your squid here and be done. No need to re-sync. However, for the duration of this beta we strongly recommend that you re-sync your squid. This will allow you to make sure everything is working as it should and evaluate the improved data source performance. Follow the zero-downtime update procedure: Deploy your squid into a new slot. Wait for it to sync, observing the improved data fetching. Assign your production tag to the new deployment to redirect the GraphQL requests there. See this section for details. What's next? ​ Stay hyped for news on the introduction of real time data to EVM and Substate - it is coming soon. Edit this page For users on EVM and Substrate networks Step 1: update to @latest packages Step 2: tell your code to use the Portal Step 3 (recommended): testing What's next?
Versions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Versions On this page Versions Current â€‹ Restructured documentation covering SQD Network, Squid SDK and SQD Cloud as separate products. All older versions have been removed. If you came here looking for information on migrating from the old FireSquid SDK release, here are the guides: EVM guide Substrate guide Edit this page Current
Step 4: Optimization | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Step 1: Transfer events Step 2: Owners & tokens Step 3: External data Step 4: Optimization Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Indexing BAYC Step 4: Optimization On this page Step 4: Optimization This is the fourth part of the tutorial where we build a squid that indexes Bored Ape Yacht Club NFTs, their transfers, and owners from the Ethereum blockchain , fetches the metadata from IPFS and regular HTTP URLs, stores all the data in a database, and serves it over a GraphQL API. In the first three parts ( 1 , 2 , 3 ), we created a squid that does all the above but performs many IO operations sequentially, resulting in a long sync time. In this part, we discuss strategies for mitigating that shortcoming. We also discuss an alternative metadata fetching strategy that reduces redundant fetches and handles the changes in metadata of "cold" (i.e., not involved in any transfers) tokens more effectively. Pre-requisites: Node.js, Squid CLI , Docker, a project folder with the code from the third part ( this commit ). Using Multicall for aggregating state queries ​ We begin by introducing batch processing wherever possible, and our first step is to replace individual contract state queries with batch calls to a MakerDAO multicall contract . Retrieve the multicall contract ABI by re-running squid-evm-typegen with --multicall option: npx squid-evm-typegen --multicall src/abi 0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d #bayc This adds a Typescript ABI interface at src/abi/multicall.ts . Let us use it in a rewrite of completeTokens() : src/main.ts import { Multicall } from './abi/multicall' const MULTICALL_ADDRESS = '0xeefba1e63905ef1d7acba5a8513c70307c1ce441' const MULTICALL_BATCH_SIZE = 100 // ... async function completeTokens ( ctx : Context , partialTokensMap : Map < string , PartialToken > ) : Promise < Map < string , Token >> { let partialTokens : PartialToken [ ] = [ ... partialTokensMap . values ( ) ] let tokens : Map < string , Token > = new Map ( ) if ( partialTokens . length === 0 ) return tokens let lastBatchBlockHeader = ctx . blocks [ ctx . blocks . length - 1 ] . header let contract = new Multicall ( ctx , lastBatchBlockHeader , MULTICALL_ADDRESS ) let tokenURIs = await contract . aggregate ( bayc . functions . tokenURI , CONTRACT_ADDRESS , partialTokens . map ( t => [ t . tokenId ] ) , MULTICALL_BATCH_SIZE // paginating to avoid RPC timeouts ) for ( let [ i , ptoken ] of partialTokens . entries ( ) ) { let uri = tokenURIs [ i ] let metadata : TokenMetadata | undefined = await fetchTokenMetadata ( ctx , uri ) tokens . set ( ptoken . id , new Token ( { ... ptoken , uri , ... metadata } ) ) } return tokens } // ... Here we replaced the direct calls to tokenURI() of the BAYC token contract with aggreaged batches of 100 state calls and called aggregate() of the multicall contract for each batch. Multicall.aggregate() takes care of splitting the set of state calls into chunks and merging the results together. The number 100 for the batch size was chosen to be as large as possible while not triggering any response size limits on the public RPC endpoint we use. With a private RPC endpoint one would try to further increase the batch size. Note that we used the old Multicall V1 contract that was deployed before the BAYC contract. This is required because we're making queries to the historical state, and some of them will be made at block heights just a few hundred blocks above the BAYC contract deployment. Those queries would fail we used a multicall contract that did not yet exist back then. You can find the full code after this optimization at this commit . In our test this optimization reduced the time required for retrieving the contract state for all 10000 tokens once from 114 minutes to 94 seconds (figures out of date). Retrieving metadata from HTTPS concurrently ​ Next, we make our metadata retrieval requests concurrent. Many metadata URIs point to the same HTTPS server or to IPFS that we are accessing through a single gateway. When retrieving data from just a few servers, it is a common courtesy to not send all the requests at once; often, this is enforced by rate limits. Our code takes that into account, limiting the rate of the outgoing requests. We implement HTTPS batching at src/metadata.ts : import { asyncSleep , splitIntoBatches } from './util' const MAX_REQ_SEC = 10 export async function fetchTokenMetadatasConcurrently ( ctx : Context , uris : string [ ] ) : Promise < ( TokenMetadata | undefined ) [ ] > { let metadatas : ( TokenMetadata | undefined ) [ ] = [ ] for ( let batch of splitIntoBatches ( uris , MAX_REQ_SEC ) ) { let m = await Promise . all ( batch . map ( ( uri , index ) => { // spread out the requests evenly within a second interval let sleepMs = Math . ceil ( 1000 * ( index + 1 ) / MAX_REQ_SEC ) return asyncSleep ( sleepMs ) . then ( ( ) => fetchTokenMetadata ( ctx , uri ) ) } ) ) metadatas . push ( ... m ) } return metadatas } then we call the function from completeTokens() and use its output to populate metadata fields of Token entity instances. Utility functions asyncSleep() and splitIntoBathches() are implemented here . Full code is available at this commit . On the first processor batch, when we retrieve metadata from us-central1-bayc-metadata.cloudfunctions.net for all 10000 tokens, this optimization reduced the required time from 45 to 21 minutes (figures out of date). Retrieving immutable metadata once ​ Another glaring inefficiency in our current code is the fact that we often retrieve metadata from each URI more than once. This is understandable when we work with HTTPS links that point to mutable data; however, the data IPFS links point to is immutable, so there is no need to retrieve it more than once. To avoid these repeated retrievals we find the already known tokens in the database. If a token metadata URI is an immutable IPFS link (that is, does not point to MFS ) and metadata is already available, we skip the retrieval. The implementation is at src/metadata.ts : import { Token } from './model' import { In } from 'typeorm' export async function selectivelyUpdateMetadata ( ctx : Context , tokens : Map < string , Token > ) : Promise < Map < string , Token >> { let knownTokens : Map < string , Token > = await ctx . store . findBy ( Token , { id : In ( [ ... tokens . keys ( ) ] ) } ) . then ( ts => new Map ( ts . map ( t => [ t . id , t ] ) ) ) let updatedTokens : Map < string , Token > = new Map ( ) let tokensToBeUpdated : Token [ ] = [ ] for ( let [ id , t ] of tokens ) { let ktoken : Token | undefined = knownTokens . get ( id ) if ( ktoken != null && ktoken . image != null && ktoken . attributes != null && ktoken . uri === t . uri && uriPointsToImmutable ( t . uri ) ) { ctx . log . info ( ` Repeated retrieval from ${ t . uri } skipped ` ) updatedTokens . set ( id , ktoken ) } else { ctx . log . info ( ` Re-retrieving from ${ t . uri } ` ) tokensToBeUpdated . push ( t ) } } let metadatas : ( TokenMetadata | undefined ) [ ] = await fetchTokenMetadatasConcurrently ( ctx , tokensToBeUpdated . map ( t => t . uri ) ) for ( let [ i , t ] of tokensToBeUpdated . entries ( ) ) { let m = metadatas [ i ] if ( m != null ) { t . image = m . image t . attributes = m . attributes } updatedTokens . set ( t . id , t ) } return updatedTokens } uriPointsToImmutable() used to classify metadata URIs rejects any non-IPFS links and IPFS links that may point to MFS: export function uriPointsToImmutable ( uri : string ) : boolean { return uri . startsWith ( 'ipfs://' ) && ! uri . includes ( 'ipns' ) } Full code is available at this commit . In our test this optimization reduced the total sync time from about 4.1 to 1.5 hours (figures out of date). Alternative: Post-sync retrieval of metadata ​ Despite all the optimizations, our squid still takes 1.5 hours to sync instead of 11 minutes it needed before we introduced metadata retrieval (figures out of date). This is the cost of maintaining a fully populated database at all stages of the sync, which is rarely a requirement. An alternative is to begin retrieving metadata only after the rest of the data has been fully synced, and the squid has caught up with the blockchain. We do that by reading Token entities from the database after the initial sync, retrieving their metadata and persisting them. This approach has another advantage: metadata URIs can change without notice, and our old retrieval strategy would only pick up the changes when the token has been transferred. If our goal is to keep token metadata as up-to-date as possible, we have to constantly renew it even for tokens that are not involved in any recent transfers. This is easy to implement with our new metadata retrieval strategy: simply add a field to the Token entity that tracks the block height of the most recent metadata update and select Token s that were updated some fixed number of blocks ago for metadata updates. Here is how the new metadata retrieval strategy reflects in the batch handler code: processor.run(new TypeormDatabase(), async (ctx) => { let rawTransfers: RawTransfer[] = getRawTransfers(ctx) let owners: Map<string, Owner> = createOwners(rawTransfers) let tokens: Map<string, Token> = await createTokens(ctx, rawTransfers, owners) let transfers: Transfer[] = createTransfers(rawTransfers, owners, tokens) await ctx.store.upsert([...owners.values()]) await ctx.store.upsert([...tokens.values()]) await ctx.store.insert(transfers) + + if (ctx.isHead) { + let updatedTokens = await updateTokensWithOutdatedMetadata(ctx) + await ctx.store.upsert(updatedTokens) + ctx.log.info(`Updated metadata for ${updatedTokens.length} tokens`) + } }) Full implementation requires changes to the schema, replacement of completeTokens() with updateTokensWithOutdatedMetadata() and rewrites of createTokens() and selectivelyUpdateMetadata() . It is available in this branch . The resulting squid took 10 minutes for the initial sync, then 50 minutes more to retrieve metadata at least once for every token (figures out of date). Extra: Using Multicall for metadata exploration ​ In part 3 of this tutorial we explored metadata URIs by running tokenURI() directly on the BAYC token contract. This process took several hours. Replacing the exploration code with its [multicall]-based equivalent, we can reduce that time to about 17 minutes (figure out of date). Starting with the code as it was at the end of part two , get src/abi/multicall.ts by running npx squid-evm-typegen --multicall src/abi 0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d #bayc then add the code for batch URI retrieval to the batch handler: src/main.ts + import {Multicall} from './abi/multicall' + const MULTICALL_ADDRESS = '0xeefba1e63905ef1d7acba5a8513c70307c1ce441' + const MUTLTICALL_BATCH_SIZE = 100 processor.run(new TypeormDatabase(), async (ctx) => { let tokens: Map<string, Token> = createTokens(rawTransfers, owners) let transfers: Transfer[] = createTransfers(rawTransfers, owners, tokens) + let lastBatchBlockHeader = ctx.blocks[ctx.blocks.length-1].header + let contract = new Multicall(ctx, lastBatchBlockHeader, MULTICALL_ADDRESS) + let tokenURIs = await contract.aggregate( + bayc.functions.tokenURI, + CONTRACT_ADDRESS, + [...tokens.values()].map(t => [t.tokenId]), + MUTLTICALL_BATCH_SIZE + ) + for (let uri of tokenURIs) { + ctx.log.info(`Retrieved a metadata URI: ${uri}`) + } + await ctx.store.upsert([...owners.values()]) await ctx.store.upsert([...tokens.values()]) await ctx.store.insert(transfers) }) Edit this page Previous Step 3: External data Next Index to local CSV files Using Multicall for aggregating state queries Retrieving metadata from HTTPS concurrently Retrieving immutable metadata once Alternative: Post-sync retrieval of metadata Extra: Using Multicall for metadata exploration
General settings | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Block data for Tron General settings Transactions Logs Internal transactions Field selection Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing TronBatchProcessor General settings On this page General settings tip The method documentation is also available inline and can be accessed via suggestions in most IDEs. The following setters configure the global settings of TronBatchProcessor . They return the modified instance and can be chained. The only requirement is to specify at least one data souce via setGateway() or `setHttpApi() . If you need real-time data, providing an HTTP API data source is a hard requirement. If you add both an SQD Network gateway and an HTTP API endpoint, the processor will obtain as much data as is currently available from the gateway, then switch to ingesting recent data via HTTP API. If you only add an SQD Network gateway, your data will be being several thousands of blocks behind the chain head most of the time. setGateway(url: string | GatewaySettings) ​ Use a SQD Network gateway. The argument is either a string URL of the gateway or { url : string // gateway URL requestTimeout ? : number // in milliseconds } setHttpApi(settings?: HttpApiSettings) ​ We must use a regular HTTP API endpoint to get through the last mile and stay on top of the chain. This is a limitation, and we promise to lift it in the future. interface HttpApiSettings = { url : string strideConcurrency ? : // num of concurrent connections, default 2 strideSize ? : number // query size, default 10 headPollInterval ? : number // milliseconds, default 1000 } setBlockRange({from: number, to?: number}) ​ Limits the range of blocks to be processed. When the upper bound is specified, processor will terminate with exit code 0 once it reaches it. Note that block ranges can also be specified separately for each data request. This method sets global bounds for all block ranges in the configuration. includeAllBlocks(range?: {from: number, to?: number}) ​ By default, processor will fetch only blocks which contain requested items. This method modifies such behavior to fetch all chain blocks. Optionally a range of blocks can be specified for which the setting should be effective. setPrometheusPort(port: string | number) ​ Sets the port for a built-in prometheus health metrics server (serving at http://localhost:${port}/metrics ). By default, the value of PROMETHEUS_PORT environment variable is used. When it is not set, processor will pick an ephemeral port. Edit this page Previous Block data for Tron Next Transactions setGateway(url: string | GatewaySettings) setHttpApi(settings?: HttpApiSettings) setBlockRange({from: number, to?: number}) includeAllBlocks(range?: {from: number, to?: number}) setPrometheusPort(port: string | number)
Project structure | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Environment set up Indexer from scratch Development flow Project structure sqd CLI cheatsheet Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Getting started Project structure Squid project structure All files and folders except package.json are optional. package.json -- Configuration file for dependencies and the build script (invoked with npm run build ). Hard requirement for deploying to SQD Cloud . package-lock.json OR yarn.lock OR pnpm-lock.yaml -- Dependencies shrinkwrap. Required for Cloud deployment, except those that override the dependencies installation command . tsconfig.json -- Configuration of tsc . Required for most squids. Deployment manifest ( squid.yaml by default) -- Definitions of squid services used for running it locally with sqd run and deploying to SQD Cloud . .squidignore -- Files and patterns to be excluded when sending the squid code to the Cloud . When not supplied, some files will still be omitted: see the reference page for details. schema.graphql -- The schema definition file . Required if your squid stores its data in PostgreSQL . /src -- The TypeScript source code folder for the squid processor. /src/main.ts -- The entry point of the squid processor process. Typically, contains a processor.run() call. /src/processor.ts -- Processor object ( EVM or Substrate ) definition and configuration. /src/model/generated -- The folder for the TypeORM entities generated from schema.graphql . /src/model -- The module exporting the entity classes. /src/server-extension/resolvers -- A folder for user-defined GraphQL resolvers used by OpenReader . /src/types -- A folder for types generated by the Substrate typegen tool for use in data decoding. /src/abi -- A folder for modules generated by the EVM typegen tool containing type definitions and data decoding boilerplate code. /db -- The designated folder with the database migrations . /lib -- The output folder for the compiled squid code. /assets -- A designated folder for custom user-provided files (e.g. static data files to seed the squid processor with). /abi -- A designated folder for JSON ABI files used as input by the EVM typegen . docker-compose.yml -- A Docker compose file for local runs. Has a Postgres service definition by default. .env -- Defines environment variables used by docker-compose.yml and when the squid is run locally. typegen.json -- The config file for the Substrate typegen tool. commands.json -- User-defined scripts picked up by Squid CLI . See also the CLI cheatsheet . Edit this page Previous Development flow Next sqd CLI cheatsheet
RPC service networks | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference RPC service networks RPC service networks Enable the add-on with: deploy : addons : rpc : - eth - goerli.http See the RPC addon guide for details. Available EVM networks Network name network.protocol Arbitrum One arbitrum-one.http Arbitrum One Goerli arbitrum-goerli.http Arbitrum One Sepolia arbitrum-sepolia.http Arbitrum Nova arbitrum-nova.http Astar astar.http AVA ava.http AVA Testnet ava-testnet.http Base base.http Base Goerli base-goerli.http Base Sepolia base-sepolia.http Berachain berachain.http Blast L2 blast-l2.http BSC bsc.http BSC Testnet bsc-testnet.http Ethereum eth.http Ethereum Goerli eth-goerli.http Ethereum Holesky eth-holesky.http Ethereum Sepolia eth-sepolia.http Evmos evmos.http Fantom fantom.http Fantom Testnet fantom-testnet.http Gnosis gnosis.http Linea linea.http Mantle mantle.http Mantle mantle-sepolia.http Metis metis.http Moonbase moonbase-alpha.http Moonbeam moonbeam.http Moonriver moonriver.http OKTC oktc.http opBNB opbnb.http opBNB opbnb-testnet.http Optimism optimism.http Optimism Goerli optimism-goerli.http Polygon polygon.http Polygon Amoy polygon-amoy-testnet.http Polygon Testnet polygon-testnet.http Polygon zkEVM polygon-zkevm.http Polygon zkEVM Cardona polygon-zkevm-cardona-testnet.http Polygon zkEVM Testnet polygon-zkevm-testnet.http Scroll scroll.http Scroll Sepolia scroll-sepolia.http Shibuya shibuya.http Shiden shiden.http Sonic sonic-mainnet.http Unichain unichain.http Unichain Sepolia unichain-sepolia.http zkSync zksync.http zkSync Sepolia zksync-sepolia.http Available Substrate networks Network name network.protocol Acala acala.http Aleph Zero aleph-zero.http Aleph Zero Testnet aleph-zero-testnet.http Amplitude amplitude.http Asset Hub Kusama asset-hub-kusama.http Asset Hub Polkadot asset-hub-polkadot.http Asset Hub Rococo asset-hub-rococo.http Asset Hub Westend asset-hub-westend.http Astar astar-substrate.http Basilisk basilisk.http Bittensor bittensor.http Bittensor Testnet bittensor-testnet.http Bridge Hub Kusama bridge-hub-kusama.http Bridge Hub Polkadot bridge-hub-polkadot.http Bridge Hub Rococo bridge-hub-rococo.http Bridge Hub Westend bridge-hub-westend.http Centrifuge centrifuge.http Collectives Polkadot collectives-polkadot.http Collectives Westend collectives-westend.http Crust crust.http Darwinia darwinia.http Darwiniacrab darwiniacrab.http Eden eden.http Frequency frequency.http Hydradx hydradx.http Interlay interlay.http Karura karura.http Khala khala.http Kilt kilt.http Kintsugi kintsugi.http Kusama kusama.http Litentry litentry.http Moonbase moonbase.http Moonbeam moonbeam-substrate.http Moonriver moonriver-substrate.http Pendulum pendulum.http Phala phala.http Polkadex polkadex.http Polkadot polkadot.http Rococo rococo.http Shibuya shibuya-substrate.http Shiden shiden-substrate.http Turing turing.http Zeitgeist zeitgeist.http Edit this page Previous addons.hasura section Next .squidignore file(s)
Storage state diffs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM Storage state diffs On this page Storage state diffs tip State diffs for historical blocks are currently available from SQD Network on the same basis as all other data stored there: for free. If you deploy a squid that indexes traces in real-time to SQD Cloud and use our RPC addon , the necessary trace_ or debug_ RPC calls made will be counted alongside all other calls and the price will be computed for the total count. There are no surcharges for traces or state diffs. addStateDiff(options) â€‹ Subscribe to changes in the contract storage . This allows for tracking the contract state changes that are difficult to infer from events or transactions, such as the changes that take into account the output of internal calls. options has the following structure: { // data requests address ? : string [ ] key ? : string [ ] kind ? : ( '=' | '+' | '*' | '-' ) [ ] range ? : { from : number , to ? : number } // related data retrieval transaction ? : boolean } The data requests here are: address : the set of addresses of contracts to track. Leave undefined to subscribe to state changes of all contracts from the whole network. key : the set of storage keys that should be tracked. Regular hexadecimal contract storage keys and special keys ( 'balance' , 'code' , 'nonce' ) are allowed. Leave undefined to subscribe to all state changes. kind : the set of diff kinds that should be tracked. Refer to the StateDiff section of data items documentation for an explanation of the meaning of the permitted values. range : the range of blocks within which the storage changes should be tracked. Enabling the transaction flag will cause the processor to retrieve the transaction that gave rise to each state change and add it to the transactions iterable of block data . Note that state diffs can also be requested by the addTransaction() method as related data. Selection of the exact data to be retrieved for each state diff item and its optional parent transaction is done with the setFields() method documented on the Field selection page. Unlike other data items, state diffs do not have any fields that can be enabled, but some can be disabled for improved sync performance. Edit this page Previous Transactions Next Traces
Fuel Network API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Cheatsheet Network API Fuel Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing Network API Fuel Network API On this page Fuel SQD Network API warning The Fuel API of SQD Network is currently in beta. Breaking changes may be introduced in the future releases. SQD Network API distributes the requests over a ( potentially decentralized ) network of workers . The main gateway URL points at a router that provides URLs of workers that do the heavy lifting. Each worker has its own range of blocks on each dataset it serves. Suppose you want to retrieve an output of some query on a block range starting at firstBlock (can be the genesis block) and ending at the highest available block. Proceed as follows: Retrieve the dataset height from the router with GET /height and make sure it's above firstBlock . Save the value of firstBlock to some variable, say currentBlock . Query the router for an URL of a worker that has the data for currentBlock with GET /${currentBlock}/worker . Retrieve the data from the worker by posting the query ( POST / ), setting the "fromBlock" query field to ${currentBlock} . Parse the retrieved data to get a batch of query data plus the height of the last block available from the current worker. Take the header.number field of the last element of the retrieved JSON array - it is the height you want. Even if your query returns no data, you'll still get the block data for the last block in the range, so this procedure is safe. Set currentBlock to the height from the previous step plus one . Repeat steps 3-6 until all the required data is retrieved. See the Fuel section of the public gateways list for URL gateways. Implementation examples: Manually with cURL Suppose we want data on Fuel receipts from block 1000000 . We begin by finding the main URL for the Fuel Mainnet dataset. Then we have to: Retrieve the dataset height from the router with curl https://v2.archive.subsquid.io/network/fuel-mainnet/height Output 1211611 Save the value 1000000 to some variable, say currentBlock . Query the router for an URL of a worker that has the data for currentBlock curl https://v2.archive.subsquid.io/network/fuel-mainnet/1000000/worker Output https://rb03.sqd-archive.net/worker/query/czM6Ly9mdWVsLXRlc3RuZXQ Retrieve the data from the worker curl https://rb03.sqd-archive.net/worker/query/czM6Ly9mdWVsLXRlc3RuZXQ \ -X 'POST' -H 'content-type: application/json' -H 'accept: application/json' \ -d '{ "type": "fuel", "fromBlock":1000000, "fields":{"receipt":{"contract":true, "receiptType": true}}, "receipts":[ {"type": ["LOG_DATA"]} ] }' | jq Output: [ { "header" : { "number" : 1000000 , "hash" : "0xdc31db7fa3c1fb4f3e0910dc5abf927e64cc985eb2eb13418a9f2e00c4b7ad23" } , "receipts" : [ ] } , { "header" : { "number" : 1002527 , "hash" : "0x649f045675405f9d4ee34bb19479d0e5706ed14615e8f97da9f34dd166e37f35" } , "receipts" : [ { "transactionIndex" : 0 , "index" : 1 , "contract" : "0xe637b4c254aa07baa9845eb9f8c7ad0965fad5c5b1194cb37193f956be0ce6f3" , "receiptType" : "LOG_DATA" } ] } , ... { "header" : { "number" : 1211611 , "hash" : "0x04c9ef60f2b54d32569a410477c136f11b692c1d3eadaa5b946a0295b526223e" } , "receipts" : [ ] } ] Parse the retrieved data to get a batch of query data plus the height of the last block available from the current worker. Take the header.number field of the last element of the retrieved JSON array - it is the height you want. Even if your query returns no data, you'll still get the block data for the last block in the range, so this procedure is safe. Set currentBlock to the height from the previous step plus one. Repeat steps 3-6 until all the required data is retrieved. In Python def get_text ( url : str ) - > str : res = requests . get ( url ) res . raise_for_status ( ) return res . text def dump ( gateway_url : str , query : Query , first_block : int , last_block : int ) - > None : assert 0 <= first_block <= last_block query = dict ( query ) # copy query to mess with it later dataset_height = int ( get_text ( f' { gateway_url } /height' ) ) next_block = first_block last_block = min ( last_block , dataset_height ) while next_block <= last_block : worker_url = get_text ( f' { gateway_url } / { next_block } /worker' ) query [ 'fromBlock' ] = next_block query [ 'toBlock' ] = last_block res = requests . post ( worker_url , json = query ) res . raise_for_status ( ) blocks = res . json ( ) last_processed_block = blocks [ - 1 ] [ 'header' ] [ 'number' ] next_block = last_processed_block + 1 for block in blocks : print ( json . dumps ( block ) ) Full code here . Router API ​ GET /height (get height of the dataset) Example response: 16576911 . GET ${firstBlock}/worker (get a suitable worker URL) The returned worker will be capable of processing POST / requests in which the "fromBlock" field is equal to ${firstBlock} . Example response: https://rb03.sqd-archive.net/worker/query/czM6Ly9mdWVsLXRlc3RuZXQ . Worker API ​ POST / (query inputs and receipts) Query Fields ​ fromBlock : Block number to start from (inclusive). toBlock : (optional) Block number to end on (inclusive). If this is not given, the query will go on for a fixed amount of time or until it reaches the height of the dataset. includeAllBlocks : (optional) If true, the Network will include blocks that contain no data selected by data requests into its response. fields : (optional) A selector of data fields to retrieve. Common for all data items. receipts : (optional) A list of receipts requests . An empty list requests no data. inputs : (optional) A list of inputs requests . An empty list requests no data. outputs : (optional) A list of outputs requests . An empty list requests no data. transactions : (optional) A list of transactions requests . An empty list requests no data. Example Request ​ { "type" : "fuel" , "fromBlock" : 1000000 , "toBlock" : 1100000 , "fields" : { "receipt" : { "contract" : true , "receiptType" : true } } , "receipts" : [ { "type" : [ "LOG_DATA" ] } ] , "inputs" : [ { "type" : [ "InputCoin" ] } ] } Example Response ​ [ { "header" : { "number" : 1000000 , "hash" : "0xdc31db7fa3c1fb4f3e0910dc5abf927e64cc985eb2eb13418a9f2e00c4b7ad23" } , "receipts" : [ ] , "inputs" : [ ] } , { "header" : { "number" : 1002527 , "hash" : "0x649f045675405f9d4ee34bb19479d0e5706ed14615e8f97da9f34dd166e37f35" } , "receipts" : [ { "transactionIndex" : 0 , "index" : 1 , "contract" : "0xe637b4c254aa07baa9845eb9f8c7ad0965fad5c5b1194cb37193f956be0ce6f3" , "receiptType" : "LOG_DATA" } ] , "inputs" : [ { "transactionIndex" : 0 , "index" : 1 } ] } , ... { "header" : { "number" : 1100000 , "hash" : "0x4e1420d7c2cd973842ef1ce919560f15e5461376b7533c371fb895034c85dfd3" } , "receipts" : [ ] , "inputs" : [ ] } ] Data requests ​ Receipts ​ { type ? : ReceiptType [ ] contract ? : string [ ] transaction ? : boolean } Receipts will be included in the response if it matches all the requests. An empty array matches no instructions; omit all requests to match all receipts. See addReceipt() SDK data request method for details on this request; also see Receipt fields . Transactions ​ { type ? : TransactionType [ ] receipts ? : boolean inputs ? : boolean outputs ? : boolean } A transaction will be included in the response if it matches all the requests. An empty array matches no transactions; omit all requests to match all transactions. See addTransaction() SDK data request method for details on this request; also see Transaction fields . Inputs ​ { type ? : InputType [ ] coinOwner ? : Bytes [ ] coinAssetId ? : Bytes [ ] contractContract ? : Bytes [ ] messageSender ? : Bytes [ ] messageRecipient ? : Bytes [ ] transaction ? : boolean } An input will be included in the response if it matches all the requests. An empty array matches no inputs; omit all requests to match all inputs. See addInput() SDK data request method for details on this request; also see Input fields . Outputs ​ { type ? : OutputType [ ] transaction ? : boolean } An output will be included in the response if it matches all the requests. An empty array matches no outputs; omit all requests to match all outputs. See addOutput() SDK data request method for details on this request; also see Output fields . Data fields selector ​ A JSON selector of fields for the returned data items. Documented in the Field selectors section. Edit this page Previous Network API Next Tron indexing Router API Worker API Data requests Receipts Transactions Inputs Outputs Data fields selector

General settings | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource General settings On this page General settings tip The method documentation is also available inline and can be accessed via suggestions in most IDEs. The following setters configure the global settings of DataSourceBuilder for Solana Procesor. They return the modified instance and can be chained. One or both of setGateway() or setRpcEndpoint() must be called to make the data source usable. setRpcEndpoint() is required if you need real-time data; setGateway() is required if you want to get historical data from SQD Network at high speed. All other methods are optional. If you add both a SQD Network gateway and an RPC endpoint, the processor will obtain as much data as is currently available from the gateway, then switch to ingesting recent data via RPC. If you only add a SQD Network gateway, your data will be being several thousands of blocks behind the chain head most of the time. setGateway(url: string | GatewaySettings) ​ Use a SQD Network gateway. The argument is either a string URL of the gateway or { url : string // gateway URL requestTimeout ? : number // in milliseconds } setRpc(settings?: RpcSettings) ​ Adds a RPC data source. If added, it will be used for RPC ingestion . The argument format is: type RpcSettings = { client : SolanaRpcClient ; strideSize ? : number ; // `getBlock` batch call size, default 5 strideConcurrency ? : number ; // num of concurrent connections, default 10 concurrentFetchThreshold ? : number ; // min distance from head that triggers a fetch, default 50 } ; SolanaRpcClient class is exported by @subsquid/solana-stream . Its constructor arg type is { url : string ; // http, https, ws and wss are supported capacity ? : number ; // num of concurrent connections, default 10 rateLimit ? : number ; // requests per second, default is no limit requestTimeout ? : number ; // in milliseconds, default 30_000 retryAttempts ? : number , // num of retries on failed RPC calls, default 0 retrySchedule ? : number , // retry pauses in ms maxBatchCallSize ? : number ; // default 100 headers ? : Record < string , string > , // http headers log ? : Logger | null // customize or disable RPC client logs } setBlockRange({from: number, to?: number}) ​ Limits the range of blocks to be processed. When the upper bound is specified, processor will terminate with exit code 0 once it reaches it. Note that block ranges can also be specified separately for each data request. This method sets global bounds for all block ranges in the configuration. includeAllBlocks(range?: {from: number, to?: number}) ​ By default, processor will fetch only blocks which contain requested items. This method modifies such behavior to fetch all chain blocks. Optionally a range of blocks can be specified for which the setting should be effective. Edit this page Previous Block data for Solana Next Instructions setGateway(url: string | GatewaySettings) setRpc(settings?: RpcSettings) setBlockRange({from: number, to?: number}) includeAllBlocks(range?: {from: number, to?: number})
scale section | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference scale section On this page Scale the deployment The scale: section of the deployment manifest allows allocating additional computing resources for the squid add-ons and services. This option is only available for paid squids deployed in professional organizations . info Visit the Pricing page and/or our costs calculator if you're looking for an estimate. The manifest supports the following scaling options: dedicated: ​ Default: dedicated: true . When a dedicated profile is used, the resources that the squid requests are reserved in advance. This is the new/current default and is the recommended profile for production squids. Squid deployment must be dedicated to be a subject of SQD Cloud SLA . By setting dedicated: false you can request for your deployment to be collocated - that is, share resources with other deployments. The allocation of full resources is not guaranteed for collocated squids. All playground squids must use the collocated profile, that is, explicitly specify dedicated: false . addons: ​ postgres: ​ See Postgres add-on for details. services: ​ api: ​ Name Description Type Default value Optional profile Allocated resources profile small | medium | large | xlarge | 2xlarge small Optional replicas The number of gateway replicas. The API requests are distributed between the replicas in the round-robin fashion Number 1 Optional The profile specifications for API service replicas are as follows: Profile colocated vCPU (max) colocated RAM (max) dedicated vCPU (requested) dedicated RAM (max) small 0.2 768Mi 0.5 768Mi medium 0.5 1.5Gi 1 1.5Gi large 1 3Gi 2 3Gi xlarge - - 4 6Gi 2xlarge - - 8 12Gi processor: ​ Name Description Type Default value Optional profile Allocated resources profile small | medium | large | xlarge | 2xlarge small Optional The profile specifications for a processor service are as follows: Profile colocated vCPU (max) colocated RAM (max) dedicated vCPU (requested) dedicated RAM (max) small 0.2 768Mi 0.5 768Mi medium 0.5 1.5Gi 1 1.5Gi large 1 3Gi 2 3Gi xlarge - - 4 6Gi 2xlarge - - 8 12Gi Example ​ squid.yaml manifest_version : subsquid.io/v0.1 name : sample - squid build : deploy : addons : postgres : processor : cmd : [ "sqd" , "process:prod" ] api : cmd : [ "sqd" , "serve:prod" ] scale : addons : postgres : storage : 100G profile : medium processor : profile : medium api : profile : large # load-balance three replicas replicas : 3 Edit this page Previous Deployment manifest Next addons.postgres section dedicated: addons: postgres: services: api: processor: Example
Index Solana with SQD | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing Index Solana with SQD üóÉÔ∏è How to start 2 items üóÉÔ∏è SDK 2 items üóÉÔ∏è Network API 1 items Previous Changelog: slots and tags Next How to start
Block data for Tron | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Block data for Tron General settings Transactions Logs Internal transactions Field selection Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing TronBatchProcessor Block data for Tron Block data for Tron In Tron Squid SDK, the data is processed by repeatedly calling the user-defined batch handler function on batches of on-chain data. The sole argument of the batch handler is its context ctx , and ctx.blocks is an array of Block objects containing the data to be processed, aligned at the block level. For TronBatchProcessor the Block interface is defined as follows: export interface Block < F extends FieldSelection = { } > { header : BlockHeader < F > transactions : Transaction < F > [ ] logs : Log < F > [ ] internalTransactions : InternalTransaction < F > [ ] } F here is the type of the argument of the setFields() processor method. Block.header contains the block header data. The rest of the fields are iterables containing the three kinds of blockchain data. The items within each iterable are ordered in the same way as they are within the block. The exact fields available in each data item type are inferred from the setFields() call argument. The method is documented on the field selection page: Transaction section ; Log section ; InternalTransaction section . BlockHeader section Edit this page Previous TronBatchProcessor Next General settings
Batch processing | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Batch processing On this page Batch processing Batch data processing model employed by Squid SDK relies on the following principles: Minimize the number of database hits by grouping multiple single-row transactions into multi-row batch transactions. Transform the data in memory using vectorized operators. Use the MakerDAO Multicall contract to batch EVM state queries. Use XXX.getMany() to batch Substrate state queries. In practice, batching is a more flexible (compared to handler parallelization) way to speed up the inherently sequential indexing of on-chain transactions and logs. To illustrate, assume the processor must infer the current state of an on-chain record. It is configured to listen to the two on-chain events, Create and Update , that are emitted once the record is created or updated. The data batch received by the processor is then an array of event items, i.e. [ Create ( { id : 1 , name : 'Alice' } ) , Update ( { id : 1 , name : 'Bob' } ) , Create ( { id : 2 , name : 'Mikee' } ) , Update ( { id : 1 , name : 'Carol' } ) , Update ( { id : 2 , name : 'Mike' } ) ] Following the principles above, a processor would update the intermediary entity states in memory, persisting only the final state: [ { id : 1 , name : 'Carol' } , { id : 2 , name : 'Mike' } ] in a single transaction. Let's see the batch processing principles in action. Patterns ​ An idiomatic use of processor.run() is as follows: processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { // a decoded and normalized ctx.blocks data // (just the logs in this example) const logDataBatch = [ ] for ( const block of ctx . blocks ) { for ( const log of c . logs ) { // transform and normalize the raw logs data // based on the onchain data logDataBatch . push ( decodeAndTransformToMyData ( log ) ) } } // the set of my target entity IDs to be updated/created const myEntityIds = new Set ( ) for ( const d of logDataBatch ) { // the business logic mapping // the on-chain data with the target // entity ID myEntityIds . add ( extractEntityId ( d ) ) } // load the enities by the list of IDs and // put into an ID map const myEntities : Map < string , MyEntity > = new Map ( // batch-load using IN operator ( await ctx . store . findBy ( MyEntity , { id : In ( [ ... myEntityIds ] ) } ) ) // put the result into the ID map . map ( ( entity ) => [ entity . id , entity ] ) ) ; // calculate the updated state of the entities for ( const d of logDataBatch ) { const myEntity = myEntities . get ( extractEntityId ( d ) ) if ( myEntity == null ) { // create a new instance with d and // add to myEntities map } else { // update myEntity using d } } // batch-update all entities in the map await ctx . store . save ( [ ... myEntities . values ( ) ] ) } ) ; For a full implementation of the above pattern, see EVM squid example or Substrate squid example . Anti-patterns ​ Avoid loading or persisting single entities unless strictly necessary. For example, here is a possible antipattern for the Gravatar example : processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( const c of ctx . blocks ) { for ( const log of c . logs ) { // making sure that we process only the relevant logs if ( log . address !== GRAVATAR_CONTRACT || ( log . topics [ 0 ] !== events . NewGravatar . topic && log . topics [ 0 ] !== events . UpdatedGravatar . topic ) ) continue const { id , owner , displayName , imageUrl } = extractData ( log ) // ANTIPATTERN!!! // Doing an upsert per event drastically decreases the indexing speed await ctx . store . save ( Gravatar , new Gravatar ( { id : id . toHexString ( ) , owner : decodeHex ( owner ) , displayName , imageUrl } ) ) } } } ) Instead, use an in-memory cache, and batch upserts: processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { const gravatars : Map < string , Gravatar > = new Map ( ) ; for ( const c of ctx . blocks ) { for ( const log of c . logs ) { if ( log . address !== GRAVATAR_CONTRACT || ( log . topics [ 0 ] !== events . NewGravatar . topic && log . topics [ 0 ] !== events . UpdatedGravatar . topic ) ) continue const { id , owner , displayName , imageUrl } = extractData ( log ) gravatars . set ( id . toHexString ( ) , new Gravatar ( { id : id . toHexString ( ) , owner : decodeHex ( owner ) , displayName , imageUrl } ) ) } } await ctx . store . save ( [ ... gravatars . values ( ) ] ) } ) Migrate from handlers ​ Batch-based processing can be used as a drop-in replacement for the handler-based mappings employed by e.g. subgraphs. While the handler-based processing is significantly slower due to excessive database lookups and writes, it may be a good intermediary step while migrating an existing subgraph to Squid SDK . One can simply re-use the existing handlers while looping over the ctx items: processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( const c of ctx . blocks ) { for ( const log of c . logs ) { switch ( log . topics [ 0 ] ) { case abi . events . FooEvent . topic : await handleFooEvent ( ctx , log ) continue case abi . events . BarEvent . topic : await handleFooEvent ( ctx , log ) continue default : continue } } for ( const txn of c . transactions ) { // 0x + 4 bytes const sighash = txn . input . slice ( 0 , 10 ) switch ( sighash ) { case '0xa9059cbb' : // transfer(address,uint256) sighash await handleTransferTx ( ctx , txn ) continue case abi . functions . approve . sighash : await handleApproveTx ( ctx , txn ) continue default : continue } } } } ) Block hooks ​ Similarly, one can implement pre- and post- block hooks: processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { for ( const c of ctx . blocks ) { await preBlockHook ( ctx , c ) for ( const log of c . logs ) { // some logic } for ( const txn of c . transactions ) { // some more logic } await postBlockHook ( ctx , c ) } } ) Edit this page Previous Features & Guides Next RPC ingestion and reorgs Patterns Anti-patterns Migrate from handlers Block hooks
Custom API extensions | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Caching Custom API extensions Subscriptions DoS protection Access control Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Configuration Custom API extensions On this page Custom GraphQL resolvers One can extend the GraphQL API generated by OpenReader with custom queries. To do that, one can define GraphQL query resolvers in the designated module src/server-extension/resolvers . Note that all resolver classes (including any additional types) must be exported by src/server-extension/resolvers/index.ts . A custom resolver should import TypeGraphQL types and use annotations provided by the library to define query arguments and return types. If your squid lacks a type-graphql dependency, add it with: npm i type-graphql Custom resolvers are normally used in combination with TypeORM EntityManager for accessing the API server target database. It is automatically injected when defined as a single constructor argument of the resolver. Examples ​ Simple entity counter ​ import { Query , Resolver } from 'type-graphql' import type { EntityManager } from 'typeorm' import { Burn } from '../model' @ Resolver ( ) export class CountResolver { constructor ( private tx : ( ) => Promise < EntityManager > ) { } @ Query ( ( ) => Number ) async totalBurns ( ) : Promise < number > { const manager = await this . tx ( ) return await manager . getRepository ( Burn ) . count ( ) } } This example is designed to work with the evm template: grab a test squid as described here ; install type-graphql ; save the example code to src/server-extension/resolver.ts ; re-export CountResolver at src/server-extension/resolvers/index.ts : export { CountResolver } from '../resolver' rebuild the squid with npm run build ; (re)start the GraphQL server with npx squid-graphql-server . totalBurns selection will appear in the GraphiQL playground . Custom SQL query ​ import { Arg , Field , ObjectType , Query , Resolver } from 'type-graphql' import type { EntityManager } from 'typeorm' import { MyEntity } from '../model' // Define custom GraphQL ObjectType of the query result @ ObjectType ( ) export class MyQueryResult { @ Field ( ( ) => Number , { nullable : false } ) total ! : number @ Field ( ( ) => Number , { nullable : false } ) max ! : number constructor ( props : Partial < MyQueryResult > ) { Object . assign ( this , props ) ; } } @ Resolver ( ) export class MyResolver { // Set by depenency injection constructor ( private tx : ( ) => Promise < EntityManager > ) { } @ Query ( ( ) => [ MyQueryResult ] ) async myQuery ( ) : Promise < MyQueryResult [ ] > { const manager = await this . tx ( ) // execute custom SQL query const result : = await manager . getRepository ( MyEntity ) . query ( ` SELECT COUNT(x) as total, MAX(y) as max FROM my_entity GROUP BY month ` ) return result } } More examples ​ Some great examples of @subsquid/graphql-server -based custom resolvers can be spotted in the wild in the Rubick repo by KodaDot . For more examples of resolvers, see TypeGraphQL examples repo . Logging ​ To keep logging consistent across the entire GraphQL server, use @subsquid/logger : import { createLogger } from '@subsquid/logger' // using a custom namespace ':my-resolver' for resolver logs const LOG = createLogger ( 'sqd:graphql-server:my-resolver' ) LOG . info ( 'created a dedicated logger for my-resolver' ) LOG here is a logger object identical to ctx.log interface-wise. Interaction with global settings ​ --max-response-size used for DoS protection is ignored in custom resolvers. Caching works on custom queries in exactly the same way as it does on the schema-derived queries. Troubleshooting ​ Reflect.getMetadata is not a function ​ Add import 'reflect-metadata' on top of your custom resolver module and install the package if necessary. Edit this page Previous Caching Next Subscriptions Examples Logging Interaction with global settings Troubleshooting
Step 2: Owners & tokens | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Step 1: Transfer events Step 2: Owners & tokens Step 3: External data Step 4: Optimization Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Indexing BAYC Step 2: Owners & tokens On this page Step 2: Deriving owners and tokens This is the second part of the tutorial in which we build a squid that indexes Bored Ape Yacht Club NFTs, their transfers, and owners from the Ethereum blockchain , fetches the metadata from IPFS and regular HTTP URLs, stores all the data in a database, and serves it over a GraphQL API. In the first part we created a simple squid that scraped Transfer events emitted by the BAYC token contract . Here, we go a step further and derive separate entities for the NFTs and their owners from the transfers. The new entities will reference the corresponding Transfer entities. It will be automatically translated into primary key-foreign key references in the new database schema, and enable efficient cross-entity GraphQL queries. Prerequisites: Node.js, Squid CLI , Docker, a project folder with the code from the first part ( this commit ). Writing schema.graphql ​ Start the process by adding new entities to the schema.graphql file: # any unique string can be used as id type Owner @entity { id : ID ! # owner address } type Token @entity { id : ID ! # string form of tokenId tokenId : BigInt ! } Next, add entity relations . Let us begin with adding a simple relation linking tokens to their owners: type Token @entity { id: ID! # string form of tokenId tokenId: BigInt! + owner: Owner! } Now, Token is considered an owning entity in relation to Owner . As a result, On the database side: the token table that maps to the Token entity gains a foreign key column owner_id holding primary keys of the owner table. The column is automatically indexed - no need to add @index . On the Typeorm side: the Token entity gains an owner field decorated with @ManyToOne . To create a well-formed Token entity instance in processor code, we now will have to first get a hold of an appropriate Owner entity instance and populate the owner field of a new Token with a reference to it: let newOwner : Owner = new Owner ( { id : '0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045' } ) let newToken : Token = new Token ( { id : '1' , tokenId : 1 , owner : newOwner // the whole entity instance } ) On the GraphQL side: queries to token can now select owner and any of its subfields ( id is the only one available now). Introduce more entity relations by replacing the from , to and tokenId fields of the Transfer entity with fields from the new entity types: type Transfer @entity { id: ID! - tokenId: BigInt! @index - from: String! @index - to: String! @index + token: Token! + from: Owner! + to: Owner! timestamp: DateTime! blockNumber: Int! txHash: String! @index } Lastly, include the virtual (i.e., not mapped to a column in the database schema) reverse lookup fields : type Owner @entity { id: ID! # owner address + ownedTokens: [Token!]! @derivedFrom(field: "owner") } type Token @entity { id: ID! # string form of tokenId tokenId: BigInt! + transfers: [Transfer!]! @derivedFrom(field: "token") } This addition doesn't create any new database columns, but it makes the ownedTokens and transfers fields accessible through GraphQL and Typeorm. You can find the final version of schema.graphql here . Once you're finished, regenerate the Typeorm entity code with the following command: npx squid-typeorm-codegen We also need to regenerate the database migrations to match the new schema. However, we'll postpone this step for now: it requires recompiling the squid code and it is not possible until we fix the entity creation code. Creating the entities ​ Note how the entities we define form an acyclic dependency graph: Owner entity instances can be made straight from the raw events data; Token s require the raw data plus the Owner s; Transfer entities require all the above. As a consequence, the creation of entity instances must proceed in a particular order . Squids usually use small graphs like this one, and in these the order can be easily found manually (e.g. Owner s then Token s then Transfer s in this case). We will assume that it can be hardcoded by the programmer. Further, at each step we will process the data for the whole batch instead of handling the items individually. This is crucial for achieving a good syncing performance. With all that in mind, let's create a batch processor that generates and persists all of our entities: src/main.ts import { Owner , Token } from './model' processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { let rawTransfers : RawTransfer [ ] = getRawTransfers ( ctx ) let owners : Map < string , Owner > = createOwners ( rawTransfers ) let tokens : Map < string , Token > = createTokens ( rawTransfers , owners ) let transfers : Transfer [ ] = createTransfers ( rawTransfers , owners , tokens ) await ctx . store . upsert ( [ ... owners . values ( ) ] ) await ctx . store . upsert ( [ ... tokens . values ( ) ] ) await ctx . store . insert ( transfers ) } ) where interface RawTransfer { id : string tokenId : bigint from : string to : string timestamp : Date blockNumber : number txHash : string } is an interface very similar to that of the Transfer entity as it was at the beginning of this part of the tutorial. This allows us to reuse most of the code of the old batch handler in getRawTransfers() : src/main.ts import { Context } from './processor' function getRawTransfers ( ctx : Context ) : RawTransfer [ ] { let transfers : RawTransfer [ ] = [ ] for ( let block of ctx . blocks ) { for ( let log of block . logs ) { if ( log . address === CONTRACT_ADDRESS && log . topics [ 0 ] === bayc . events . Transfer . topic ) { let { from , to , tokenId } = bayc . events . Transfer . decode ( log ) transfers . push ( { id : log . id , tokenId , from , to , timestamp : new Date ( block . header . timestamp ) , blockNumber : block . header . height , txHash : log . transactionHash , } ) } } } return transfers } The next step involves creating Owner entity instances. We will need these to create both Tokens and Transfers . In both scenarios, we'll have the IDs of the owners (i.e., their addresses) prepared. To simplify future lookups, we choose to return the Owner instances as a Map<string, Owner> : src/main.ts function createOwners ( rawTransfers : RawTransfer [ ] ) : Map < string , Owner > { let owners : Map < string , Owner > = new Map ( ) for ( let t of rawTransfers ) { owners . set ( t . from , new Owner ( { id : t . from } ) ) owners . set ( t . to , new Owner ( { id : t . to } ) ) } return owners } Similarly, Token instances will also need to be looked up later, so we return them as a Map<string, Token> . To identify the most recent owner of each token, we traverse all the transfers in the order they appear on the blockchain and assign the owner of any involved tokens to their recipient: src/main.ts function createTokens ( rawTransfers : RawTransfer [ ] , owners : Map < string , Owner > ) : Map < string , Token > { let tokens : Map < string , Token > = new Map ( ) for ( let t of rawTransfers ) { let tokenIdString = ` ${ t . tokenId } ` tokens . set ( tokenIdString , new Token ( { id : tokenIdString , tokenId : t . tokenId , owner : owners . get ( t . to ) } ) ) } return tokens } Some Token and Owner instances might have been created in previous batches, so we use ctx.store.upsert() to store these instances while updating any existing ones. info In some circumstances we might have had to retrieve the old entity instances from the database before updating, but here we have all the required fields populated, so we simply overwrite the whole entity with ctx.store.upsert() . Finally, we create an array of Transfer entity instances through a simple mapping: src/main.ts function createTransfers ( rawTransfers : RawTransfer [ ] , owners : Map < string , Owner > , tokens : Map < string , Token > ) : Transfer [ ] { return rawTransfers . map ( t => new Transfer ( { id : t . id , token : tokens . get ( ` ${ t . tokenId } ` ) , from : owners . get ( t . from ) , to : owners . get ( t . to ) , timestamp : t . timestamp , blockNumber : t . blockNumber , txHash : t . txHash } ) ) } Since Transfer s are unique, we can safely use ctx.store.insert() to persist them. At this point, the squid has accomplished everything planned for this part of the tutorial. The only remaining task is to drop and recreate the database (if it's running), then regenerate and apply the migrations: npm run build docker compose down docker compose up -d rm -r db/migrations npx squid-typeorm-migration generate npx squid-typeorm-migration apply Full code can be found at this commit . To test it, start the processor and the GraphQL server by running node -r dotenv/config lib/main.js and npx squid-graphql-server in separate terminals. Then, visit the GraphiQL playground : The new entities should be displayed in the query schema. Thanks to the added entity relations, we can now execute more complex nested queries. For example, the one displayed in the screenshot selects a transfer, retrieves its token, looks up its owner, and finds out which tokens are currently owned by them. Edit this page Previous Step 1: Transfer events Next Step 3: External data Writing schema.graphql Creating the entities
Classes for saving data | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks Classes for saving data üìÑÔ∏è typeorm-store Optimized TypeORM-based store üóÉÔ∏è file-store extensions 4 items üìÑÔ∏è bigquery-store @subsquid/bigquery-store reference Previous Fields selection Next typeorm-store
S3 support | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions CSV support Parquet support JSON support S3 support bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks file-store extensions S3 support On this page S3 destination support Overview ​ Writing to Amazon S3-compatible file storage services such as AWS and Filebase is supported via the S3Dest class from the @subsquid/file-store-s3 package. Use it by setting the dest field of the Database constructor argument to its instance. Constructor of S3Dest accepts the following arguments: url: string : S3 URL in the s3://bucket/path format. optionsOrClient?: S3Client | S3ClientConfig : an optional S3 client or client config . By default, a simple config parameterized by environment variables is used: { region : process . env . S3_REGION , endpoint : process . env . S3_ENDPOINT , credentials : { accessKeyId : assertNotNull ( process . env . S3_ACCESS_KEY_ID ) , secretAccessKey : assertNotNull ( process . env . S3_SECRET_ACCESS_KEY ) , } , } Example ​ This saves the processor data in the transfers-data folder of the subsquid-testing-bucket bucket at the Filebase service. The service only has one region and one endpoint, and here they are hardcoded to reduce the number of required envirionment variables and illustrate how connection parameters can be supplied programmatically. Full squid code is available in this repo . import { Database } from '@subsquid/file-store' import { S3Dest } from '@subsquid/file-store-s3' import { assertNotNull } from '@subsquid/util-internal' // pulled by @subsquid/file-store-s3 ... const dbOptions = { ... dest : new S3Dest ( 's3://subsquid-testing-bucket/transfers-data' , { region : 'us-east-1' , endpoint : 'https://s3.filebase.com' , credentials : { accessKeyId : assertNotNull ( process . env . S3_ACCESS_KEY_ID ) , secretAccessKey : assertNotNull ( process . env . S3_SECRET_ACCESS_KEY ) } } ) , ... } processor . run ( new Database ( dbOptions ) , async ( ctx ) => { ... } Edit this page Previous JSON support Next bigquery-store Overview Example
Intro | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Intro Intro info At the moment, Squid SDK GraphQL server can only be used with squids that use Postgres as their target database. GraphQL is an API query language, and a server-side runtime for executing queries using a custom type system. Head over to the official documentation website for more info. A GraphQL API served by the GraphQL server has two components: Core API is defined by the schema file . Extensions added via custom resolvers . In this section we cover the core GraphQL API, with short explanations on how to perform GraphQL queries, how to paginate and sort results. This functionality is supported via OpenReader , SQD's own implementation of OpenCRUD . Edit this page Previous Core API Next Entity queries
SQD Network API documentation for Tron | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK Network API Solana API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing Network API SQD Network API documentation for Tron üìÑÔ∏è Solana API Access the data of Solana blockchain Previous Typegen Next Solana API
Environment variables | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Environment variables On this page Environment variables info Store all your sensitive inputs (API keys, passwords etc) as secrets . SQD Cloud supports adding environment variables to squid deployments. The variables can be defined as key-value pairs at any of the env: sections of the manifest . For example, here is how to add a variable that will be visible only to the squid processor : squid.yaml deploy : processor : env : MY_PROCESSOR_VAR : string_value You can also add variables visible only to the GraphQL server or to the migration script . There is also an option to add variables for all services : squid.yaml deploy : env : MY_SQUIDWIDE_VAR : string_value Variables can be assigned either to strings, or to member variables of contexts provided by the service. For example, to make a processor-scoped API_KEY variable and populate it with the value of secrets.API_KEY , do this: squid.yaml deploy : processor : env : RPC_ENDPOINT : $ { { secrets.API_KEY } } Variable shadowing ​ There is one special case in which the variables defined in the manifest will get overwritten by the Cloud: database connection settings are shadowed by the system-defined values whenever the postgres addon is enabled (see the Variable shadowing section of the addon page). For example, in the snippet below all the DB_* variable definitions will be ignored: squid.yaml deploy : addons : postgres : env : DB_HOST : mypostgreshost.xyz DB_PORT : 5432 DB_NAME : squid - tests DB_USER : me DB_PASS : $ { { secrets.DATABASE_PASSWORD } } DB_SSL : true Contexts ​ The Cloud exposes some useful variables via a mechanism identical to GitHub Actions contexts . Namely, any string ${{ <context> }} added to the manifest at any environment variable definition gets replaced by the value supplied by the Cloud. Secrets ​ Secrets are designed to store sensitive data, such as API keys or private URLs for chain RPC endpoints. They are defined at the organization level and are exposed to all organization squids that request them in their environment variable definitions. To add a secret: Create it in the Cloud. You can do it at the secrets page or with sqd secrets : sqd secrets set MOONRIVER_GRPC_ENDPOINT wss://moonriver.my-endpoint.com/ws/my-secret-key If you do not specify the value, sqd will attempt to read it from standard input. This is useful when setting a value to the contents of some file: sqd secrets set MY_JSON_CREDENTIALS < creds.json At your squid's manifest , add an environment variable and assign it to the secret: deploy : env : RPC_ENDPOINT : $ { { secrets.MOONRIVER_GRPC_ENDPOINT } } Note: a deployment requesting a secret unknown to the Cloud will fail . Access the value in the squid with process.env , e.g. const processor = new EvmBatchProcessor ( ) . setRpcEndpoint ( { url : process . env . RPC_ENDPOINT , rateLimit : 1000rps } ) Inspect, remove and update the secrets using the sqd secrets command. info Any changes to secrets will take effect only when the squid is restarted, e.g. with sqd deploy . Edit this page Previous Best practices Next Inspect logs Variable shadowing Contexts Secrets
Block data for Fuel Network | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource Block data for Fuel Network Block data for Fuel Network In Fuel Squid SDK, the data is processed by repeatedly calling the user-defined batch handler function on batches of on-chain data. The sole argument of the batch handler is its context ctx , and ctx.blocks is an array of Block objects containing the data to be processed, aligned at the block level. For Fuel DataSource the Block interface is defined as follows: export interface Block { header : BlockHeader ; transactions : Transaction [ ] ; inputs : TransactionInput [ ] ; outputs : TransactionOutput [ ] ; receipts : Receipt [ ] ; } Block.header contains the block header data. The rest of the fields are iterables containing the four kinds of blockchain data. The items within each iterable are ordered in the same way as they are within blocks. The exact fields available in each data item type are inferred from the setFields() call argument. The method is documented on the field selection page: Input section ; Transaction section ; Output section ; Receipt section . Edit this page Previous FuelDataSource Next General settings
Cheatsheet | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing Cheatsheet On this page CLI cheatsheet This guide provides a quick reference to the commands needed to launch the squid and manage the database. Install dependencies ​ npm i Compile the project ​ npx tsc Launch Postgres database to store the data ​ docker compose up -d Apply database migrations to create the target schema ​ npx squid-typeorm-migration apply Run indexer ​ node -r dotenv/config lib/main.js Check out the indexed swaps ​ docker exec "$(basename " $( pwd ) " ) -db-1 " psql -U postgres \ -c " SELECT id, logs_count, found_at FROM contract ORDER BY logs_count desc LIMIT 10 " You can use the sqd utility to shorten these commands and manage their interrelations. Learn more here . Edit this page Previous Field selection Next Network API Install dependencies Compile the project Launch Postgres database to store the data Apply database migrations to create the target schema Run indexer Check out the indexed swaps
Outputs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource Outputs On this page Outputs addOutput(options) ​ Get some or all outputs on the network. options has the following structure: { // data requests type ? : OutputType [ ] // related data retrieval transaction ? : boolean range ? : { from : number to ? : number } } Data requests: type sets the type of the output. Output type has the following options: 'CoinOutput' | 'ContractOutput' | 'ChangeOutput' | 'VariableOutput' | 'ContractCreated' . Leave it undefined to subscribe to all outputs. Enabling the transaction flag will cause the processor to retrieve transactions where the selected outputs have occurred. The data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/fuel-objects on the block data to populate the convenience reference fields like output.transaction . Note that receipts can also be requested by the other FuelDataSource methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Examples ​ Request all outputs with ChangeOutput type and include transactions: processor . addOutput ( { type : [ "ChangeOutput" ] , transaction : true , } ) . build ( ) ; Edit this page Previous Inputs Next Receipts Examples
Self-hosting | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Self-hosting On this page Self-hosting tip Many tips from our best practices guide apply to self-hosted squids, too. Check it out if you intend to use your squid in production. To deploy a squid locally or on-premises, use the following Dockerfile template to build a single image for both api and processor services: Dockerfile FROM node:20-alpine AS node FROM node AS node-with-gyp FROM node-with-gyp AS builder WORKDIR /squid ADD package.json . ADD package-lock.json . # remove if needed ADD assets assets # remove if needed ADD db db # remove if needed ADD schema.graphql . RUN npm ci ADD tsconfig.json . ADD src src RUN npm run build FROM node-with-gyp AS deps WORKDIR /squid ADD package.json . ADD package-lock.json . RUN npm ci --production FROM node AS squid WORKDIR /squid COPY --from = deps /squid/package.json . COPY --from = deps /squid/package-lock.json . COPY --from = deps /squid/node_modules node_modules COPY --from = builder /squid/lib lib # remove if no assets folder COPY --from = builder /squid/assets assets # remove if no db folder COPY --from = builder /squid/db db # remove if no schema.graphql is in the root COPY --from = builder /squid/schema.graphql schema.graphql # remove if no commands.json is in the root ADD commands.json . RUN echo -e "loglevel=silent\\nupdate-notifier=false" > /squid/.npmrc RUN npm i -g @subsquid/commands && mv $(which squid-commands) /usr/local/bin/sqd ENV PROCESSOR_PROMETHEUS_PORT 3000 Then build an image with docker buildx build . -t my-squid or with docker build . -t my-squid if you're using an older Docker version. Sample compose file â€‹ Once can the run the squid services with the freshly built image. Here is a sample docker-compose file: services : db : image : postgres : 15 environment : POSTGRES_DB : squid POSTGRES_PASSWORD : postgres healthcheck : test : [ "CMD-SHELL" , "pg_isready" , "-d" , "squid" ] interval : 5s timeout : 5s retries : 5 # Uncomment for logging all SQL statements # command: ["postgres", "-c", "log_statement=all"] api : image : my - squid environment : - DB_NAME=squid - DB_PORT=5432 - DB_HOST=db - DB_PASS=postgres - GQL_PORT=4350 ports : # GraphQL endpoint at port 4350 - "4350:4350" command : [ "sqd" , "serve:prod" ] depends_on : db : condition : service_healthy processor : image : my - squid environment : - DB_NAME=squid - DB_PORT=5432 - DB_HOST=db - DB_PASS=postgres # any other variables that your squid processor may be using ports : # prometheus metrics exposed at port 3000 - "3000:3000" command : [ "sqd" , "process:prod" ] depends_on : db : condition : service_healthy Note that sqd serve:prod and sqd process:prod commands are defined in the commands.json file . Edit this page Previous Serving GraphQL Next Persisting data Sample compose file
sqd explorer | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd explorer sqd explorer danger sqd explorer is disabled in @subsquid/cli>=3.0.0 . If you've been using it, please let us know in the SquidDevs Telegram channel . Visual explorer of deployed squids Left pane: List of deployed squids.
Right pane: details of the selected squid. Navigate by pressing the corresponding number (e.g. 1 for Summary). Summary: endpoint URL, sync status, DB storage utilization Logs DB access details This command requires specifying an organization with the -o/--org flag when invoked by accounts with more than one organization. SQD Cloud users with just one organization can omit this flag. See code: src/commands/explorer.ts Edit this page Previous sqd deploy Next sqd gateways
Starknet API | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Public gateways EVM API Substrate API Starknet API Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Reference Starknet API On this page Starknet SQD Network API warning The Starknet API of SQD Network is currently in beta. Breaking changes may be introduced in the future releases. SQD Network API distributes the requests over a ( potentially decentralized ) network of workers . The main gateway URL points at a router that provides URLs of workers that do the heavy lifting. Each worker has its own range of blocks on each dataset it serves. Suppose you want to retrieve an output of some query on a block range starting at firstBlock (can be the genesis block) and ending at the highest available block. Proceed as follows: Retrieve the dataset height from the router with GET /height and make sure it's above firstBlock . Save the value of firstBlock to some variable, say currentBlock . Query the router for an URL of a worker that has the data for currentBlock with GET /${currentBlock}/worker . Retrieve the data from the worker by posting the query ( POST / ), setting the "fromBlock" query field to ${currentBlock} . Parse the retrieved data to get a batch of query data plus the height of the last block available from the current worker. Take the header.number field of the last element of the retrieved JSON array - it is the height you want. Even if your query returns no data, you'll still get the block data for the last block in the range, so this procedure is safe. Set currentBlock to the height from the previous step plus one . Repeat steps 3-6 until all the required data is retrieved. The main URL of the Starknet gateway is https://v2.archive.subsquid.io/network/starknet-mainnet warning Unlike in the explorer , addresses in this API do not have leading zeros (both in valid requests and in the returned data). For example, explorer's 0x00ce6c...0552 becomes 0xce6c...0552 . This is the format that Starknet RPC nodes use. Implementation examples: Manually with cURL Suppose we want data on all txs sent by Layerswap / 0x19252b1deef483477c4d30cfcc3e5ed9c82fafea44669c182a45a01b4fdb97a starting from block 600_000. The steps are: Verify that the dataset has reached the required height: curl https://v2.archive.subsquid.io/network/starknet-mainnet/height Output 632494 Remember that your current height is 600000. Get a worker URL for the current height: curl https://v2.archive.subsquid.io/network/starknet-mainnet/600000/worker Output https://rb03.sqd-archive.net/worker/query/czM6Ly9zdGFya25ldC1tYWlubmV0 Retrieve the data from the current worker curl https://rb03.sqd-archive.net/worker/query/czM6Ly9zdGFya25ldC1tYWlubmV0 \ -X 'POST' -H 'content-type: application/json' -H 'accept: application/json' \ -d '{ "type": "starknet", "fromBlock":600000, "toBlock":632494, "fields":{"transaction":{"transactionHash":true}}, "transactions":[{"senderAddress":["0x19252b1deef483477c4d30cfcc3e5ed9c82fafea44669c182a45a01b4fdb97a"]}] }' | jq Output: [ { "header" : { "number" : 600000 , "hash" : "0x898fe7f61f5d662199d223de496988f221d150ed054f2fe5e681b2988b9e2c" } , "transactions" : [ ] } , { "header" : { "number" : 600007 , "hash" : "0x44aa251cee1baaf3f19accefd223ce5208815686c881bf645ffb3e3348a5ddc" } , "transactions" : [ { "transactionIndex" : 24 , "transactionHash" : "0x6a88edb0713769de4ad4d450df70911c3e9a7e8253c135c9574d0b3542ced18" } ] } , ... { "header" : { "number" : 617979 , "hash" : "0x7f6a8516a91eefa6a65972c47002cbe3851e3c4287f670c914850960d29ca29" } , "transactions" : [ ] } ] Parse the retrieved data: Grab the network data you requested from the list items with non-empty data fields ( transactions , events ). For the example above, this data will include the txn 0x6a88... . Observe that we received the data up to and including block 617979. To get the rest of the data, update the current height to 617980 and go to step 3. Note how the worker URL you're getting while repeating step 3 points to a different host than before. This is how data storage and reads are distributed across the SQD Network. Repeat steps 3 through 6 until the dataset height of 632494 is reached. In Python def get_text ( url : str ) - > str : res = requests . get ( url ) res . raise_for_status ( ) return res . text def dump ( gateway_url : str , query : Query , first_block : int , last_block : int ) - > None : assert 0 <= first_block <= last_block query = dict ( query ) # copy query to mess with it later dataset_height = int ( get_text ( f' { gateway_url } /height' ) ) next_block = first_block last_block = min ( last_block , dataset_height ) while next_block <= last_block : worker_url = get_text ( f' { gateway_url } / { next_block } /worker' ) query [ 'fromBlock' ] = next_block query [ 'toBlock' ] = last_block res = requests . post ( worker_url , json = query ) res . raise_for_status ( ) blocks = res . json ( ) last_processed_block = blocks [ - 1 ] [ 'header' ] [ 'number' ] next_block = last_processed_block + 1 for block in blocks : print ( json . dumps ( block ) ) Full code here . Router API ​ GET /height (get height of the dataset) Example response: 632494 . GET ${firstBlock}/worker (get a suitable worker URL) The returned worker is capable of processing POST / requests in which the "fromBlock" field is equal to ${firstBlock} . Example response: https://rb06.sqd-archive.net/worker/query/czM6Ly9zdGFya25ldC1tYWlubmV0 . Worker API ​ POST / (query transactions and events) Query Fields ​ type : "starknet" . fromBlock : Block number to start from (inclusive). toBlock : (optional) Block number to end on (inclusive). If this is not given, the query will go on for a fixed amount of time or until it reaches the height of the dataset. includeAllBlocks : (optional) If true, the Network will include blocks that contain no data selected by data requests into its response. fields : (optional) A selector of data fields to retrieve. Common for all data items. transactions : (optional) A list of transaction requests . An empty list requests no data. events : (optional) A list of event requests . An empty list requests no data. The response is a JSON array of per-block data items that covers a block range starting from fromBlock . The last block of the range is determined by the worker. You can find it by looking at the header.number field of the last element in the response array. The first and the last block in the range are returned even if all data requests return no data for the range. In most cases the returned range will not contain all the range requested by the user (i.e. the last block of the range will not be toBlock ). To continue, retrieve a new worker URL for blocks starting at the end of the current range plus one block and repeat the query with an updated value of fromBlock . Example Request ​ { "type" : "starknet" , "fromBlock" : 632000 , "toBlock" : 632494 , "fields" : { "block" : { "timestamp" : true } , "event" : { "keys" : true , "data" : true } , "transaction" : { "transactionHash" : true } } , "events" : [ { "fromAddress" : [ "0x19252b1deef483477c4d30cfcc3e5ed9c82fafea44669c182a45a01b4fdb97a" ] , "transaction" : true } ] } Example Response ​ [ { "header" : { "number" : 632000 , "hash" : "0xdfebe2b6af20dfe7f27d5fe8b1b4e8ee48ad812ce9bfd9c756c9db7dbcdb22" , "timestamp" : 1712950160 } , "transactions" : [ { "transactionIndex" : 110 , "transactionHash" : "0x151fa3c8633e6ed71301af4afc8f73a141ef39cca1d51d0f72d66a11911e2f3" } , { "transactionIndex" : 306 , "transactionHash" : "0x7e1c307624c5c78e311e2a08f0355dfef80e6fc6ed47c64ceda757e044f2c85" } ] , "events" : [ { "transactionIndex" : 110 , "eventIndex" : 1 , "keys" : [ "0x1dcde06aabdbca2f80aa51392b345d7549d7757aa855f7e37f5d335ac8243b1" , "0x151fa3c8633e6ed71301af4afc8f73a141ef39cca1d51d0f72d66a11911e2f3" ] , "data" : [ "0x1" , "0x1" , "0x1" ] } , { "transactionIndex" : 306 , "eventIndex" : 1 , "keys" : [ "0x1dcde06aabdbca2f80aa51392b345d7549d7757aa855f7e37f5d335ac8243b1" , "0x7e1c307624c5c78e311e2a08f0355dfef80e6fc6ed47c64ceda757e044f2c85" ] , "data" : [ "0x1" , "0x1" , "0x1" ] } ] } , ... { "header" : { "number" : 632492 , "hash" : "0x440ee029f2a970b2546eb39ab23075659ec8e0246c94f62d21e21f912dfb58d" , "timestamp" : 1713049189 } , "transactions" : [ { "transactionIndex" : 125 , "transactionHash" : "0x61bd3d233cfe9cb387f3b016127ffb0d66d265c2593f80a317404e2f3c334bb" } ] , "events" : [ { "transactionIndex" : 125 , "eventIndex" : 1 , "keys" : [ "0x1dcde06aabdbca2f80aa51392b345d7549d7757aa855f7e37f5d335ac8243b1" , "0x61bd3d233cfe9cb387f3b016127ffb0d66d265c2593f80a317404e2f3c334bb" ] , "data" : [ "0x1" , "0x1" , "0x1" ] } ] } , { "header" : { "number" : 632494 , "hash" : "0x2782c5ca3f1d3eb2e4c085fc17908b9b86bfe91807cd452374bcb40b2245925" , "timestamp" : 1713049569 } , "transactions" : [ ] , "events" : [ ] } ] Data requests ​ Transactions ​ { contractAddress : string [ ] , senderAddress : string [ ] , type : string [ ] , firstNonce : int , lastNonce : int , events : boolean } A transaction will be included in the response if it matches all the requests. An empty array matches no transactions; omit all requests to match all transactions. All events emitted by the selected transactions will be included into the response if events is set to true . See Data fields selector for info on field selection. Events ​ { fromAddress : string [ ] , key0 : string [ ] , key1 : string [ ] , key2 : string [ ] , key3 : string [ ] , transaction : boolean } An event will be included in the response if it matches all the requests. An empty array matches no events; omit all requests to match all events. If transaction is set to true , all parent transactions will be included into the response. See Data fields selector for info on field selection. Data fields selector ​ A selector of fields for the returned data items. Its structure is as follows: { block:               // field selector for blocks transaction:         // field selector for transactions event:               // field selector for events } Block fields ​ { parentHash status newRoot timestamp sequencerAddress } A valid field selector for blocks is a JSON that has a subset of these fields as keys and true as values, e.g. {"status": true, "timestamp": true} . Transaction fields ​ { transactionHash contractAddress entryPointSelector calldata maxFee type senderAddress version signature nonce classHash compiledClassHash contractAddressSalt constructorCalldata } A valid field selector for transactions is a JSON that has a subset of these fields as keys and true as values, e.g. {"transactionHash": true, "type": true, "calldata": true} . Event fields ​ { fromAddress keys data } A valid field selector for logs is a JSON that has a subset of these fields as keys and true as values, e.g. {"fromAddress": true, "data": true} . Edit this page Previous Substrate API Next Portal beta info Router API Worker API Data requests Transactions Events Data fields selector Block fields Transaction fields Event fields
SQD Cloud resources | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources SQD Cloud resources 📄️ Best practices Checklist for going to production 📄️ Environment variables Variables, contexts, secrets 📄️ Inspect logs Inspect the deployment logs 📄️ Monitoring Prometheus endpoints for squid services 📄️ Organizations Grouping squids 📄️ Slots and tags Manage multiple squid deployments 📄️ Query optimization Ensuring your squid's perfomance 📄️ RPC addon Built-in RPC endpoints 📄️ Portal for EVM+Substrate EVM/Substrate data from the permissionless SQD Network 📄️ Migrate to the Cloud portal For users on Solana 📄️ production-alias Production aliasing feature is deprecated in @subsquid/cli>=3.0.0. Use Slots and tags instead. Previous Troubleshooting Next Best practices
Organizations | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Best practices Environment variables Inspect logs Monitoring Organizations Slots and tags Query optimization RPC addon Portal for EVM+Substrate Migrate to the Cloud portal production-alias Reference Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Resources Organizations On this page Organizations All squids within SQD Cloud are grouped into organizations . This additional layer of hierarchy enables team access and per-team billing. It also prevents unnecessary data sharing, as environment variables are separate between organizations. To create an organization, click on the organizations dropdown menu in the upper left corner of the Cloud homepage and select "Create new organization". Playgrounds ​ A playground organization is created for each account on its first login. There, you can deploy one squid for development or prototyping, free of charge. Playground squids cannot be used in production, as they are collocated and run on spot VMs . Expect 3-5 minutes of downtime once every few days. Other limitations include: squid manifests with the scale: section are forbidden; 10 GB of database storage; 500k monthly requests to the built-in RPC service . Unlike other organizations, playgrounds cannot be shared or billed. Draft organizations ​ Freshly created organizations are marked as drafts until upgraded to Professional status. It is not possible to deploy squids to draft organizations, but you can invite other users into them and set environment variables . Professional organizations ​ Adding a valid payment method promotes an organization to Professional status. Visit the billing page , select your organization in the dropdown menu in the top left corner and follow the upgrade instructions: Once an organization is upgraded you can deploy as many squids as you requre. scale: section is now unlocked: use it to request any resources suitable for your use case. Your organization will be billed according to our pricing schedule . Working with organizations ​ When your account has access to more than one organization, it is necessary to specify one when listing or deploying (with some exceptions) your squids, as well as when setting secrets . Do it with the --org/-o flag: sqd secrets ls -o my-organization sqd secrets rm SECRET --org my-organization sqd secrets set SECRET --org my-organization sqd ls -o my-organization sqd deploy . -o my-organization If you omit the flag, sqd will ask you to choose an organization interactively. Edit this page Previous Monitoring Next Slots and tags Playgrounds Draft organizations Professional organizations Working with organizations
Factory contracts | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Factory contracts Proxy contracts Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides EVM-specific Factory contracts On this page Factory contracts In some cases the set of contracts to be indexed by the squid is not known in advance. For example, a DEX contract typically creates a new contract for each trading pair added, and each such trading contract is of interest. While the set of handler subscriptions is static and defined at the processor creation, one can leverage wildcard subscriptions and filter for contracts of interest at runtime. Let's consider how it works in a DEX example, with a contract emitting PoolCreated log when a new pool contract is created by the main contract. Full code is available in the examples repo . src/processor.ts export const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setBlockRange ( { from : 12_369_621 , } ) . setFields ( { log : { topics : true , data : true , } , transaction : { hash : true , } , } ) . addLog ( { address : [ FACTORY_ADDRESS ] , topic0 : [ factoryAbi . events . PoolCreated . topic ] , } ) . addLog ( { topic0 : [ poolAbi . events . Swap . topic ] , transaction : true , } ) src/main.ts let factoryPools : Set < string > processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { if ( ! factoryPools ) { factoryPools = await ctx . store . findBy ( Pool , { } ) . then ( ( q ) => new Set ( q . map ( ( i ) => i . id ) ) ) } let pools : PoolData [ ] = [ ] let swaps : SwapEvent [ ] = [ ] for ( let block of ctx . blocks ) { for ( let log of block . logs ) { if ( log . address === FACTORY_ADDRESS ) { pools . push ( getPoolData ( ctx , log ) ) } else if ( factoryPools . has ( log . address ) ) { swaps . push ( getSwap ( ctx , log ) ) } } } await createPools ( ctx , pools ) await processSwaps ( ctx , swaps ) } ) Two-pass indexing for factory contracts â€‹ Squids built with the pattern shown above get the job done, but retrieve a lot of data that ends up discarded in the process. Complete elimination of this overhead would require dynamically changing the processor configuration, which is not currently possible. However, the configuration can be changed at a fixed block and that can be used to eliminate most of the overhead, drastically reducing the sync time. The technique has a couple of limitations: The number of newly deployed contracts should be moderate (roughly up to tens of thousands). If your factory contract deploys contracts by millions (e.g. Pancakeswap), then vanilla factory pattern will be faster. You will need to periodically perform an extra action to keep the syncing overhead of your squid to a minimum. The idea is to retrieve the list of the contracts that the factory deploys up to a certain block before the main sync starts . Then all data of interest up to that block can be requested only for these contracts. Once that data is retrieved, the contract can switch back to retrieving the data chain-wide and filtering it in processor. The example above can be changed to: src/processor.ts const { preloadHeight , preloadedPools } = loadPools ( ) // e.g. from a filesystem export const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<eth_rpc_endpoint_url>' ) . setBlockRange ( { from : 12_369_621 , } ) . setFields ( { log : { topics : true , data : true , } , transaction : { hash : true , } , } ) . addLog ( { address : [ FACTORY_ADDRESS ] , topic0 : [ factoryAbi . events . PoolCreated . topic ] , } ) . addLog ( { range : { from : 12_369_621 , to : preloadHeight , } , address : preloadedPools , topic0 : [ poolAbi . events . Swap . topic ] , transaction : true , } ) . addLog ( { range : { from : preloadHeight + 1 , } , topic0 : [ poolAbi . events . Swap . topic ] , transaction : true , } ) The list of deployments can be preloaded with a small auxiliary squid and stored e.g. in ./assets . This squid should be re-ran every time the number of blocks for which the whole network data is retrieved (that is, preloadedHeight+1 to current head) becomes unacceptably large. This approach is implemented in the squid indexing the thena.fi decentralized exchange . Edit this page Previous EVM-specific Next Proxy contracts Two-pass indexing for factory contracts
Nested field queries | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Nested field queries Nested field queries With OpenReader, fields of an Entity that contain fields themselves are shown as nested fields and it is possible to filter these as well. GraphQL queries can traverse related objects and their fields, letting clients fetch lots of related data in one request, instead of making several roundtrips as one would need in a classic REST architecture. As an example, this query searches for all accounts whose balance is bigger than a threshold value, fetching the id and balance simple fields, as well as the historicalBalances nested field . query { accounts ( orderBy : balance_ASC , where : { balance_gte : "250000000000000000" } ) { id balance historicalBalances { balance date id } } } A nested field is a list (one account can have multiple historicalBalances ) of objects with fields of their own. These objects can be filtered, too. In the following query the historicalBalances are filtered in order to only return the balances created after a certain date: query { accounts ( orderBy : balance_ASC , where : { balance_gte : "250000000000000000" } ) { id balance historicalBalances ( where : { date_lte : "2020-10-31T11:59:59.000Z" } , orderBy : balance_DESC ) { balance date id } } } Note that the newer and more advanced {entityName}sConnection queries support exactly the same format of the where argument as the older {entityName}s queries used in the examples provided here. Edit this page Previous AND/OR filters Next Cross-relation queries
Pagination | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API Pagination On this page Paginate query results There are multiple ways to obtain this behavior, let's take a look at a couple of them. Cursor based pagination ​ Cursors are used to traverse across entities of an entity set. They work by returning a pointer ("cursor") to a specific entity which can then be used to fetch the next batch. The batch will start with the entity after the one the cursor points to. For cursor-based pagination, OpenReader follows the Relay Cursor Connections spec . Currently, only forward pagination is supported. If your use case requires bidirectional pagination please let us know at our Telegram channel . In SQD GraphQL server, cursor based pagination is implemented with {entityName}sConnection queries available for every entity in the input schema. These queries require an explicitly supplied orderBy argument , and the field that is used for ordering must also be requested by the query itself . Check out this section for a valid query template. Example: this query fetches a list of videos where isExplicit is true and gets their count. query { videosConnection ( orderBy : id_ASC , where : { isExplicit_eq : true } ) { totalCount edges { node { id title } } } } Operator first ​ The first operator is used to fetch a specified number of entities from the beginning of the output. Example: Fetch the first 5 videos: query Query1 { videosConnection ( orderBy : id_ASC , first : 5 ) { edges { node { id title } } } } PageInfo object ​ PageInfo is a "virtual" entity that can be requested from any {entityName}sConnection query (see below). It returns the relevant cursors and some page information: pageInfo { startCursor endCursor hasNextPage hasPreviousPage } Operator after ​ Example: Fetch the first 10 channels, ordered by createdAt . Then, in a second query, fetch the next 10 channels: query FirstBatchQ { channelsConnection ( first : 10 , orderBy : createdAt_ASC ) { pageInfo { endCursor hasNextPage } edges { node { id handle createdAt } } } } query SecondBatchQ { channelsConnection ( after : < endCursor > , orderBy : createdAt_ASC ) { pageInfo { endCursor hasNextPage } edges { node { id handle createdAt } } } } Important Note on orderBy ​ The field chosen to orderBy needs to be present in the query itself. For example, any after query must follow this template: query QueryName { < entityName > sConnection ( after : < endCursor > , orderBy : < fieldNameToOrderBy > _ASC ) { pageInfo { endCursor hasNextPage ... < any other page info fields > ... } edges { node { < fieldNameToOrderBy > ... < any other fields of interest > ... } } } } Otherwise, the returned result wouldn't be ordered correctly. Examples ​ An interactive example of using cursor-based pagination can be found in this repo . Paginating with {entityName}s queries ​ Arguments limit and offset ​ In a list of entities returned by a query, the limit argument specifies how many should be retained, while the offset argument specifies how many should be skipped first. Default values are 50 for limit and 0 for offset . Limit results ​ Example: Fetch the first 5 channels: query { channels ( limit : 5 ) { id handle } } Limit results from an offset ​ Example: Fetch 5 channels from the list of all channels, starting with the 6th one: query { channels ( limit : 5 , offset : 5 ) { id handle } } Edit this page Previous JSON queries Next Sorting Cursor based pagination Operator first PageInfo object Operator after Important Note on orderBy Examples Paginating with {entityName}s queries Arguments limit and offset Limit results Limit results from an offset
Step 3: External data | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Step 1: Transfer events Step 2: Owners & tokens Step 3: External data Step 4: Optimization Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Indexing BAYC Step 3: External data On this page Step 3: Adding external data This is the third part of the tutorial in which we build a squid that indexes Bored Ape Yacht Club NFTs, their transfers and owners from the Ethereum blockchain , fetches the metadata from IPFS and regular HTTP URLs, stores all the data in a database and serves it over a GraphQL API. In the first two parts of the tutorial ( 1 , 2 ) we created a squid that scraped Transfer events emitted by the BAYC token contract and derived some information on tokens and their owners from that data. In this part we enrich token data with information obtained from contract state calls, IPFS and regular HTTP URLs. Prerequisites: Node.js, Squid CLI , Docker, a project folder with the code from the second part ( this commit ). Exploring token metadata ​ Now that we have a record for each BAYC NFT, let's explore how we can retrieve more data for each token. EIP-721 suggests that token metadata contracts may make token data available in a JSON referred to by the output of the tokenURI() contract function. Upon examining src/abi/bayc.ts , we find that the BAYC token contract implements this function. Also, the public ABI has no obvious contract methods that may set token URI or events that may be emitted on its change. In other words, it appears that the only way to retrieve this data is by querying the contract state . This requires a RPC endpoint of an archival Ethereum node, but we do not need to add one here: processor will reuse the endpoint we supplied in part one of the tutorial for use in RPC ingestion . The next step is to prepare for retrieving and parsing the metadata proper. For this, we need to understand the protocols used in the URIs and the structure of metadata JSONs. To learn that, we retrieve and inspect some URIs ahead of the main squid sync. The most straightforward way to achieve this is by adding the following to the batch handler: src/main.ts processor.run(new TypeormDatabase(), async (ctx) => { let tokens: Map<string, Token> = createTokens(rawTransfers, owners) let transfers: Transfer[] = createTransfers(rawTransfers, owners, tokens) + + let lastBatchBlockHeader = ctx.blocks[ctx.blocks.length-1].header + let contract = new bayc.Contract(ctx, lastBatchBlockHeader, CONTRACT_ADDRESS) + for (let t of tokens.values()) { + const uri = await contract.tokenURI(t.tokenId) + ctx.log.info(`Token ${t.id} has metadata at "${uri}"`) + } await ctx.store.upsert([...owners.values()]) await ctx.store.upsert([...tokens.values()]) await ctx.store.insert(transfers) }) Here, we utilize an instance of the Contract class provided by the src/abi/bayc.ts module. It uses an RPC endpoint supplied by the processor via ctx to call methods of contract CONTRACT_ADDRESS at the height corresponding to the last block of each batch. Once we have the Contract instance, we call tokenURI() for each token mentioned in the batch and print the retrieved URI. This simple approach is rather slow: the modified squid needs about three to eight hours to get a reasonably sized sample of URIs (figure out of date). A faster yet more complex alternative will be discussed in the next part of the tutorial. Running the modified squid reveals that some metadata URIs point to HTTPS and some point to IPFS. Here is one of the metadata JSONs: { "image" : "https://ipfs.io/ipfs/QmRRPWG96cmgTn2qSzjwr2qvfNEuhunv6FNeMFGa9bx6mQ" , "attributes" : [ { "trait_type" : "Fur" , "value" : "Robot" } , { "trait_type" : "Eyes" , "value" : "X Eyes" } , { "trait_type" : "Background" , "value" : "Orange" } , { "trait_type" : "Earring" , "value" : "Silver Hoop" } , { "trait_type" : "Mouth" , "value" : "Discomfort" } , { "trait_type" : "Clothes" , "value" : "Striped Tee" } ] } Note how it does not conform to the ERC721 Metadata JSON Schema . Summary of our findings: BAYC metadata URIs can point to HTTPS or IPFS -- we need to be able to retrieve both. Metadata JSONs have two fields: "image" , a string, and "attributes" , an array of pairs {"trait_type": string, "value": string} . Once finished, roll back the exploratory code: processor.run(new TypeormDatabase(), async (ctx) => { let tokens: Map<string, Token> = createTokens(rawTransfers, owners) let transfers: Transfer[] = createTransfers(rawTransfers, owners, tokens) - - let lastBatchBlockHeader = ctx.blocks[ctx.blocks.length-1].header - let contract = new bayc.Contract(ctx, lastBatchBlockHeader, CONTRACT_ADDRESS) - for (let t of tokens.values()) { - const uri = await contract.tokenURI(t.tokenId) - ctx.log.info(`Token ${t.id} has metadata at "${uri}"`) - } Extending the Token entity ​ We will save both image and attributes metadata fields and the metadata URI to the database. To do this, we need to add some new fields to the existing Token entity: type Token @entity { id: ID! # string form of tokenId tokenId: Int! owner: Owner! + uri: String! + image: String + attributes: [Attribute!] transfers: [Transfer!]! @derivedFrom(field: "token") } + + type Attribute { + traitType: String! + value: String! + } Here, Attribute is a non-entity type that we use to type the attributes field. Once schema.graphql is updated, we regenerate the TypeORM data model code:: npx squid-typeorm-codegen To populate the new fields, let us add an extra step at the end of createTokens() : async function createTokens ( ctx : Context , rawTransfers : RawTransfer [ ] , owners : Map < string , Owner > ) : Promise < Map < string , Token >> { let tokens : Map < string , PartialToken > = new Map ( ) for ( let t of rawTransfers ) { let tokenIdString = ` ${ t . tokenId } ` let ptoken : PartialToken = { id : tokenIdString , tokenId : t . tokenId , owner : owners . get ( t . to ) ! } tokens . set ( tokenIdString , ptoken ) } return await completeTokens ( ctx , tokens ) } interface PartialToken { id : string tokenId : bigint owner : Owner } Here, PartialToken stores the incomplete Token information obtained purely from blockchain events and function calls, before any state queries or enrichment with external data .
The function completeTokens() is responsible for filling Token fields that are missing in PartialToken s. This involves IO operations, so both the function and its caller createTokens() have to be asynchronous. The functions also require a batch context for state queries and logging. We modify the createTokens() call in the batch handler to accommodate these changes: processor.run(new TypeormDatabase(), async (ctx) => { let rawTransfers: RawTransfer[] = getRawTransfers(ctx) let owners: Map<string, Owner> = createOwners(rawTransfers) - let tokens: Map<string, Token> = createTokens(rawTransfers, owners) + let tokens: Map<string, Token> = await createTokens(ctx, rawTransfers, owners) let transfers: Transfer[] = createTransfers(rawTransfers, owners, tokens) Next, we implement completeTokens() : async function completeTokens ( ctx : Context , partialTokens : Map < string , PartialToken > ) : Promise < Map < string , Token >> { let tokens : Map < string , Token > = new Map ( ) if ( partialTokens . size === 0 ) return tokens let lastBatchBlockHeader = ctx . blocks [ ctx . blocks . length - 1 ] . header let contract = new bayc . Contract ( ctx , lastBatchBlockHeader , CONTRACT_ADDRESS ) for ( let [ id , ptoken ] of partialTokens ) { let uri = await contract . tokenURI ( ptoken . tokenId ) ctx . log . info ( ` Retrieved metadata URI ${ uri } ` ) let metadata : TokenMetadata | undefined = await fetchTokenMetadata ( ctx , uri ) tokens . set ( id , new Token ( { ... ptoken , uri , ... metadata } ) ) } return tokens } URI retrieval here is similar to what we did in the exploration step: we create a Contract object and use it to call the tokenURI() method of the BAYC token contract. The retrieved URIs are then used by the fetchTokenMetadata() function, which is responsible for HTTPS/IPFS metadata retrieval and parsing. Once we have its output, we can create and return the final Token entity instances. Retrieving external resources ​ In the fetchTokenMetadata() implementation we first classify the URIs depending on the protocol. For IPFS links we replace 'ipfs://' with an address of an IPFS gateway, then retrieve the metadata from all links using a regular HTTPS client. Here for the demonstration purposes we use the public ipfs.io gateway, which is slow and prone to dropping requests due to rate-limiting. For production squids we recommend using a dedicated gateway, e.g. from Filebase . export async function fetchTokenMetadata ( ctx : Context , uri : string ) : Promise < TokenMetadata | undefined > { try { if ( uri . startsWith ( 'ipfs://' ) ) { const gatewayURL = path . posix . join ( IPFS_GATEWAY , ipfsRegExp . exec ( uri ) ! [ 1 ] ) let res = await client . get ( gatewayURL ) ctx . log . info ( ` Successfully fetched metadata from ${ gatewayURL } ` ) return res . data } else if ( uri . startsWith ( 'http://' ) || uri . startsWith ( 'https://' ) ) { let res = await client . get ( uri ) ctx . log . info ( ` Successfully fetched metadata from ${ uri } ` ) return res . data } else { ctx . log . warn ( ` Unexpected metadata URL protocol: ${ uri } ` ) return undefined } } catch ( e ) { throw new Error ( ` Failed to fetch metadata at ${ uri } . Error: ${ e } ` ) } } const ipfsRegExp = / ^ ipfs: \/ \/ ( . + ) $ / We use Axios for HTTPS retrieval. Install it with npm i axios To avoid reinitializing the HTTPS client every time we call the function we bind it to a module-scope constant: const client = axios . create ( { headers : { 'Content-Type' : 'application/json' } , httpsAgent : new https . Agent ( { keepAlive : true } ) , transformResponse ( res : string ) : TokenMetadata { let data : { image : string ; attributes : { trait_type : string ; value : string } [ ] } = JSON . parse ( res ) return { image : data . image , attributes : data . attributes . map ( ( a ) => new Attribute ( { traitType : a . trait_type , value : a . value } ) ) , } } , } ) We move all the code related to metadata retrieval to a separate module src/metadata.ts . Examine its full contents here . Then all that is left is to import the relevant parts in src/main.ts : src/main.ts + import {TokenMetadata, fetchTokenMetadata} from './metadata' and we are done with the processor code for this part of the tutorial. Full squid code at this point is available at this commit . Recreate the database, rebuild the code and refresh the migrations with docker compose down docker compose up -d npm run build rm -r db/migrations npx squid-typeorm-migration generate npx squid-typeorm-migration apply and test the processor by running node -r dotenv/config lib/main.js It runs much slower than before, requiring about three hours to get through the first batch and more than a day to sync (figures out of date). This is something we will address in the next part of the tutorial. Nevertheless, the squid is already fully capable of scraping token metadata and serving it over GraphQL. Verify that by running npx squid-graphql-server and visiting the local GraphiQL playground . It is now possible to retrieve image URLs and attributes for each token: Edit this page Previous Step 2: Owners & tokens Next Step 4: Optimization Exploring token metadata Extending the Token entity Retrieving external resources
Home | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Home Get started SQD Network is a decentralized query engine optimized for batch extraction of large volumes of data. It currently serves historical on-chain data ingested from 200+ EVM and Substrate networks, as well as Solana (in beta), Tron , Starknet and Fuel . The data is comprehensive: for example, on EVM it includes event logs, transaction receipts, traces and per-transaction state diffs. This documentation covers the SQD Network itself and the complimentary products developed by SQD: Squid SDK - a Typescript toolkit for high-performance batch indexing sourcing the data from the SQD Network, without accessing an archival RPC. SQD Cloud - a hosted service for custom indexers and GraphQL APIs. SQD Firehose - a lightweight adapter for running subgraphs against SQD Network, without accessing an archival RPC node. ApeWorx SQD plugin - use SQD Network as a fast data source for the ApeWorx framework. Overview Squid SDK indexing examples Mine the tx data from millions of wallets .addTransaction({}) // ... if (wallets.has(txn.from)) { /* ... */ } if (wallets.has(txn.to)) { /* ... */ } Track USDC Transfers in real time .addLog({ address : [USDC_CONTRACT_ADDRESS], topic0 : [usdcAbi.events.Transfer.topic], }) Real time data is fetched from a chain node RPC; a Database object with unfinalized blocks support is required to store it (see this page for more details). Extract all Transfers to/from vitalik.eth .addLog({ topic0 : [erc20abi.events.Transfer.topic], topic2 : [VITALIK_ETH_TOPIC], }) All Transfer(address,address,uint256) will be captured, including ERC20 and ERC721 transfers and possibly events with the same signature made with other protocols. Index the AAVE Pool tx data, decoding the event logs .addTransaction({ to : [AAVE_CONTRACT], logs : true , }) Including events emitted by other contracts. Get ETH value involved in each call. Index all NFT mints .addLog({ topic0 : [usdcAbi.events.Mint.topic], transaction : true , }) Index all DEX trading pairs and Swap events .addLog({ address : FACTORY_ADDRESSES, topic0 : [PAIR_CREATED_TOPIC], }) .addLog({ topic0 : [SWAP_TOPIC] }) Index internal contract calls and traces .addTrace({ type : [ 'call' ], callTo : [BAYC_ADDRESS], }) .addStateDiff({ address : [BAYC_ADDRESS] }) Call traces will expose any internal calls to BAYC by other contracts. Also retrieves all changes to contract storage state. Mine all NFT contracts ever deployed .addTrace({ type : [ 'create' ], }) .addLog({ topic0 : [erc721.events.Transfer.topic] }) All contract creations are scraped; they will be checked for ERC721 compliance in the batch handler. All ERC721 Transfer events are scraped so that they can be filtered and binned by the contract in the batch handler. Full squid Showcase Expand Highlights Quickstart A 5 minutes intro into Squid SDK Query APIs and IPFS Power up your indexer with third-party APIs and IPFS queries GraphQL subscriptions Live query updates with GraphQL subscriptions Production aliases Manage indexers deployed to SQD Cloud with zero downtime Index Substrate data The most advanced SDK for indexing Substrate-based chains ink! smart contracts First-class indexing of WASM contracts developed with ink! SQD vs The Graph Compare the feature set and the architecture SQD Blog Stay ahead of the curve and discover the latest trends in Web3 data Show me the code! Learn by example: ready-to-use indexers built with Squid SDK FAQ What is a squid? A squid is a indexing project developed using the Squid SDK. It normally extracts the historicaln on-chain data from SQD Network, decodes and transforms it, and persists into a target store (such as Postgres or s3 bucket). The transformed data may be optionally served with a GraphQL API. Why should I use SQD? Indexing on-chain data is essential for building Web3 applications and analytic dashboards. SQD Network and the products build on top offer extremely fast and cost-efficient way to extract and index the historical data, even from lesser known chains. It reduces the data extraction and indexing costs by up to 90% compared to direct indexing using centralized RPC providers like Alchemy or Infura. Finally, by using the SQD Cloud, developers no longer have to care about indexing infrastructure maintenance costs and hassle. How does SQD compare to The Graph? SQD is modular -- the on-chain data is extracted from a decentralized data layer ( SQD Network ), rather than directly from a blockchain node. It enables up to 100x faster indexing, guaranteed data consistensy and reliable indexing even for small networks. For a detailed feature comparison, see SQD vs The Graph . How much does SQD cost? The Squid SDK is open source. Accessing the data from the SQD Network is free until the mainnet launch, and afterwards is projected to be in the range of $1-$5 for a terabyte of extracted data. The SQD Cloud offers a free playground space for developing indexers and a hosted service for production-ready indexing pipelines. The pay-as-you-go pricing only accounts for the actual compute and storage resources consumed by the indexer, see for the pricing details . What is SQD Cloud? SQD Cloud is a service for hosting indexers, managed by Subsquid Labs. Squid CLI provides a convenient way to run, deploy and manage indexing projects (squids) locally and in the Cloud. More questions? Check out our technical community Edit this page
Schema file and codegen | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file Schema file and codegen Entities Indexes and constraints Entity relations Unions and typed JSON Interfaces OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Schema file Schema file and codegen On this page Schema file and codegen The schema file schema.graphql uses a GraphQL dialect to model the target entities and entity relations. The tooling around the schema file is then used to: Generate TypeORM entities (with squid-typeorm-codegen(1) , see below) Generate the database schema from the TypeORM entities (see db migrations ) Optionally, the schema can be used to present the target data with a GraphQL API . The schema file format is loosely compatible with the subgraph schema file, see Migrate from subgraph section for details. TypeORM codegen ​ The squid-typeorm-codegen(1) tool is used to generate TypeORM entity classes from the schema defined in schema.graphql . Invoke it with npx squid-typeorm-codegen By default the entity classes are generated in src/model/generated . Example ​ A Foo entity defined in the schema file: schema.graphql type Foo @entity { id : ID ! bar : String baz : BigInt ! } The generated Foo entity with TypeORM decorators: src/model/generated/foo.ts import { Entity as Entity_ , Column as Column_ , PrimaryColumn as PrimaryColumn_ } from "typeorm" import * as marshal from "./marshal" @ Entity_ ( ) export class Foo { constructor ( props ? : Partial < Foo > ) { Object . assign ( this , props ) } @ PrimaryColumn_ ( ) id ! : string @ Column_ ( "text" , { nullable : true } ) bar ! : string | undefined | null @ Column_ ( "numeric" , { transformer : marshal . bigintTransformer , nullable : false } ) baz ! : bigint } Edit this page Previous Schema file Next Entities TypeORM codegen Example
AND/OR filters | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API Intro Entity queries AND/OR filters Nested field queries Cross-relation queries JSON queries Pagination Sorting Union type resolution The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader Core API AND/OR filters On this page AND/OR filters Overview ​ Our GraphQL implementation offers a vast selection of tools to filter and section results. One of these is the where clause, very common in most database query languages and explained here in detail. In our GraphQL server implementation, we included logical operators to be used in the where clause, allowing to group multiple parameters in the same where argument using the AND and OR operators to filter results based on more than one criteria. Note that the newer and more advanced {entityName}sConnection queries support exactly the same format of the where argument as the older {entityName}s queries used in the examples provided here. Example of an OR clause: ​ Fetch a list of accounts that either have a balance bigger than a certain amount, or have a specific id. query { accounts ( orderBy : balance_DESC , where : { OR : [ { balance_gte : "240000000000000000" } { id_eq : "CksmaBx9rKUG9a7eXwc5c965cJ3QiiC8ELFsLtJMYZYuRWs" } ] } ) { balance id } } Example of AND clause: ​ Fetch a list of accounts that have a balance between two specific amounts: query { accounts ( orderBy : balance_DESC , where : { AND : [ { balance_lte : "240000000000000000" } { balance_gte : "100000000000000" } ] } ) { balance id } } Edit this page Previous Entity queries Next Nested field queries Overview Example of an OR clause: Example of AND clause:
FAQ | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network FAQ On this page FAQ What is SQD Network? ​ SQD Network is a decentralized data lake focused on making blockchain data accessible, regardless of scale. Worker nodes of the network store the data and run queries on their local data chunks; data access nodes (called SQD portals ) request partial query results from multiple workers, concatenate the outputs and expose a stream-based API. How can I participate in SQD Network? ​ Here are the possibilities: You can run a worker node . Workers contribute storage and compute resources to the network and are rewarded with SQD tokens. You can reward well-performing workers by delegating your SQD tokens to them and get rewarded for that. What is a decentralized data lake? ​ A decentralized data lake is a distributed system that stores and organizes large amounts of raw data in its native format until it's needed. SQD Network is an example of a decentralized data lake, providing scalable and efficient access to blockchain data. What are some use cases for SQD’s decentralized data lake? ​ SQD's decentralized data lake can be used for indexing blockchain data, powering blockchain explorers, analytics platforms, multi-chain aggregators, APIs for real-time and historical data, dApp backends and more. Is SQD production-ready? Can I trust it for my dApp? ​ Yes! SQD is production-ready and reliable for your dApp. It has been effectively used in over 500 use cases across ecosystems, proving its trustworthiness and utility within the Web3 developer community. What are the hardware requirements to run a worker node? ​ To run a single worker you will need: 4 vCPU 16GB RAM 1TB SSD stable 24/7 Internet connection, at least 1Gbit public IP and two ports open for incoming traffic: one UDP port for p2p communication (default 12345) one TCP port for Prometheus metrics (default 9090) 100000 SQD tokens (in your wallet or in a special vesting contract) some Arbitrum ETH (for gas) Please refer to this page for further instructions. A single account can register multiple workers. How do I deploy a simple squid indexer? ​ Start your first squid indexer in 5 minutes! Learn more here . What kind of datasets are available from SQD Network? ​ Historical blockchain data optimized for batch access. What chains are currently supported by SQD’s decentralized data lake and SDK? ​ SQD Network currently supports EVM / Ethereum-compatible chains, Substrate-based chains, Solana, Fuel and Starknet ( full list ). More ecosystems will be integrated in the near future -- feel free to request in SquidDevs . How can I add a new chain to be indexed by SQD Network? ​ Simply reach out via this form , and we’ll get back to you as soon as possible. I can’t code. How can I participate in SQD Network? ​ As a non-coder, you will be able to participate in the SQD Network directly by delegating SQD tokens to reliable workers to receive a percentage of the reward. Additionally you can drive the growth of the SQD Network by helping to cultivate a vibrant community, amplifying the network's reach through social media and other platforms. What is SQD’s roadmap? ​ You can find the up-to-date roadmap here . Does SQD Network have an app? ​ Take a look at network.subsquid.io . Why did SQD choose Arbitrum for its contracts? ​ We have chosen Arbitrum for its large community and the availability of quality tooling for smart contract deployment. Where is the data that SQD Network indexes stored? ​ The data indexed by the SQD Network is stored in persistent storage. During the bootstrap phase, the persistent storage used by Subsquid Labs is an s3 compatible service with backups pinned to IPFS. What is the difference between SQD Network and the Graph Network? ​ Learn about the differences and see a full feature matrix here . How does the SQD Network make blockchain data accessible for free? Are there costs involved? ​ The open private SQD Network offers the data for free to consumers while charging fixed subscription fees to chains from which the data is provided. The permissioness public version of SQD Network requires users to lock SQD tokens to reserve bandwidth (i.e. requests per second). However, this limitation only applies if you're running an SQD portal yourself: SQD provides a free rate-limited portal open to everyone. For hosting indexers, we offer our SQD Cloud service. Free unlimited access to a dedicated SQD portal is included in subscription; you only pay for the computational resources you're using (such as CPUs, SSD space etc). Check out its competitive pricing . Note that the fact that we have a paid hosting service does not mean that you cannot self-host your indexer(s). Feel free to shop around and find the best hosting infrastructure for your project! Here's our self-hosting guide . How does SQD ensure the security and privacy of the data accessed through the network? ​ SQD Network will employ validators who listen to the logs of the executed queries and resubmit them to validate the response. There's also an arbitration process to handle any discrepancies. What is "an epoch" in the context of SQD Network? ​ "Epoch" is a unit of time that SQD Network uses for internal settlement. It is defined on-chain and is currently set to be 100 L1 (Ethereum) blocks, or roughly 20 minutes. What is Tethys? ​ Tethys is the long-running testnet of SQD Network. Its contracts run on Arbitrum Sepolia. More details are available here . Edit this page Previous Whitepaper Next Tokenomics What is SQD Network? How can I participate in SQD Network? What is a decentralized data lake? What are some use cases for SQD’s decentralized data lake? Is SQD production-ready? Can I trust it for my dApp? What are the hardware requirements to run a worker node? How do I deploy a simple squid indexer? What kind of datasets are available from SQD Network? What chains are currently supported by SQD’s decentralized data lake and SDK? How can I add a new chain to be indexed by SQD Network? I can’t code. How can I participate in SQD Network? What is SQD’s roadmap? Does SQD Network have an app? Why did SQD choose Arbitrum for its contracts? Where is the data that SQD Network indexes stored? What is the difference between SQD Network and the Graph Network? How does the SQD Network make blockchain data accessible for free? Are there costs involved? How does SQD ensure the security and privacy of the data accessed through the network? What is "an epoch" in the context of SQD Network? What is Tethys?
Frontier EVM-indexing squid | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Frontier EVM-indexing squid On this page Frontier EVM-indexing Squid Objective ​ The goal of this tutorial is to guide you through creating a simple blockchain indexer ("squid") using Squid SDK. The squid will be indexing the data from two contracts ( AstarDegens and AstarCats ) deployed on the Astar network . The objective will be to track ownership and transfers of all NFTs issued by these contracts. A somewhat outdated version of the final result can be browsed here . Pre-requisites ​ Familiarity with Git A properly set up development environment consisting of Node.js, Git and Docker Squid CLI Scaffold using sqd init ​ We will start with the frontier-evm squid template available through sqd init . It is built to index EVM smart contracts deployed on Astar/Shiden, but it is also capable of indexing Substrate events. To retrieve the template and install the dependencies, run sqd init astar-evm-tutorial --template frontier-evm cd astar-tutorial npm i Define Entity Schema ​ Next, we ensure that the data schema of the squid defines entities that we would like to track. We are interested in: Token transfers Ownership of tokens Contracts and their minted tokens Here is a schema that defines the exact entities we need: schema.graphql type Token @entity { id : ID ! owner : Owner uri : String transfers : [ Transfer ! ] ! @derivedFrom ( field : "token" ) contract : Contract } type Owner @entity { id : ID ! ownedTokens : [ Token ! ] ! @derivedFrom ( field : "owner" ) balance : BigInt } type Contract @entity { id : ID ! name : String symbol : String totalSupply : BigInt mintedTokens : [ Token ! ] ! @derivedFrom ( field : "contract" ) } type Transfer @entity { id : ID ! token : Token ! from : Owner to : Owner timestamp : Int ! block : Int ! } It's worth noting a couple of things in this schema definition : @entity : Signals that this type will be translated into an ORM model that is going to be persisted in the database. @derivedFrom : Signals that the field will not be persisted in the database. Instead, it will be derived from the entity relations. type references (e.g. from: Owner ): When used on entity types, they establish a relation between two entities. TypeScript entity classes have to be regenerated whenever the schema is changed, and to do that we use the squid-typeorm-codegen tool: npx squid-typeorm-codegen The (re)generated entity classes can then be browsed at src/model/generated . ABI Definition and Wrapper ​ SQD maintains tools for automated generation of TypeScript classes for handling EVM logs and transactions based on a JSON ABI of the contract. For our squid we will need such a module for the ERC-721 -compliant part of the contracts' interfaces. Once again, the template repository already includes it, but it is still important to explain what needs to be done in case one wants to index a different type of contract. Place any ABIs you requre for interfacing your contracts at ./abi and run npx squid-evm-typegen ./src/abi ./abi/*.json --multicall The results will be stored at src/abi . One module will be generated for each ABI file, and it will include constants useful for filtering and functions for decoding EVM events and functions defined in the ABI. Processor object and the batch handler ​ Squid SDK provides users with the SubstrateBatchProcessor class . Its instances connect to SQD Network gateways at chain-specific URLs, to get chain data and apply custom transformations. The indexing begins at the starting block and keeps up with new blocks after reaching the tip. SubstrateBatchProcessor s expose methods that "subscribe" them to specific data such as Substrate events and calls. There are also specialized methods for subscribing to EVM logs and transactions by address. The actual data processing is then started by calling the .run() function. This will start generating requests to the SQD Network gateway for batches of data specified in the configuration, and will trigger the callback function, or batch handler (passed to .run() as second argument) every time a batch is returned by the gateway. It is in this callback function that all the mapping logic is expressed. This is where chain data decoding should be implemented, and where the code to save processed data on the database should be defined. Managing the EVM contract ​ Before we begin defining the mapping logic of the squid, we are going to write a src/contracts.ts utility module for managing the involved EVM contracts. It will export: Addresses of astarDegens and astarCats contracts. A Map from the contract addresses to hardcoded Contract entity instances. Here are the full file contents: src/contracts.ts import { Store } from '@subsquid/typeorm-store' import { Contract } from './model' export const astarDegensAddress = '0xd59fC6Bfd9732AB19b03664a45dC29B8421BDA9a' . toLowerCase ( ) ; export const astarCatsAddress = '0x8b5d62f396Ca3C6cF19803234685e693733f9779' . toLowerCase ( ) ; export const contractMapping : Map < string , Contract > = new Map ( ) contractMapping . set ( astarDegensAddress , new Contract ( { id : astarDegensAddress , name : 'AstarDegens' , symbol : 'DEGEN' , totalSupply : 10000n , mintedTokens : [ ] } ) ) contractMapping . set ( astarCatsAddress , new Contract ( { id : astarCatsAddress , name : 'AstarCats' , symbol : 'CAT' , totalSupply : 7777n , mintedTokens : [ ] } ) Create the processor object ​ The src/processor.ts file is where squids instantiate and configure their processor objects. We will use an instance of SubstrateBatchProcessor . We adapt the template code to handle two contracts instead of one and point the processor data source setting to the astar SQD Network gateway URL . Here is the end result: src/processor.ts import { assertNotNull } from '@subsquid/util-internal' import { BlockHeader , DataHandlerContext , SubstrateBatchProcessor , SubstrateBatchProcessorFields , Event as _Event , Call as _Call , Extrinsic as _Extrinsic } from '@subsquid/substrate-processor' import * as erc721 from './abi/erc721' import { astarDegensAddress , astarCatsAddress } from './contracts' const processor = new SubstrateBatchProcessor ( ) . setBlockRange ( { from : 442693 } ) . setGateway ( 'https://v2.archive.subsquid.io/network/astar-substrate' ) . setRpcEndpoint ( { url : assertNotNull ( process . env . RPC_ENDPOINT ) , rateLimit : 10 , } ) . addEvmLog ( { address : [ astarDegensAddress ] , range : { from : 442693 } , topic0 : [ erc721 . events . Transfer . topic ] } ) . addEvmLog ( { address : [ astarCatsAddress ] , range : { from : 800854 } , topic0 : [ erc721 . events . Transfer . topic ] } ) export type Fields = SubstrateBatchProcessorFields < typeof processor > export type Block = BlockHeader < Fields > export type Event = _Event < Fields > export type Call = _Call < Fields > export type Extrinsic = _Extrinsic < Fields > export type ProcessorContext < Store > = DataHandlerContext < Store , Fields > warning This code expects to find an URL of a working Astar RPC endpoint in the RPC_ENDPOINT environment variable. Set it in the .env file and in SQD Cloud secrets if and when you deploy your squid there. We tested the code using a public endpoint available at wss://astar.public.blastapi.io ; for production, we recommend using private endpoints or our RPC addon . Define the batch handler ​ We change the batch handler logic taking care to avoid token ID clashing: src/main.ts import { Store , TypeormDatabase } from '@subsquid/typeorm-store' import { In } from 'typeorm' import { astarDegensAddress , astarCatsAddress , contractMapping } from './contracts' import { Owner , Token , Transfer } from './model' import * as erc721 from './abi/erc721' import { processor , ProcessorContext , Event , Block } from './processor' var contractsSaved = false processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { const transfersData : TransferData [ ] = [ ] ; for ( const block of ctx . blocks ) { for ( const event of block . events ) { if ( event . name === 'EVM.Log' ) { const transfer = handleTransfer ( block . header , event ) transfersData . push ( transfer ) } } } if ( ! contractsSaved ) { await ctx . store . upsert ( [ ... contractMapping . values ( ) ] ) contractsSaved = true } await saveTransfers ( ctx , transfersData ) } ) type TransferData = { id : string from : string to : string token : bigint timestamp : number block : number contractAddress : string } function handleTransfer ( block : Block , event : Event ) : TransferData { const { from , to , tokenId } = erc721 . events . Transfer . decode ( event ) return { id : event . id , from , to , token : tokenId , timestamp : block . timestamp , block : block . height , contractAddress : event . args . address } } async function saveTransfers ( ctx : ProcessorContext < Store > , transfersData : TransferData [ ] ) { const getTokenId = transferData => ` ${ contractMapping . get ( transferData . contractAddress ) ?. symbol ?? "" } - ${ transferData . token . toString ( ) } ` const tokensIds : Set < string > = new Set ( ) const ownersIds : Set < string > = new Set ( ) for ( const transferData of transfersData ) { tokensIds . add ( getTokenId ( transferData ) ) ownersIds . add ( transferData . from ) ownersIds . add ( transferData . to ) } const tokens : Map < string , Token > = new Map ( ( await ctx . store . findBy ( Token , { id : In ( [ ... tokensIds ] ) } ) ) . map ( token => [ token . id , token ] ) ) const owners : Map < string , Owner > = new Map ( ( await ctx . store . findBy ( Owner , { id : In ( [ ... ownersIds ] ) } ) ) . map ( owner => [ owner . id , owner ] ) ) const transfers : Set < Transfer > = new Set ( ) for ( const transferData of transfersData ) { const contract = new erc721 . Contract ( // temporary workaround for SDK issue 212 // passing just the ctx as first arg may already work { _chain : { client : ctx . _chain . rpc } } , { height : transferData . block } , transferData . contractAddress ) let from = owners . get ( transferData . from ) if ( from == null ) { from = new Owner ( { id : transferData . from , balance : 0n } ) owners . set ( from . id , from ) } let to = owners . get ( transferData . to ) if ( to == null ) { to = new Owner ( { id : transferData . to , balance : 0n } ) owners . set ( to . id , to ) } const tokenId = getTokenId ( transferData ) let token = tokens . get ( tokenId ) if ( token == null ) { token = new Token ( { id : tokenId , uri : await contract . tokenURI ( transferData . token ) , contract : contractMapping . get ( transferData . contractAddress ) } ) tokens . set ( token . id , token ) } token . owner = to const { id , block , timestamp } = transferData const transfer = new Transfer ( { id , block , timestamp , from , to , token } ) transfers . add ( transfer ) } await ctx . store . upsert ( [ ... owners . values ( ) ] ) await ctx . store . upsert ( [ ... tokens . values ( ) ] ) await ctx . store . insert ( [ ... transfers ] ) } info The contract.tokenURI call is accessing the state of the contract via a chain RPC endpoint. This is slowing down the indexing a little bit, but this data is only available this way. You'll find more information on accessing state in the dedicated section of our docs . Database and the migration ​ Before giving your squid processor a local test, launch a PostgreSQL container with docker compose up -d Squid projects automatically manage the database connection and schema via an ORM abstraction . In this approach the schema is managed through migration files. Since we've made changes to the schema, we need to remove the existing migration(s) and create a new one. This involves the following steps: Build the code: npm run build Make sure you start with a clean Postgres database. The following commands drop-create the Postgres instance in Docker: docker compose down docker compose up -d Skip this step if you haven't used your database since the last docker compose up -d . Regenerate the DB migration: rm -r db/migrations npx squid-typeorm-migration generate Apply the migration: npx squid-typeorm-migration apply Launch the Project ​ To launch the processor run the following command (this will block the current terminal): node -r dotenv/config lib/main.js Finally, in a separate terminal window, launch the GraphQL server: npx squid-graphql-server Visit localhost:4350/graphql to access the GraphiQL console. From this window, you can perform queries such as this one, to find out the account owners with the biggest balances: query MyQuery { owners ( limit : 10 , where : { } , orderBy : balance_DESC ) { balance id } } Or this other one, looking up the tokens owned by a given owner: query MyQuery { tokens ( where : { owner : { id_eq : "0x1210f3ea18ef463c162fff9084cee5b6e5ccab37" } } ) { uri contract { id name symbol totalSupply } } } Have fun playing around with queries, after all, it's a playground ! Edit this page Previous ink! contract indexing Next Processor in action Objective Pre-requisites Scaffold using sqd init Define Entity Schema ABI Definition and Wrapper Processor object and the batch handler Managing the EVM contract Create the processor object Define the batch handler Database and the migration Launch the Project
Log messages | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Log messages On this page Log messages addLog(options) ​ Get log messages emitted by some or all programs in the network. options has the following structure: { // data requests where ? : { programId ? : string [ ] kind ? : ( 'log' | 'data' | 'other' ) [ ] } // related data retrieval include ? : { transaction ? : boolean instruction ? : boolean } range ? : { from : number , to ? : number } } Data requests: programId : the set of addresses of programs emitting the logs. Leave it undefined to subscribe to logs from all programs in the network. kind : the set of values of kind . With transaction = true the processor will retrieve all parent transactions and add them to the transactions iterable within the block data . You can also call augmentBlock() from @subsquid/solana-objects on the block data to populate the convenience reference fields like log.transaction . Note that logs can also be requested by the other SolanaDataSource methods as related data. Selection of the exact fields to be retrieved for each log and its optional parent transaction is done with the setFields() method documented on the Field selection page. Examples ​ Fetch all event logs emitted by Orca Whirlpool. const dataSource = new DataSourceBuilder ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/solana-mainnet' ) . addLog ( { where : { programId : [ PYTH_PUSH_ORACLE_PROGRAM_ID ] } , include : { instruction : true } , range : { from : 241_000_000 } } ) . build ( ) Edit this page Previous Transactions Next Balances Examples
Index Fuel with SQD | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing Index Fuel with SQD üìÑÔ∏è Indexing Fuel Network data Indexing Fuel Network data with Squid SDK. üóÉÔ∏è FuelDataSource 7 items üìÑÔ∏è Cheatsheet Commonly used CLI commands üóÉÔ∏è Network API 1 items Previous Solana API Next Indexing Fuel Network data
addons.postgres section | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference addons.postgres section On this page Postgres add-on To provision a postgres instance, add the addons.postgres: section to the deployment manifest. The add-on deploys a Postgres 14 instance and injects variables with database connection params into the environment of the api and processor squid services. Variable shadowing ​ Any of the following variables set globally or for api or processor squid services in the manifest will be overwritten with Cloud-supplied values: DB_SSL DB_HOST DB_PORT DB_NAME DB_USER DB_PASS DB_URL Config options ​ The addon supports additional PG config options: Name Description Type Default value Optional config.statement_timeout Max execution time after which any query is forcefully aborted, ms number 60000 Optional config. log_min_duration_statement Log queries executing longer than the given threshold, ms number 5000 Optional config. max_pred_locks_per_transaction See Lock Management number 64 Optional config.max_locks_per_transaction See Lock Management number 64 Optional Direct access ​ SQD Cloud enables direct read access to the deployed PG instances. Go to the "DB access" tab of your squid deployment's card in the Cloud web app to get the PG connection string. Scaling ​ The postgres add-on supports storage and compute resource scaling by extending the scale.addons.postgres section of the deploy manifest. The following options are supported Name Description Type Default value Optional storage Volume size for the postgres container memory resource units 10G Optional profile Allocated resources profile small | medium | large | xlarge | 2xlarge small Optional The profile specifications for a postgres service are as follows: Profile colocated vCPU (max) colocated RAM (max) dedicated vCPU (requested) dedicated RAM (max) small 0.2 768Mi 1 2Gi medium 0.5 1.5Gi 2 4Gi large 1 3Gi 4 4Gi xlarge - - 8 16Gi 2xlarge - - 16 32Gi Examples ​ manifest_version : subsquid.io/v0.1 name : sample - squid version : 1 description : | - My advanced squid build : deploy : addons : postgres : config : statement_timeout : 100000 log_min_duration_statement : 100000 processor : cmd : [ "sqd" , "process:prod" ] api : cmd : [ "sqd" , "serve:prod" ] scale : addons : postgres : storage : 100G profile : medium Edit this page Previous scale section Next addons.hasura section Variable shadowing Config options Direct access Scaling Examples
Indexing USDT on Tron | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing Indexing USDT on Tron On this page Indexing USDT on Tron info Tron support in SQD is now in public beta. Please report any bugs or suggestions to Squid Devs . In this tutorial we will look into a squid that indexes USDT transfers on the Tron Network . Pre-requisites: Node.js v20 or newer, Git, Docker. Download the project ​ Begin by retrieving the template and installing the dependencies: git clone https://github.com/subsquid-labs/tron-example cd tron-example npm i TronBatchProcessor configuration ​ Squid processor is a term with two meanings: The process responsible for retrieving and transforming the data. The main object implementing that process. On Tron, TronBatchProcessor is the class that should be used for processor objects. Our first step is to create the processor and configure it to fetch USDT transfers data: src/main.ts // Note that the address is in lowercase hex and has no leading `0x`. // This is the format used throughout the SQD Tron stack. const USDT_ADDRESS = 'a614f803b6fd780986a42c78ec9c7f77e6ded13c' const TRANSFER_TOPIC = 'ddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' const processor = new TronBatchProcessor ( ) // The SQD Network gateway URL - this is where the bulk of data will // be coming from. . setGateway ( 'https://v2.archive.subsquid.io/network/tron-mainnet' ) // SQD Network is always about N blocks behind the head. // We must use regular HTTP API endpoint to get through the last mile // and stay on top of the chain. // This is a limitation, and we promise to lift it in the future! . setHttpApi ( { // Ankr public endpoint is heavily rate-limited so expect many 429 errors url : 'https://rpc.ankr.com/http/tron' , strideConcurrency : 1 , strideSize : 1 , } ) // We request data items via `.addXxx()` methods. // Each `.addXxx()` method accepts item selection criteria // and also allows to request related items. . addLog ( { // select logs where : { address : [ USDT_ADDRESS ] , topic0 : [ TRANSFER_TOPIC ] } , // for each log selected above load related transactions include : { transaction : true } } ) // For each data item we can specify a set of fields we want to fetch. // Accurate selection of only the required fields can have a notable // positive impact on performance when data is sourced from SQD Network. . setFields ( { block : { timestamp : true , } , transaction : { hash : true } , log : { address : true , data : true , topics : true } } ) See also TronBatchProcessor reference . The next step is to configure data decoding and transformation. Processing the data ​ The other part of the squid processor configuration is the callback function used to process batches of filtered data, the batch handler . It is typically defined within a processor.run() call, like this: processor . run ( database , async ctx => { ... } Here, processor is the object described in the previous section database is a Database implementation specific to the target data sink. We want to store the data in a PostgreSQL database and present with a GraphQL API, so we provide a TypeormDatabase object here. ctx is the batch context object that exposes a batch of data (at ctx.blocks ) and any data persistence facilities derived from db (at ctx.store ). See Block data for Tron for details on how the data batches are presented. Batch handler is where the raw on-chain data is decoded, transformed and persisted. Here is our definition: src/main.ts processor . run ( new TypeormDatabase ( ) , async ctx => { let transfers : Transfer [ ] = [ ] for ( let block of ctx . blocks ) { for ( let log of block . logs ) { if ( log . address == USDT_ADDRESS && log . topics ?. [ 0 ] === TRANSFER_TOPIC ) { assert ( log . data , 'USDT transfers always carry data' ) let tx = log . getTransaction ( ) // `0x` prefixes make log data compatible with EVM codec let event = { topics : log . topics . map ( t => '0x' + t ) , data : '0x' + log . data } let { from , to , value } = erc20 . events . Transfer . decode ( event ) transfers . push ( new Transfer ( { id : log . id , blockNumber : block . header . height , timestamp : new Date ( block . header . timestamp ) , tx : tx . hash , from , to , amount : value } ) ) } } } await ctx . store . insert ( transfers ) } ) This goes through all the logs in the block, verifies that they originate from USDT_ADDRESS and have a TRANSFER_TOPIC as the first topic, decodes the log and saves a Transfer record with the obtained data to the database. See Prepare the store for more details on how the database interface works. info Notice that we used the SQD EVM codec and squid-evm-typegen -generated module to decode events. Since Tron events are fully EVM-compatible, this is the recommended way. This tooling requires that the hex values are prefixed with 0x , as shown above. At this point the squid is ready for its first test run. Execute npx tsc docker compose up -d npx squid-typeorm-migration apply node -r dotenv/config lib/main.js You can verify that the data is being stored in the database by running docker exec "$(basename " $( pwd ) " ) -db-1 " psql -U postgres -c " SELECT * FROM transfer LIMIT 100 " Full code can be found here . Edit this page Previous Tron indexing Next TronBatchProcessor Download the project TronBatchProcessor configuration Processing the data
Gear support | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific ink! contracts support Frontier EVM support Gear support Substrate data sourcing Substrate types bundles Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Substrate-specific Gear support Gear support info SQD Network has gateways for two networks that use Gear Protocol: Vara and Vara Testnet . Here are their endopint URLs: https://v2.archive.subsquid.io/network/vara https://v2.archive.subsquid.io/network/vara-testnet Indexing Gear programs is supported with addGearMessageQueued() and addGearUserMessageSent() specialized data requests. These subscribe to the events Gear.MessageQueued and Gear.UserMessageSent emitted by a specified Gear program. The processor can also subscribe to any other event with addEvent() and filter by program ID in the batch handler, if so necessary. An example of a squid indexing a Gear program (an NFT contract) can be found here . Edit this page Previous Frontier EVM support Next Substrate data sourcing
Index Tron with SQD | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing Indexing USDT on Tron TronBatchProcessor Cheatsheet Network API SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Tron indexing Index Tron with SQD üìÑÔ∏è Indexing USDT on Tron Indexing Tron data with Squid SDK üóÉÔ∏è TronBatchProcessor 6 items üìÑÔ∏è Cheatsheet Commonly used CLI commands üóÉÔ∏è Network API 1 items Previous Fuel Network API Next Indexing USDT on Tron
SQD Cloud reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Deployment workflow Pricing Troubleshooting Resources Reference Deployment manifest scale section addons.postgres section addons.hasura section RPC service networks .squidignore file(s) Changelog: slots and tags Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Cloud Reference SQD Cloud reference documentation 📄️ Deployment manifest A reference for the squid deployment manifest 📄️ scale section Scale the squid with the deployment manifest 📄️ addons.postgres section Provision and scale postgres for a squid 📄️ addons.hasura section Run a Hasura instance 📄️ RPC service networks Request a built-in RPC service 📄️ .squidignore file(s) Exclude files from squid images 📄️ Changelog: slots and tags Update details for existing users Previous production-alias Next Deployment manifest
commands.json | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI commands.json commands.json The sqd tool automatically discovers and loads any extra commands defined in the commands.json file. Here is a sample file demonstrating the available features: { // comments are ok "$schema" : "https://subsquid.io/schemas/commands.json" , "commands" : { "clean" : { "description" : "delete all build artifacts" , "cmd" : [ "rm" , "-rf" , "lib" ] } , "build" : { "description" : "build the project" , "deps" : [ "clean" ] , // commands to execute before "cmd" : [ "tsc" ] } , "typegen" : { "hidden" : true , // Don't show in the overview listing "workdir" : "abi" , // change working dir "command" : [ "squid-evm-typegen" , // node_modules/.bin is in the PATH "../src/abi" , { "glob" : "*.json" } // cross-platform glob expansion ] , "env" : { // additional environment variables "DEBUG" : "*" } } } } This functionality is managed by the @subsquid/commands package. All squid templates include such a file with a predefined set of useful shortcuts. See Cheatsheet . Edit this page Previous Installation Next sqd auth
A distributed query engine and a data lake for blockchain data | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network A distributed query engine and a data lake for blockchain data 📄️ Overview Flavors of SQD Network 📄️ Whitepaper A fairly detailed design overview 📄️ FAQ What is SQD Network? 📄️ Tokenomics SQD Tokenomics overview 🗃️ Participate 5 items 🗃️ Reference 4 items 📄️ Portal beta info Welcome to the SQD Portal Open Beta! Below you’ll find easy-to-follow instructions on what to do next and what you can expect from this release. Previous Overview Next Overview
RPC ingestion and reorgs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides RPC ingestion and reorgs On this page RPC ingestion and reorgs Starting with the ArrowSquid release, the processor can ingest data either from a SQD Network gateway or directly from an RPC endpoint. If both a network gateway and a RPC endpoint are provided, the processor will use network data until it reaches the highest block available there, then index the few remaining blocks using the RPC endpoint. This allows squids to combine low sync times with near real-time chain data access. It is, however, possible to use just the RPC endpoint (e.g. for local development ). On EVM it is also possible to use just the gateway (e.g. for non-realtime analytics). RPC ingestion can create a heavy load on node endpoints. The load is typically short in duration when a SQD Network gateway is used, and the total number of requests is low. However, the request rate may be sufficient to trigger HTTP 429 responses. Use private endpoints and rate limit your requests with the rateLimit option ( EVM / Substrate ). Indexing unfinalized blocks â€‹ When ingesting from RPC, Squid SDK can index blocks before they are finalized , enabling real-time use cases. If a blockhain reorganization happens, processor will roll back any changes to the database made due to orphaned blocks, then re-run its batch handler on consensus blocks. To take advantage of this feature, a squid processor must have a node RPC endpoint as one of its data sources and use a Database with unfinalized blocks support (e.g. TypeormDatabase ) in its processor.run() call, and not disable RPC ingestion explicitly with setRpcDataIngestionSettings({ disabled: true }) . With this, the processor will consider some blocks to be "hot": EVM Substrate All blocks with fewer confirmations than the number set by the setFinalityConfirmations() setting All blocks above the latest finalized block provided by the chain_getFinalizedHead() RPC method The processor will periodically (interval setting for EVM , Substrate ) poll the RPC endpoint for changes in consensus. When the consensus changes, it will re-run the batch handler with the new consensus data and ask the Database to adjust its state. The Database then must roll back the changes made due to orphaned blocks and apply the new changes. With this, the state of the Database reflects the current blockchain consensus at all times. Edit this page Previous Batch processing Next External APIs and IPFS Indexing unfinalized blocks
Proxy contracts | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Factory contracts Proxy contracts Substrate-specific Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides EVM-specific Proxy contracts Indexing proxy contracts Proxy contracts are smart contracts that use the DELEGATECALL EVM instruction to forward calls to some of their methods to another contract (referred to as the implementation contract ). This setup allows changing the code that runs on method calls without redeploying the contract and losing its state . Proxy contracts are standardized in ERC-1967 . The easiest way to know if the contract is a proxy or not is to visit the "Contract" tab of its Etherscan page. Proxy contracts will typically have the "Read as Proxy" and "Write as Proxy" buttons available. Here is how it looks for the USDC contract : Clicking on the "Read as Proxy" button reveals the address of the implementation contract, FiatTokenV2_1 in this case. A few scenarios are possible when indexing a proxy contract: Events and calls that need to be indexed are described in the implementation contract . This is by far the most common use case. If that is what you need, simply use the implementation ABI to interface with the proxy contract. In our USDC example you would need to retrieve the ABI of the FiatTokenV2_1 contract, e.g. by running # 0xa232...bDCF is the implementation contract address npx squid-evm-typegen src/abi 0xa2327a938Febf5FEC13baCFb16Ae10EcBc4cbDCF #fiatToken That retrieves the ABI from Etherscan API and uses it to create TypeScript wrapper classes for implementation functions and events at './src/abi/fiatToken.ts`. Use these to subscribe to and decode the data of the proxy contract: ./src/processor.ts import * as fiatToken from './abi/fiatToken' export const processor = new EvmBatchProcessor ( ) . addLog ( { // 0xA0b8...eB48 is the proxy contract address address : [ '0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48' ] , topic0 : [ fiatToken . events . Mint . topic ] , } ) ./src/main.ts // ... let { minter , to , amount } = fiatToken . events . Mint . decode ( log ) // ... Complete example is available here (uses Transfer s instead of Mint s). Events and calls that need to be indexed are described in the proxy contract itself . This typically occurs in indexers that track contract upgrades. In this case simply use the ABI of the proxy contract to both request and decode the data: npx squid-evm-typegen src/abi 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 #usdcProxy ./src/processor.ts import * as usdcProxy from './abi/usdcProxy' export const processor = new EvmBatchProcessor ( ) . addLog ( { address : [ '0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48' ] , topic0 : [ usdcProxy . events . Upgraded . topic ] , } ) ./src/main.ts // ... let { implementation } = usdcProxy . events . Upgraded . decode ( log ) // ... Events and call described in both contracts are needed . If that is your use case, retrieve ABIs of both the proxy and the implementation and use both: ./src/processor.ts import * as fiatToken from './abi/fiatToken' import * as usdcProxy from './abi/usdcProxy' export const processor = new EvmBatchProcessor ( ) . addLog ( { address : [ '0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48' ] , topic0 : [ fiatToken . events . Mint . topic , usdcProxy . events . Upgraded . topic , ] , } ) Edit this page Previous Factory contracts Next Substrate-specific
sqd CLI cheatsheet | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Environment set up Indexer from scratch Development flow Project structure sqd CLI cheatsheet Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Getting started sqd CLI cheatsheet On this page Squid CLI cheatsheet The sqd CLI tool has built-in aliasing that picks up the commands defined in commands.json in the project root. In all squid templates this file is pre-populated with some handy scripts briefly described below. One can always inspect the available commands defined in commands.json with sqd --help The commands defined by commands.json will appear in the SQUID COMMANDS help sections. Before using the sqd CLI tool, make sure all the project dependencies are installed: npm i Building the squid ​ sqd build             Build the squid project sqd clean             Delete all build artifacts Running the squid ​ info Both sqd up and sqd down assume that the docker compose command is supported and the docker deamon is running. Modify the definitions in commands.json accordingly if docker-compose should be used instead. sqd up                  Start a local PG database sqd down                Drop the local PG database sqd run [PATH]          Run all the services defined in squid.yaml locally sqd serve               Start the GraphQL server sqd serve:prod          Start the GraphQL API server with caching and limits DB migrations ​ Read TypeORM Migration generation for details. sqd migration:apply             apply pending migrations sqd migration:generate          generate the migration for the schema defined in schema.graphql sqd migration:clean             clean the db/migrations folder Code generation ​ Consult TypeORM Model generation for TypeORM model generation details, and Type-safe decoding for type generation. info Depending on the template, sqd typegen is aliased to a different typegen tool specific to the chain type and thus has different usage. Consult sqd typegen --help for details. sqd codegen        Generate TypeORM entities from schema.graphql sqd typegen        Generate data access classes for an ABI file(s) in the ./abi folder Edit this page Previous Project structure Next Features & Guides Building the squid Running the squid DB migrations Code generation
CSV support | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions CSV support Parquet support JSON support S3 support bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks file-store extensions CSV support On this page CSV format support Table Implementation ​ The @subsquid/file-store-csv package provides a Table implementation for writing to CSV files. Use it by supplying one or more of its instances via the tables field of the Database constructor argument . Constructor of the Table implementation accepts the following arguments: fileName: string : the name of the output file in every dataset partition folder. schema: {[column: string]: ColumnData} : a mapping from CSV column names to ColumnData objects . A mapping of the same keys to data values is the row type used by the table writer . options?: TableOptions : see Table Options . Columns ​ ColumnData objects determine how the in-memory data representation of each table column should be serialized. They are made with the Column factory function that accepts a column data type and an optional {nullable?: boolean} options object as arguments. Column types can be obtained by making the function calls listed below from the Types submodule. They determine the type that the table writer will expect to find at the corresponding field of data row objects. Column type Type of the data row field Types.String() string Types.Numeric() number or bigint Types.Boolean() boolean Types.DateTime(format?: string) Date Types.JSON<T>() T Types.DateTime accepts an optional strftime -compatible format string. If it is omitted, the dates will be serialized to ISO strings . The type T supplied to the Types.JSON() generic function must be an object with string keys (extend {[k: string]: any} ). Table Options ​ As its optional final argument, the constructor of Table accepts an object that defines table options: TableOptions { dialect ? : Dialect header ? : boolean } Here, dialect determines the details of the CSV formatting (see the details below, default: dialects.excel ) header determines whether a CSV header should be added (default: true ) Dialect type is defined as follows: Dialect { delimiter : string escapeChar ? : string quoteChar : string quoting : Quote lineterminator : string } where enum Quote { ALL , // Put all values in quotes. MINIMAL , // Only quote strings with special characters. // A special character is one of the following: // delimiter, lineterminator, quoteChar. NONNUMERIC , // Quote strings, booleans, DateTimes and JSONs. NONE // Do not quote values. } is the enum determining how the formatted values should be quoted. The quote character is escaped for all values of quoting ; Quote.NONE additionally escapes the rest of the special characters and the escape character. Two dialect presets are available via the dialects object exported by @subsquid/file-store-csv : export let dialects = { excel : { delimiter : ',' , quoteChar : '"' , quoting : Quote . MINIMAL , lineterminator : '\r\n' } , excelTab : { delimiter : '\t' , quoteChar : '"' , quoting : Quote . MINIMAL , lineterminator : '\r\n' } } Example ​ This saves ERC20 Transfer events captured by the processor to TSV (tab-separated values) files. Full squid code is available in this repo . import { Database , LocalDest } from '@subsquid/file-store' import { Column , Table , Types , dialects } from '@subsquid/file-store-csv' ... const dbOptions = { tables : { TransfersTable : new Table ( 'transfers.tsv' , { from : Column ( Types . String ( ) ) , to : Column ( Types . String ( ) ) , value : Column ( Types . Numeric ( ) ) } , { dialect : dialects . excelTab , header : true } ) } , dest : new LocalDest ( './data' ) , chunkSizeMb : 10 } processor . run ( new Database ( dbOptions ) , async ( ctx ) => { ... let from : string = ... let to : string = ... let value : bigint = ... ctx . store . TransfersTable . write ( { from , to , value } ) ... } ) Edit this page Previous file-store extensions Next Parquet support Table Implementation Columns Table Options Example
Unions and typed JSON | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file Schema file and codegen Entities Indexes and constraints Entity relations Unions and typed JSON Interfaces OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Schema file Unions and typed JSON On this page Unions and typed JSON Complex scalar types can be modelled using a typed JSON fields together with union types, making safe union types. Typed JSON ​ It is possible to define explicit types for JSON fields. The generated entity classes and the GraphQL API will respect the type definition of the field, enforcing the data integrity. Example type Entity @entity { a : A } type A { a : String b : B c : JSON } type B { a : A b : String e : Entity } Union types ​ One can leverage union types supported both by Typescript and GraphQL .  The union operator for schema.graphql supports only non-entity types, including typed JSON types described above. JSON types, however, are allowed to reference an entity type. Example type User @entity { id : ID ! login : String ! } type Farmer { user : User ! crop : Int } type Degen { user : User ! bag : String } union Owner = Farmer | Degen type NFT @entity { name : String ! owner : Owner ! } Edit this page Previous Entity relations Next Interfaces Typed JSON Union types
Fields selection | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Substrate Block data for Substrate General settings Data requests Fields selection Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors Substrate Fields selection On this page Fields selection setFields(options) ​ Set the fields to be retrieved for data items of each supported type. The options object has the following structure: { event ? : // field selector for logs call ? : // field selector for transactions extrinsic ? : // field selector for state diffs block ? : // field selector for block headers } Every field selector is a collection of boolean fields mapping one-to-one to the fields of data items within the batch context iterables . Setting a field of a field selector of a given type to true, like this let processor = new SubstrateBatchProcessor ( ) . setFields ( { call : { origin : true } } ) will cause the processor to populate the corresponding field (in this case origin ) of all data items of that type (in this case Call ). The field will be available in all data items of the type, including nested items. Suppose we used the processor defined above to subscribe to some calls as well as some events, and for each event we requested a parent call: processor . addEvent ( { // some event data requests call : true } ) . addCall ( { // some call data requests } ) As a result, the origin field would be available both within the call items retrieved due to .addCall() and within the call items that provide parent call information for the events: processor . run ( db , async ctx => { for ( let block of ctx . blocks ) { for ( let call of block . calls ) { let callOrigin = call . origin // OK } for ( let event of block . events ) { let parentCallOrigin = event . call ?. origin // also OK! } } } ) Some data fields are enabled by default but can be disabled by setting a field of a field selector to false . For example, this code will not compile: let processor = new SubstrateBatchProcessor ( ) . setFields ( { call : { name : false } } ) . addCall ( { // some call data requests } ) processor . run ( db , async ctx => { for ( let block of ctx . blocks ) { for ( let call of block . calls ) { let callName = call . name // ERROR: no such field } } } ) Disabling unused fields will improve sync performance, as the disabled fields will not be fetched from SQD Network. Data item types and field selectors ​ tip Most IDEs support smart suggestions to show the possible field selectors. For VS Code, press Ctrl+Space : Here we describe the data item types as functions of the field selectors. In SubstrateBatchProcessor each field of a field selector maps to the eponymous field of its corresponding data item type. Item data fields are divided into three categories: Fields that are added independently of the setFields() call. These are either fixed or depend on the related data retrieval flags (e.g. call for events). Fields that can be disabled by setFields() . E.g. the args field will be fetched for calls by default, but can be disabled by setting args: false within the call field selector. Fields that can be requested by setFields() . E.g. the tip field will only be available in extrinsics if the extrinsic field selector sets tip: true . In addition to data fields, data items have methods that simplify access to related data, e.g. Call.getExtrinsic() . Events ​ Event data items may have the following fields: Event { // can be requested with field selectors phase : 'ApplyExtrinsic' | 'Initialization' | 'Finalization' // can be disabled with field selectors name : string args : any // independent of field selectors id : string index : number block : BlockHeader callAddress ? : number [ ] call ? : Call extrinsicIndex ? : number extrinsic ? : Extrinsic // methods getCall ( ) : Call getExtrinsic ( ) : Extrinsic } Definition of the types mentioned are available in their respective sections: BlockHeader , Call , Extrinsic . Calls ​ Call data items may have the following fields: Call { // can be requested with field selectors error ? : any origin ? : any success : boolean // can be disabled with field selectors name : string args : any // independent of field selectors id : string address : number [ ] block : BlockHeader events : Event [ ] parentCall ? : Call subcalls : Call [ ] extrinsicIndex : number extrinsic ? : Extrinsic // methods getParentCall ( ) : Call getExtrinsic ( ) : Extrinsic } Definition of the types mentioned are available in their respective sections: BlockHeader , Event , Extrinsic . Extrinsics ​ Extrinsic data items may have the following fields: Extrinsic { // can be requested with field selectors success : boolean error ? : unknown fee ? : bigint hash : string signature ? : { // no per-field selection here: // "signature: true" gets all fields address : unknown signature : unknown signedExtensions : unknown } tip ? : bigint version : number // independent of field selectors id : string index : number block : BlockHeader events : Event [ ] call ? : Call subcalls : Call [ ] // methods getCall ( ) : Call } Definition of mentioned types are available in their respective sections: BlockHeader , Event , Call , Block headers ​ BlockHeader data items may have the following fields: BlockHeader { // can be requested with field selectors digest : { // request with "digest: true" logs : string [ ] } extrinsicsRoot : string stateRoot : string timestamp ? : number validator ? : string // independent of field selectors id : string hash : string height : number implName : string implVersion : number parentHash : string specName : string specVersion : number _runtime : Runtime _runtimeOfPrevBlock : Runtime // methods getParent ( ) : { _runtime : Runtime hash : string height : number } } Runtime is an internal type that tools like squid-substrate-typegen use for dealing with Substrate runtime versions. The curious may take a look at the definition here . A complete example ​ Fetch Balances.Transfer events and Balances.transfer_keep_alive calls along with their parent extrinsics. Enrich the extrinsics with hashes, fees and success flags: const processor = new SubstrateBatchProcessor ( ) . addEvent ( { name : [ 'Balances.Transfer' ] , extrinsic : true } ) . addCall ( { name : [ 'Balances.transfer_keep_alive' ] , extrinsic : true } ) . setFields ( { extrinsic : { hash : true , fee : true , success : true } } ) Edit this page Previous Data requests Next Data sinks Data item types and field selectors Events Calls Extrinsics Block headers A complete example
ArrowSquid for Substrate | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Migrate from The Graph ArrowSquid for EVM ArrowSquid for Substrate hasura-configuration tool v2 Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Migration guides ArrowSquid for Substrate On this page Migrate to ArrowSquid (Substrate) This is a Substrate guide. EVM guide is available here . ArrowSquid refers to @subsquid/substrate-processor versions 3.x and above. It is not compatible with the FireSquid archive endpoints. Instead, the new version uses SQD Network gateways. See the Supported networks page. The main feature introduced by the ArrowSquid update is the new ability of the processor to ingest unfinalized blocks directly from a network node, instead of waiting for an archive to ingest and serve it first. The processor can now handle forks and rewrite the contents of its database if it happens to have indexed orphaned blocks. This allows SQD-based APIs to become near real-time and respond to the on-chain activity with subsecond latency. Other new features include the new streamlined processor configuration interface and automatic retrieval of execution traces. On the implementation side, the way in which data is fetched has been made more efficient. End-to-end Substrate ArrowSquid examples: general Substrate indexing ; indexing an Ink! contract ; working with Frontier EVM pallet data . Here is a step-by-step guide for migrating a squid built with an older SDK version to the post-ArrowSquid tooling. Step 1 ​ Update all packages affected by the update: npm i @subsquid/substrate-processor@latest @subsquid/typeorm-store@latest npm i --save-dev @subsquid/substrate-typegen@latest If your squid uses file-store , please update any related packages to the @latest version. Step 2 ​ Replace the old setDataSource() processor configuration call with a combination of setGateway() and setRpcEndpoint() . Use a public SQD Network gateway URL for your network . If your squid did not use an RPC endpoint before, find one for your network and set it with setRpcEndpoint() . For Aleph Zero your edit might look like this: processor - .setDataSource({ - archive: 'https://aleph-zero.archive.subsquid.io/graphql' - }) + .setGateway('https://v2.archive.subsquid.io/network/aleph-zero') + .setRpcEndpoint({ + url: 'https://aleph-zero-rpc.dwellir.com', + rateLimit: 10 + }) We recommend using a private RPC endpoint for the best performance, e.g. from Dwellir . For squids deployed to SQD Cloud you may also consider using our RPC addon . Your squid will work with just an RPC endpoint, but it will sync significantly slower. With a SQD Network gateway available the processor will only use RPC to retrieve metadata and sync the few most recent blocks not yet made available by the gateway; without it it will retrieve all data from the endpoint. Step 3 ​ Next, we have to account for the changes in signatures of the data requesting processor methods addEvent() , addCall() , addEvmLog() , addEthereumTransaction() , addContractsContractEmitted() , addGearMessageQueued() , addGearUserMessageSent() , Previously, each call of these methods supplied its own fine-grained data fields selector. In the new interface, these calls only request data items, either directly or by relation (for example with the call flag for event-requesting methods). Field selection is now done by the new setFields() method on a per-item-type basis: once for all Call s, once for all Event s etc. The setting is processor-wide: for example, all Call s returned by the processor will have the same set of available fields, regardless of whether they were requested directly or as related data. Begin migrating to the new interface by finding all calls to these methods and combining all the field selectors into processor-wide event , call and extrinsic field selectors that request all fields previously requested by individual selectors. Note that call.args and event.args are now requested by default and can be omitted. When done, add a call to setFields() supplying it with the new field selectors. The new field selector format is fully documented on the Field selection page. info Blanket field selections like {data: {event: {extrinsic: true}}} are not supported in ArrowSquid. If you used one of these, please find out which exact fields you use in the batch handler and specifically request them. warning Do not include related data requests into the field selection. E.g. . setFields ( { event : { call : true } } ) will at best fail to compile and at worst make your squid fail due to HTTP 400s from the network gateway. For example, suppose the processor was initialized with the following three calls: const processor = new SubstrateBatchProcessor ( ) . addCall ( 'Balances.transfer_keep_alive' , { data : { call : { origin : true , args : true } } } as const ) . addEvent ( 'Balances.Transfer' , { data : { event : { args : true , extrinsic : { hash : true , fee : true } , call : { success : true , error : true } } } } as const ) . addEvmLog ( CONTRACT_ADDRESS , filter : [ [ abi . events . SomeLog . topic ] ] , { data : { event : { extrinsic : { hash : true , tip : true } } } } as const ) then the new global selectors should be added like this: const processor = new SubstrateBatchProcessor ( ) // ... addXXX() calls ... . setFields ( { event : { } , call : { origin : true , success : true , error : true } , extrinsic : { hash : true , fee : true , tip : true } } ) Be aware that this operation will not increase the amount of data retrieved from the SQD Network gateway, since previously such coalescence was done under the hood and all fields were retrieved by the processor anyway. In fact, the amount of data should decrease due to a more efficient transfer mechanism employed by ArrowSquid. There are two old field requests that have no direct equivalent in the new interface: call.parent It is currently impossible to request just the parent call. Work around by requesting the full call stack with stack: true in the call-requesting configuration calls, then using .parentCall property or getParentCall() method of Call data items to get parent calls. event.evmTxHash Processor no longer makes EVM transaction hashes explicitly available. Currently the only way to get hashes of logs' parent txs is to figure out the subset of calls that emitted the required logs, request them explicitly with their related events, filter out Ethereum.Executed events and decode these. See this issue . Step 4 ​ Replace the old calls to the data requesting processor methods with calls using the new signatures. warning The meaning of passing [] as a set of parameter values has been changed in the ArrowSquid release: now it selects no data . Pass undefined for a wildcard selection: . addEvent ( { name : [ ] } ) // selects no events . addEvent ( { } ) // selects all events Old data request calls will be erased during the process. Make sure to request the appropriate related data with the boolean flags ( call for event-requesting methods, events for call-requesting methods and extrinsic , stack for both). Interfaces of data request methods are documented on the Data requests reference page: addEvent() , addCall() , addEvmLog() , addEthereumTransaction() , addContractsContractEmitted() , addGearMessageQueued() , addGearUserMessageSent() . Here is a fully updated initialization code for the example processor from step 3: const processor = new SubstrateBatchProcessor ( ) . addCall ( { name : [ 'Balances.transfer_keep_alive' ] } ) . addEvent ( { name : [ 'Balances.Transfer' ] , call : true , extrinsic : true } ) . addEvmLog ( { address : [ CONTRACT_ADDRESS ] , topic0 : [ abi . events . SomeLog . topic ] , extrinsic : true } ) . setFields ( { event : { } , call : { origin : true , success : true , error : true } , extrinsic : { hash : true , fee : true , tip : true } } ) Step 5 ​ Finally, update the batch handler to use the new batch context . The main change here is that now block.items is split into three separate iterables: block.calls , block.events and block.extrinsics . There are two ways to migrate: If you're in a hurry, use the orderItems(block: Block) function from this snippet : src/main.ts // ... // paste the gist here processor . run ( db , async ctx => { // ... for ( let block of ctx . blocks ) { for ( let item of orderItems ( block ) ) { // item processing code should work unchanged } } // ... } ) Alternatively, rewrite your batch handler using the new batch context interface . See Block data for Substrate for the documentation on Substrate-specific part of batch context. Step 6 ​ Rewrite your typegen.json in the new style. Here is an example: { "outDir" : "src/types" , "specVersions" : "https://v2.archive.subsquid.io/metadata/kusama" , "pallets" : { "Balances" : { "events" : [ "Transfer" ] , "calls" : [ ] , "storage" : [ ] , "constants" : [ ] } } } Note the changes: Archive URL as "specVersions" is replaced with an URL of our new metadata service ( "https://v2.archive.subsquid.io/metadata/kusama" ) Requests for data wrappers are now made on a per-pallet basis. Check out the updated Substrate typegen documentation page . If you used any storage calls, consult this documentation page for guidance. Once you're done migrating typegen.json , regenerate the wrapper classes with npx squid-substrate-typegen typegen.json Step 7 ​ Iteratively reconcile any type errors arising when building your squid (e.g. with npm run build ). If you need to specify the field selection generic type argument explicitly, get it as a typeof of the setFields argument value: import { Block } from '@subsquid/substrate-processor' const fieldSelection = { event : { } , call : { origin : true , success : true , error : true } , extrinsic : { hash : true , fee : true , tip : true } } as const type MyBlock = Block < typeof fieldSelection > // ... At this point your squid should be able to work with the ArrowSquid tooling. If it doesn't, read on. Troubleshooting ​ If these instructions did not work for you, please let us know at the SquidDevs Telegram chat . Edit this page Previous ArrowSquid for EVM Next hasura-configuration tool v2 Step 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Troubleshooting
Troubleshooting | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Troubleshooting On this page Troubleshooting Many issues can be resolved by following the best practices guide of SQD Cloud. Processor ​ Error: data out-of-bounds ethers errors on EVM Usually this means that you're using the decoder on the wrong data. Make sure that the decoder receives only the data you intend it to. Example: suppose you want to add the processing of a Mint event to a squid that is currently processing only Transfer events. You change the processor configuration to get the Mint events for you, but you forget to sort the events in the batch handler and a data item with a Mint event finds its way into a decoder of Transfer s. Another common source of this error is faulty RPC endpoints. If your EVM processor crashes during RPC ingestion on a log with '0x' in its data field, try switching to another RPC provider or, if you are developing locally, to another Ethereum network emulator. BAD_DATA when using a Multicall contract This error can occur for a variety of reasons, but one common cause is choosing a Multicall deployment that is newer than the oldest blocks that have to be indexed. When batch state queries are performed on historical chain state older than the Multicall deployment, EVM detects that and refuses to run. Solutions: Use an older Multicall deployment. Delay your chain state queries until a later block. These issues are explored in Part 4 of the BAYC tutorial . Data sinks ​ QueryFailedError: relation "..." does not exist It is likely that the generated migrations in the db/migrations folder are outdated and do not match the schema file.
Recreate the migrations from scratch as detailed on this page . Query runner already released. Cannot run queries anymore , or too late to perform db updates, make sure you haven't forgot to await on db query If your squid saves its data to a database , all operations with ctx.store are asynchronous. Make sure you await on all store operations like upsert , update , find , save etc. You may find the require-await eslint rule to be helpful. QueryFailedError: invalid byte sequence for encoding "UTF8": 0x00 PostgreSQL doesn't support storing NULL (\0x00) characters in text fields. Usually the error occurs when a raw bytes string (like UIntArray or Bytes ) is inserted into a String field. If this is the case, use hex encoding, e.g. using util-internal-hex library. For addresses, use the ss58 encoding library . GraphQL ​ API queries are too slow Make sure all the necessary fields are indexed . The best way to do that is to perform the query optimization procedure . Annotate the schema and set reasonable limits for the incoming queries to protect against DoS attacks response might exceed the size limit Make sure the input query has limits set or the entities are decorated with @cardinality . We recommend using XXXConnection queries for pagination. For configuring limits and max response sizes, see DoS protection . Edit this page Previous The frontier package Next Examples Processor Data sinks GraphQL
sqd autocomplete | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd autocomplete sqd autocomplete Display autocomplete installation instructions. Edit this page Previous sqd auth Next sqd deploy
Public gateways | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Public gateways EVM API Substrate API Starknet API Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Reference Public gateways On this page Public gateways info Starting today (2025-02-23) all open private network gateways are rate limited at 50 requests per 10 seconds per IP. Higher bandwidths will soon become available via the public network. EVM / Ethereum-compatible ​ This section lists public EVM gateways available from the open private network . If you are developing a squid, supply the gateway URL of your network to setGateway() EvmBatchProcessor configuration method, for example: const processor = new EvmBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) See EVM API if you're looking to interface with one of these gateways directly. Network Chain ID State diffs Traces Gateway URL 0g Testnet 16600 https://v2.archive.subsquid.io/network/0g-testnet Abstract Mainnet 2741 https://v2.archive.subsquid.io/network/abstract-mainnet Abstract Testnet 11124 https://v2.archive.subsquid.io/network/abstract-testnet Agung 9990 https://v2.archive.subsquid.io/network/agung-evm Aleph Zero EVM 41455 ✓ ✓ https://v2.archive.subsquid.io/network/aleph-zero-evm-mainnet Arbitrum Nova 42170 ✓ https://v2.archive.subsquid.io/network/arbitrum-nova Arbitrum One 42161 ✓ https://v2.archive.subsquid.io/network/arbitrum-one Arbitrum Sepolia 421614 up to block 55667987 ? https://v2.archive.subsquid.io/network/arbitrum-sepolia Arthera Mainnet 10242 https://v2.archive.subsquid.io/network/arthera-mainnet Astar (!) 592 https://v2.archive.subsquid.io/network/astar-mainnet Avalanche 43114 ✓ ✓ https://v2.archive.subsquid.io/network/avalanche-mainnet Avalanche Testnet 43113 ✓ ✓ https://v2.archive.subsquid.io/network/avalanche-testnet B3 8333 ✓ ✓ https://v2.archive.subsquid.io/network/b3-mainnet B3 Sepolia 1993 ✓ ✓ https://v2.archive.subsquid.io/network/b3-sepolia Base 8453 ✓ ✓ https://v2.archive.subsquid.io/network/base-mainnet Base Sepolia 84532 ✓ ✓ https://v2.archive.subsquid.io/network/base-sepolia Berachain bArtio 80084 ✓ ✓ https://v2.archive.subsquid.io/network/berachain-bartio Berachain Mainnet 80094 ✓ ✓ https://v2.archive.subsquid.io/network/berachain-mainnet Binance 56 ✓ ✓ https://v2.archive.subsquid.io/network/binance-mainnet Binance Testnet 97 https://v2.archive.subsquid.io/network/binance-testnet Bitfinity Mainnet 355110 https://v2.archive.subsquid.io/network/bitfinity-mainnet Bitfinity Testnet (*1) 355113 https://v2.archive.subsquid.io/network/bitfinity-testnet Bitgert 32520 up to block 4298094 ? ✓ https://v2.archive.subsquid.io/network/bitgert-mainnet Bitgert Testnet 64668 ✓ https://v2.archive.subsquid.io/network/bitgert-testnet Bittensor Mainnet (*2) 964 https://v2.archive.subsquid.io/network/bittensor-mainnet-evm Bittensor Testnet (*3) 945 https://v2.archive.subsquid.io/network/bittensor-testnet-evm Blast L2 81457 ✓ ✓ https://v2.archive.subsquid.io/network/blast-l2-mainnet Blast Sepolia 168587773 https://v2.archive.subsquid.io/network/blast-sepolia BOB Mainnet 60808 ✓ ✓ https://v2.archive.subsquid.io/network/bob-mainnet BOB Sepolia 808813 ✓ ✓ https://v2.archive.subsquid.io/network/bob-sepolia Botanix Mainnet 3637 ✓ ✓ https://v2.archive.subsquid.io/network/botanix-mainnet Botanix Testnet 3636 ✓ ✓ https://v2.archive.subsquid.io/network/botanix-testnet Camp Network Testnet v2 325000 ✓ ✓ https://v2.archive.subsquid.io/network/camp-network-testnet-v2 Canto 7700 https://v2.archive.subsquid.io/network/canto Canto Testnet 7701 https://v2.archive.subsquid.io/network/canto-testnet Celo Testnet (Alfajores) 44787 https://v2.archive.subsquid.io/network/celo-alfajores-testnet Celo Mainnet 42220 https://v2.archive.subsquid.io/network/celo-mainnet Core Mainnet 1116 ✓ https://v2.archive.subsquid.io/network/core-mainnet CrossFi Mainnet (*4) 4158 https://v2.archive.subsquid.io/network/crossfi-mainnet CrossFi Testnet (*4) 4157 https://v2.archive.subsquid.io/network/crossfi-testnet Cyber Mainnet 7560 https://v2.archive.subsquid.io/network/cyber-mainnet Cyberconnect L2 Testnet 111557560 ✓ ✓ https://v2.archive.subsquid.io/network/cyberconnect-l2-testnet Degen Chain 666666666 https://v2.archive.subsquid.io/network/degen-chain DFK Chain 53935 https://v2.archive.subsquid.io/network/dfk-chain Dogechain 2000 https://v2.archive.subsquid.io/network/dogechain-mainnet Dogechain Testnet 568 https://v2.archive.subsquid.io/network/dogechain-testnet Ethereum Holesky 17000 ✓ ✓ https://v2.archive.subsquid.io/network/ethereum-holesky Ethereum Hoodi 560048 https://v2.archive.subsquid.io/network/ethereum-hoodi Ethereum 1 ✓ ✓ https://v2.archive.subsquid.io/network/ethereum-mainnet Ethereum Sepolia 11155111 ✓ ✓ https://v2.archive.subsquid.io/network/ethereum-sepolia Etherlink Mainnet 42793 https://v2.archive.subsquid.io/network/etherlink-mainnet Etherlink Testnet 128123 https://v2.archive.subsquid.io/network/etherlink-testnet Exosama 2109 ✓ ✓ https://v2.archive.subsquid.io/network/exosama Fantom 250 https://v2.archive.subsquid.io/network/fantom-mainnet Fantom Testnet 4002 https://v2.archive.subsquid.io/network/fantom-testnet Flare 14 https://v2.archive.subsquid.io/network/flare-mainnet MemeCore Testnet (Formicarium) 43521 ✓ ✓ https://v2.archive.subsquid.io/network/formicarium-testnet Gravity 1625 ✓ ✓ https://v2.archive.subsquid.io/network/galxe-gravity Arbitrum Blueberry 88153591557 https://v2.archive.subsquid.io/network/gelato-arbitrum-blueberry Gnosis 100 ✓ ✓ https://v2.archive.subsquid.io/network/gnosis-mainnet Hyperliquid Mainnet 999 https://v2.archive.subsquid.io/network/hyperliquid-mainnet Hyperliquid Testnet (*5) 998 https://v2.archive.subsquid.io/network/hyperliquid-testnet Immutable zkEVM 13371 https://v2.archive.subsquid.io/network/immutable-zkevm-mainnet Immutable zkEVM Testnet 13473 https://v2.archive.subsquid.io/network/immutable-zkevm-testnet Ink Mainnet 57073 ✓ ✓ https://v2.archive.subsquid.io/network/ink-mainnet Ink Sepolia 763373 ✓ ✓ https://v2.archive.subsquid.io/network/ink-sepolia Kyoto Testnet 1998 https://v2.archive.subsquid.io/network/kyoto-testnet Linea 59144 ✓ https://v2.archive.subsquid.io/network/linea-mainnet Lukso Mainnet 42 https://v2.archive.subsquid.io/network/lukso-mainnet Manta Pacific 169 ✓ ✓ https://v2.archive.subsquid.io/network/manta-pacific Manta Pacific Sepolia 3441006 https://v2.archive.subsquid.io/network/manta-pacific-sepolia Mantle 5000 https://v2.archive.subsquid.io/network/mantle-mainnet Mantle Sepolia 5003 https://v2.archive.subsquid.io/network/mantle-sepolia Mega Testnet 6342 https://v2.archive.subsquid.io/network/mega-testnet MemeCore Mainnet 4352 ✓ ✓ https://v2.archive.subsquid.io/network/memecore-mainnet Merlin Mainnet 4200 https://v2.archive.subsquid.io/network/merlin-mainnet Merlin Testnet 686868 https://v2.archive.subsquid.io/network/merlin-testnet Metis 1088 except on blocks 16861274 - 16861286 inclusive ✓ https://v2.archive.subsquid.io/network/metis-mainnet Mode Mainnet 34443 https://v2.archive.subsquid.io/network/mode-mainnet Monad Testnet 10143 https://v2.archive.subsquid.io/network/monad-testnet Moonbase Alpha (!) 1287 https://v2.archive.subsquid.io/network/moonbase-testnet Moonbeam (!) 1284 ✓ https://v2.archive.subsquid.io/network/moonbeam-mainnet Moonriver (!) 1285 ✓ https://v2.archive.subsquid.io/network/moonriver-mainnet Naka Chain 42225 https://v2.archive.subsquid.io/network/nakachain Neon EVM Devnet (*6) 245022926 https://v2.archive.subsquid.io/network/neon-devnet Neon EVM 245022934 https://v2.archive.subsquid.io/network/neon-mainnet opBNB 204 https://v2.archive.subsquid.io/network/opbnb-mainnet opBNB Testnet 5611 https://v2.archive.subsquid.io/network/opbnb-testnet Optimism 10 from block 105235063 ✓ ✓ https://v2.archive.subsquid.io/network/optimism-mainnet Optimism Sepolia 11155420 https://v2.archive.subsquid.io/network/optimism-sepolia Ozean Testnet 42 ✓ ✓ https://v2.archive.subsquid.io/network/ozean-testnet Peaq 3338 https://v2.archive.subsquid.io/network/peaq-mainnet Plume Mainnet 98866 ✓ ✓ https://v2.archive.subsquid.io/network/plume Plume Devnet 98864 ✓ ✓ https://v2.archive.subsquid.io/network/plume-devnet Plume Mainnet (Legacy) 98865 ✓ ✓ https://v2.archive.subsquid.io/network/plume-legacy Plume Testnet 98867 ✓ ✓ https://v2.archive.subsquid.io/network/plume-testnet Polygon Amoy Testnet 80002 https://v2.archive.subsquid.io/network/polygon-amoy-testnet Polygon 137 ✓ ✓ https://v2.archive.subsquid.io/network/polygon-mainnet Polygon zkEVM Cardona Testnet 2442 https://v2.archive.subsquid.io/network/polygon-zkevm-cardona-testnet Polygon zkEVM 1101 https://v2.archive.subsquid.io/network/polygon-zkevm-mainnet Poseidon Testnet 31911 ✓ ✓ https://v2.archive.subsquid.io/network/poseidon-testnet Prom Mainnet 227 ✓ ✓ https://v2.archive.subsquid.io/network/prom-mainnet Puppynet (Shibarium's Testnet) 157 ✓ ✓ https://v2.archive.subsquid.io/network/puppynet RISE Sepolia 11155931 https://v2.archive.subsquid.io/network/rise-sepolia Scroll 534352 ✓ https://v2.archive.subsquid.io/network/scroll-mainnet Scroll Sepolia 534351 ✓ https://v2.archive.subsquid.io/network/scroll-sepolia Shibarium 109 ✓ ✓ https://v2.archive.subsquid.io/network/shibarium Shibuya (!) 81 https://v2.archive.subsquid.io/network/shibuya-testnet Shiden (!) 336 https://v2.archive.subsquid.io/network/shiden-mainnet Skale Nebula 1482601649 https://v2.archive.subsquid.io/network/skale-nebula Soneium Mainnet 1868 https://v2.archive.subsquid.io/network/soneium-mainnet Soneium Minato Testnet 1946 https://v2.archive.subsquid.io/network/soneium-minato-testnet Sonic Blaze Testnet 57054 ✓ https://v2.archive.subsquid.io/network/sonic-blaze-testnet Sonic Mainnet 146 ✓ https://v2.archive.subsquid.io/network/sonic-mainnet Sonic Testnet 64165 ✓ https://v2.archive.subsquid.io/network/sonic-testnet StratoVM Sepolia 93747 ✓ ✓ https://v2.archive.subsquid.io/network/stratovm-sepolia Superseed Mainnet 5330 https://v2.archive.subsquid.io/network/superseed-mainnet Superseed Sepolia 53302 https://v2.archive.subsquid.io/network/superseed-sepolia Taiko Mainnet 167000 https://v2.archive.subsquid.io/network/taiko-mainnet Tanssi 5678 https://v2.archive.subsquid.io/network/tanssi Unichain Mainnet 130 ✓ ✓ https://v2.archive.subsquid.io/network/unichain-mainnet Unichain Sepolia 1301 https://v2.archive.subsquid.io/network/unichain-sepolia X Layer Mainnet 196 https://v2.archive.subsquid.io/network/xlayer-mainnet X Layer Testnet 195 https://v2.archive.subsquid.io/network/xlayer-testnet zkLink Nova Mainnet 810180 ✓ https://v2.archive.subsquid.io/network/zklink-nova-mainnet zkSync 324 from block 15500000 ✓ https://v2.archive.subsquid.io/network/zksync-mainnet zkSync Sepolia 300 ✓ https://v2.archive.subsquid.io/network/zksync-sepolia Zora 7777777 ✓ ✓ https://v2.archive.subsquid.io/network/zora-mainnet Zora Sepolia 999999999 https://v2.archive.subsquid.io/network/zora-sepolia (!) Only for EVM data. For Substrate and ink! data use the corresponding Substrate gateway . (*1) Genesis block for Bitfinity Testnet is 2760000. (*2) The first available block for Bittensor Mainnet is 4400000. (*3) The first available block for Bittensor Testnet is 3300000. (*4) The first available block for CrossFI networks is 1. (*5) Hyperliquid Testnet support is in beta. The first available block is subject to change; currently it is 18374838. Unlike all other supported networks, Hyperliquid skips some blocks; these have timestamp === 0 . You have to request the timestamp field to identify these. If you notice that some of the non-skipped blocks are missing, please let us know at the SquidDevs Telegram channel . (*6) Genesis block for Neon EVM Devnet is 177455580. L1 fields availability ​ Extra information on L1 height and transaction fees is available for select Layer 2 EVM networks: Optimism Sepolia Base Base Sepolia Zora Zora Sepolia Scroll ( l1Fee only) Scroll Sepolia ( l1Fee only) Substrate-based ​ This section lists public Substrate gateways of the open private network . Gateway URLs should be used with the setGateway() SubstrateBatchProcessor configuration method, for example: const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/phala' ) . setRpcEndpoint ( 'https://api.phala.network/rpc' ) // required on Substrate See Substate API if you're looking to interface with one of these gateways directly. Network Gateway URL Acala https://v2.archive.subsquid.io/network/acala Acurast Canary https://v2.archive.subsquid.io/network/acurast-canary Agung https://v2.archive.subsquid.io/network/agung Aleph Zero https://v2.archive.subsquid.io/network/aleph-zero Aleph Zero Testnet https://v2.archive.subsquid.io/network/aleph-zero-testnet Amplitude https://v2.archive.subsquid.io/network/amplitude Asset Hub Kusama (*) https://v2.archive.subsquid.io/network/asset-hub-kusama Asset Hub Polkadot (*) https://v2.archive.subsquid.io/network/asset-hub-polkadot Asset Hub Rococo https://v2.archive.subsquid.io/network/asset-hub-rococo Asset Hub Westend https://v2.archive.subsquid.io/network/asset-hub-westend Asset Hub Westend Next https://v2.archive.subsquid.io/network/asset-hub-westend-next Astar https://v2.archive.subsquid.io/network/astar-substrate Avail Mainnet (**) https://v2.archive.subsquid.io/network/avail Basilisk https://v2.archive.subsquid.io/network/basilisk Bifrost Kusama https://v2.archive.subsquid.io/network/bifrost-kusama Bifrost Polkadot https://v2.archive.subsquid.io/network/bifrost-polkadot Bittensor https://v2.archive.subsquid.io/network/bittensor Bittensor Testnet https://v2.archive.subsquid.io/network/bittensor-testnet Bridge Hub Kusama https://v2.archive.subsquid.io/network/bridge-hub-kusama Bridge Hub Polkadot https://v2.archive.subsquid.io/network/bridge-hub-polkadot Bridge Hub Rococo https://v2.archive.subsquid.io/network/bridge-hub-rococo Bridge Hub Westend https://v2.archive.subsquid.io/network/bridge-hub-westend Centrifuge https://v2.archive.subsquid.io/network/centrifuge Cere https://v2.archive.subsquid.io/network/cere Chainfliip https://v2.archive.subsquid.io/network/chainflip Clover https://v2.archive.subsquid.io/network/clover Collectives Polkadot https://v2.archive.subsquid.io/network/collectives-polkadot Collectives Westend https://v2.archive.subsquid.io/network/collectives-westend Darwinia Crab https://v2.archive.subsquid.io/network/darwinia-crab Crust https://v2.archive.subsquid.io/network/crust Dancebox https://v2.archive.subsquid.io/network/dancebox Darwinia https://v2.archive.subsquid.io/network/darwinia Data Avail (Avail Goldberg) https://v2.archive.subsquid.io/network/data-avail Eden https://v2.archive.subsquid.io/network/eden Enjin Canary Matrix https://v2.archive.subsquid.io/network/enjin-canary-matrix Enjin Matrix https://v2.archive.subsquid.io/network/enjin-matrix Enjin Relay https://v2.archive.subsquid.io/network/enjin-relay Equilibrium https://v2.archive.subsquid.io/network/equilibrium Foucoco https://v2.archive.subsquid.io/network/foucoco Frequency https://v2.archive.subsquid.io/network/frequency Gemini 3h https://v2.archive.subsquid.io/network/gemini-3h HydraDX https://v2.archive.subsquid.io/network/hydradx Integritee Network https://v2.archive.subsquid.io/network/integritee Interlay https://v2.archive.subsquid.io/network/interlay Invarch Parachain https://v2.archive.subsquid.io/network/invarch-parachain Invarch Tinkernet https://v2.archive.subsquid.io/network/invarch-tinkernet Joystream https://v2.archive.subsquid.io/network/joystream Karura https://v2.archive.subsquid.io/network/karura Khala https://v2.archive.subsquid.io/network/khala Kilt https://v2.archive.subsquid.io/network/kilt Kintsugi https://v2.archive.subsquid.io/network/kintsugi Kusama https://v2.archive.subsquid.io/network/kusama Litentry https://v2.archive.subsquid.io/network/litentry Moonbase https://v2.archive.subsquid.io/network/moonbase-substrate Moonbeam https://v2.archive.subsquid.io/network/moonbeam-substrate Moonriver https://v2.archive.subsquid.io/network/moonriver-substrate Paseo https://v2.archive.subsquid.io/network/paseo Peaq https://v2.archive.subsquid.io/network/peaq-mainnet-substrate Pendulum https://v2.archive.subsquid.io/network/pendulum People Chain https://v2.archive.subsquid.io/network/people-chain Phala https://v2.archive.subsquid.io/network/phala Phala Testnet https://v2.archive.subsquid.io/network/phala-testnet Picasso https://v2.archive.subsquid.io/network/picasso Polimec https://v2.archive.subsquid.io/network/polimec Polkadex https://v2.archive.subsquid.io/network/polkadex Polkadot https://v2.archive.subsquid.io/network/polkadot Polymesh https://v2.archive.subsquid.io/network/polymesh Reef https://v2.archive.subsquid.io/network/reef Reef Testnet https://v2.archive.subsquid.io/network/reef-testnet Robonomics https://v2.archive.subsquid.io/network/robonomics Rococo https://v2.archive.subsquid.io/network/rococo Rolimec https://v2.archive.subsquid.io/network/rolimec Shibuya https://v2.archive.subsquid.io/network/shibuya-substrate Shiden https://v2.archive.subsquid.io/network/shiden-substrate Sora https://v2.archive.subsquid.io/network/sora-mainnet Subsocial Parachain https://v2.archive.subsquid.io/network/subsocial-parachain Ternoa https://v2.archive.subsquid.io/network/ternoa Turing https://v2.archive.subsquid.io/network/turing-mainnet Turing Avail https://v2.archive.subsquid.io/network/turing-avail Vara https://v2.archive.subsquid.io/network/vara Vara Testnet https://v2.archive.subsquid.io/network/vara-testnet Westend https://v2.archive.subsquid.io/network/westend Zeitgeist https://v2.archive.subsquid.io/network/zeitgeist Zeitgeist Testnet https://v2.archive.subsquid.io/network/zeitgeist-testnet zkVerify Testnet https://v2.archive.subsquid.io/network/zkverify-testnet (*) Asset Hub networks for Polkadot and Kusama were formerly known as Statemint and Statemine, respectively (**) The Avail Mainnet dataset starts at block 1 Solana and compatibles ​ This section lists public gateways of the open private network that serve data of Solana and compatible networks. Gateway URLs should be used with the setGateway() SolanaDataSource configuration method, for example: const dataSource = new SolanaDataSource ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/solana-mainnet' ) Read more about Solana indexing in the dedicated section . See Solana API if you're looking to interface with one of these gateways directly. Network Gateway URL Solana (*) https://v2.archive.subsquid.io/network/solana-mainnet Eclipse Mainnet (**) https://v2.archive.subsquid.io/network/eclipse-mainnet Eclipse Testnet https://v2.archive.subsquid.io/network/eclipse-testnet Soon Mainnet https://v2.archive.subsquid.io/network/soon-mainnet Soon Testnet (***) https://v2.archive.subsquid.io/network/soon-testnet Soon Devnet (****) https://v2.archive.subsquid.io/network/soon-devnet (*) Solana data is available starting from block ( not slot! ) 269_828_500. (**) The first block for Eclipse Mainnet is 24_641_070. (***) The first block for Soon Testnet is 30_255. (****) Soon Devnet data is available starting from block 2_471_639. Fuel ​ SQD's open private network serves Fuel Network data via the following gateways: Network Gateway URL Fuel Mainnet https://v2.archive.subsquid.io/network/fuel-mainnet Fuel Testnet https://v2.archive.subsquid.io/network/fuel-testnet Gateway URL should be used with the setGateway() FuelDataSource configuration method: const dataSource = new FuelDataSource ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/fuel-mainnet' ) Read more about Fuel indexing in the dedicated section . See Fuel API if you're looking to interface with any of the Fuel gateways directly. Tron ​ SQD's open private network serves Tron data via the following gateway: https://v2.archive.subsquid.io/network/tron-mainnet Gateway URL should be used with the setGateway() TronBatchProcessor configuration method: qconst processor = new TronBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/tron-mainnet' ) Read more about Tron indexing in the dedicated section . See Tron API if you're looking to interface with the Tron gateway directly. Starknet ​ SQD's open private network serves Starknet data via the following gateways: Network Gateway URL Starknet Mainnet https://v2.archive.subsquid.io/network/starknet-mainnet Starknet Sepolia https://v2.archive.subsquid.io/network/starknet-sepolia See Starknet API to learn how to interface with it. Edit this page Previous Reference Next EVM API EVM / Ethereum-compatible L1 fields availability Substrate-based Solana and compatibles Fuel Tron Starknet
General settings | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Substrate Block data for Substrate General settings Data requests Fields selection Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors Substrate General settings On this page General settings tip The method documentation is also available inline and can be accessed via suggestions in most IDEs. The following setters configure the global settings of SubstrateBatchProcessor . They return the modified instance and can be chained. Calling setRpcEndpoint() is a hard requirement on Substrate, as chain RPC is used to retrieve chain metadata. Adding a SQD Network gateway with setGateway() is optional but highly recommended, as it greatly reduces RPC usage. To reduce it further, you can explicitly disable RPC ingestion by calling setRpcDataIngestionSettings({ disabled: true }) : in this scenario the RPC will only be used for metadata retrieval and to perform any direct RPC queries you might be doing in your squid code. This will, however, introduce a delay of a few thousands of blocks between the chain head and the highest block available to your squid. setGateway(url: string | GatewaySettings) ​ Adds a SQD Network data source. The argument is either a string URL of a SQD Network gateway or { url : string // gateway URL requestTimeout ? : number // in milliseconds } See Substrate gateways . setRpcEndpoint(rpc: ChainRpc) ​ Adds a RPC data source. If added, it will be used for RPC ingestion (unless explicitly disabled with setRpcDataIngestionSettings() ) any direct RPC queries you make in your squid code A node RPC endpoint can be specified as a string URL or as an object: type ChainRpc = string | { url : string // http, https, ws and wss are supported capacity ? : number // num of concurrent connections, default 10 maxBatchCallSize ? : number // default 100 rateLimit ? : number // requests per second, default is no limit requestTimeout ? : number // in milliseconds, default 30_000 headers : Record < string , string > // http headers } Setting maxBatchCallSize to 1 disables batching completely. tip We recommend using private endpoints for better performance and stability of your squids. For SQD Cloud deployments you can use the RPC addon . If you use an external private RPC, keep the endpoint URL in a Cloud secret . setDataSource(ds: {archive?: string, chain?: ChainRpc}) (deprecated) ​ Replaced by setGateway() and setRpcEndpoint() . setRpcDataIngestionSetting(settings: RpcDataIngestionSettings) ​ Specify the RPC ingestion settings. type RpcDataIngestionSettings = { disabled ? : boolean headPollInterval ? : number newHeadTimeout ? : number } Here, disabled : Explicitly disables data ingestion from an RPC endpoint. RPC endpoint is still required on Substrate because SubstrateBatchProcessor relies on it for metadata. The only effect of this setting is to have the processor stop once it reaches the max SQD Network dataset height. headPollInterval : Poll interval for new blocks in milliseconds. Poll mechanism is used to get new blocks via HTTP connections. Default: 5000. newHeadTimeout : When ingesting from a websocket, this setting specifies the timeout in milliseconds after which the connection will be reset and subscription re-initiated if no new blocks were received. Default: no timeout. setBlockRange({from: number, to?: number}) ​ Limits the range of blocks to be processed. When the upper bound is specified, processor will terminate with exit code 0 once it reaches it. Note that block ranges can also be specified separately for each data request. This method sets global bounds for all block ranges in the configuration. includeAllBlocks(range?: {from: number, to?: number}) ​ By default, processor will fetch only blocks which contain requested items. This method modifies such behavior to fetch all chain blocks. Optionally a range of blocks can be specified for which the setting should be effective. setTypesBundle(bundle: string | OldTypesBundle | OldSpecsBundle | PolkadotjsTypesBundle) ​ Sets a types bundle . Types bundle is only required for historical blocks which have metadata version below 14 and only if we don't have built-in support for the chain in question. Most chains listed in the polkadot.js app are supported. SQD project has its own types bundle format, however, most of polkadotjs types bundles will work as well. Types bundles can be specified in 2 different ways: as a name of a JSON file: processor . setTypesBundle ( 'typesBundle.json' ) as an OldTypesBundle / OldSpecsBundle or PolkadotjsTypesBundle object: // OldTypesBundle object processor . setTypesBundle ( { types : { Foo : 'u8' } } ) There a mini-guide on how to obtain type bundles for Substrate chains without relying on SQD tools. setPrometheusPort(port: string | number) ​ Sets the port for a built-in prometheus health metrics server (serving at http://localhost:${port}/metrics ). By default, the value of PROMETHEUS_PORT environment variable is used. When it is not set, processor will pick an ephemeral port. Edit this page Previous Block data for Substrate Next Data requests setGateway(url: string | GatewaySettings) setRpcEndpoint(rpc: ChainRpc) setDataSource(ds: {archive?: string, chain?: ChainRpc}) (deprecated) setRpcDataIngestionSetting(settings: RpcDataIngestionSettings) setBlockRange({from: number, to?: number}) includeAllBlocks(range?: {from: number, to?: number}) setTypesBundle(bundle: string | OldTypesBundle | OldSpecsBundle | PolkadotjsTypesBundle) setPrometheusPort(port: string | number)
Field selection | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing How to start SDK SolanaDataSource Block data for Solana General settings Instructions Transactions Log messages Balances Token balances Rewards Field selection Typegen Network API Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Solana indexing SDK SolanaDataSource Field selection On this page Field selection setFields(options) ​ Set the fields to be retrieved for data items of each supported type. The options object has the following structure: { instruction ? : // field selector for instructions transaction ? : // field selector for transactions log ? : // field selector for log messages balance ? : // field selector for balances tokenBalance ? : // field selector for token balances reward ? : // field selector for reward block ? : // field selector for block headers } Every field selector is a collection of boolean fields, typically mapping one-to-one to the fields of data items within the batch context iterables . Defining a field of a field selector of a given type and setting it to true will cause the processor to populate the corresponding field of all data items of that type. Here is a definition of a processor that requests signatures and err fields for transactions and the data field for instructions: const dataSource = new DataSourceBuilder ( ) . setFields ( { transaction : { signatures : true , err : true } , instruction : { data : true } } ) Same fields will be available for all data items of any given type, including the items accessed via nested references. Suppose we used the processor defined above to subscribe to some transactions as well as some instructions, and for each instruction we requested a parent transaction: dataSource . addInstruction ( { where : { // some instruction data requests } , include : { transaction : true } } ) . addTransaction ( { where : { // some transaction data requests } , include : { instruction : true } } ) . build ( ) After populating the convenience reference fields with augmentBlock() from @subsquid/solana-objects , signatures and err fields would be available both within the transaction items of the transactions iterable of block data and within the transaction items that provide parent transaction information for the instructions matching the addInstruction() data request: run ( dataSource , database , async ( ctx ) => { let blocks = ctx . blocks . map ( augmentBlock ) for ( let block of blocks ) { for ( let txn of block . transactions ) { let sig = txn . signature ; // OK } for ( let ins of block . instructions ) { if ( /* ins matches the data request */ ) { let parentTxSig = log . transaction . signature ; // also OK! } } } } ) Some data fields, like signatures for transactions, are enabled by default but can be disabled by setting a field of a field selector to false . For example, this code will not compile: const dataSource = new DataSourceBuilder ( ) . setFields ( { transaction : { signatures : false , } } ) . build ( ) run ( dataSource , database , async ( ctx ) => { for ( let block of ctx . blocks ) { for ( let txn of block . transactions ) { let signatures = txn . signatures ; // ERROR: no such field } } } ) Disabling unused fields will improve sync performance, as the disabled fields will not be fetched from the SQD Network gateway. Data item types and field selectors ​ tip Most IDEs support smart suggestions to show the possible field selectors. For VS Code, press Ctrl+Space . info All addresses and pubkeys are represented as base58-encoded strings. Here we describe the data item types as functions of the field selectors. Unless otherwise mentioned, each data item type field maps to the eponymous field of its corresponding field selector. Item fields are divided into three categories: Fields that are always added regardless of the setFields() call. Fields that are enabled by default and can be disabled by setFields() . E.g. a signatures field will be fetched for transactions by default, but can be disabled by setting signatures: false within the log field selector. Fields that can be requested by setFields() . Instruction ​ Instruction data items may have the following fields: Instruction { // independent of field selectors transactionIndex : number instructionAddress : number [ ] // can be disabled with field selectors programId : string accounts : string [ ] data : string isCommitted : boolean // can be enabled with field selectors computeUnitsConsumed ? : bigint error ? : string hasDroppedLogMessages : boolean } Here, instruction address is an array of tree indices addressing the instruction in the call tree: Top level instructions get addresses [0] , [1] , ..., [N-1] , where N is the number of top level instructions in the transaction. An address of length 2 indicates an inner instruction directly called by one of the top level instructions. For example, [3, 0] is the first inner instruction called by the fourth top level instruction. Addresses of length 3 or more indicate inner instructions invoked by other inner instructions. Transaction ​ Transaction data items may have the following fields: Transaction { // independent of field selectors transactionIndex : number // can be disabled with field selectors signatures : string [ ] err : null | object // can be requested with field selectors version : 'legacy' | number accountKeys : string [ ] addressTableLookups : AddressTableLookup [ ] numReadonlySignedAccounts : number numReadonlyUnsignedAccounts : number numRequiredSignatures : number recentBlockhash : string computeUnitsConsumed : bigint fee : bigint loadedAddresses : { readonly : string [ ] writable : string [ ] } // request the whole struct with loadedAddresses: true hasDroppedLogMessages : boolean } LogMessage ​ LogMessage data items may have the following fields: LogMessage { // independent of field selectors transactionIndex : number logIndex : number instructionAddress : number [ ] // can be disabled with field selectors programId : string kind : 'log' | 'data' | 'other' message : string } Balance ​ Balance data items may have the following fields: Balance { // independent of field selectors transactionIndex : number account : string [ ] // can be disabled with field selectors pre : bigint post : bigint } TokenBalance ​ Field selection for token balances data items is more nuanced, as depending on the subtype of the token balance some fields may be undefined . PostTokenBalance and PreTokenBalance both represent token balances, however PreTokenBalance will have postProgramId, postMint, postDecimals, postOwner and postAmount as undefined . PostTokenBalance will have preProgramId, preMint, preDecimals, preOwner and preAmount as undefined . TokenBalance data items may have the following fields: TokenBalance { // independent of field selectors transactionIndex : number account : string // can be disabled with field selectors preMint : string preDecimals : number preOwner ? : string preAmount : bigint postMint : string postDecimals : number postOwner ? : string postAmount : bigint // can be enabled by field selectors postProgramId ? : string preProgramId ? : string } Reward ​ Reward data items may have the following fields: Reward { // independent of field selectors pubkey : string // can be disabled with field selectors lamports : bigint rewardType ? : string // can be enabled by field selectors postBalance : bigint commission ? : number } Block header ​ BlockHeader data items may have the following fields: BlockHeader { // independent of field selectors hash : string height : number parentHash : string // can be disabled with field selectors slot : number parentSlot : number timestamp : number } A complete example ​ import { run } from "@subsquid/batch-processor" ; import { augmentBlock } from "@subsquid/solana-objects" ; import { DataSourceBuilder , SolanaRpcClient } from "@subsquid/solana-stream" ; import { TypeormDatabase } from "@subsquid/typeorm-store" ; import * as whirlpool from "./abi/whirpool" ; const dataSource = new DataSourceBuilder ( ) . setGateway ( "https://v2.archive.subsquid.io/network/solana-mainnet" ) . setRpc ( { client : new SolanaRpcClient ( { url : process . env . SOLANA_NODE , } ) , strideConcurrency : 10 , } ) . setBlockRange ( { from : 259_984_950 } ) ; . setFields ( { block : { timestamp : false } , transaction : { signatures : false , version : true , fee : true } , instruction : { accounts : false , error : true } , tokenBalance : { postProgramId : true } } ) . addInstruction ( { // select instructions that where : { // were executed by the Whirlpool program programId : [ whirlpool . programId ] , // have the first 8 bytes of .data equal to swap descriptor d8 : [ whirlpool . instructions . swap . d8 ] , // were successfully committed isCommitted : true , } , include : { // inner instructions innerInstructions : true , // transaction that executed the given instruction transaction : true , // all token balance records of executed transaction transactionTokenBalances : true } , } ) . build ( ) ; processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { // Simply output all the items in the batch. // It is guaranteed to have all the data matching the data requests, // but not guaranteed to not have any other data. ctx . log . info ( ctx . blocks , "Got blocks" ) ; } ) Edit this page Previous Rewards Next Typegen Data item types and field selectors Instruction Transaction LogMessage Balance TokenBalance Reward Block header A complete example
Tokenomics | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Tokenomics On this page Tokenomics SQD is the ERC-20 protocol token that is native to the SQD Network ecosystem. The token smart contract is to be deployed on the Ethereum mainnet and bridged to Arbitrum One. This strategy seeks to ensure the blockchain serves as a reliable, censorship-resistant, and verifiably impartial ledger, facilitating reward settlements and managing access to network resources. Token Utility ​ The SQD token is a critical component of the SQD ecosystem. Use cases for the SQD token are focused on streamlining and securing network operations in a permissionless manner: Alignment of incentives for infrastructure providers: SQD is used to reward node operators that contribute computation and storage resources to the network. Curation of network participants: Via delegation, the SQD token design includes built-in curation of nodes, facilitating permissionless selection of trustworthy operators for rewards. Fair resource consumption: By locking SQD tokens, consumers of data from the decentralized data lake may increase rate limits. Network decision making: SQD tokenholders can participate in governance, and are enabled to vote on protocol changes and other proposals. The SQD token’s innovative curation component allows the SQD community to delegate SQD to Node Operators of their choice, ensuring trustlessness. SQD’s utility as a tool for adjusting rate limits is unique in increasing trustless performance, by locking SQD tokens, without having to pay a centralized provider for quicker or more efficient data access. Category % of Total Supply Description Pre-Seed Backers 12 SQD’s earliest backers from the beginning of 2021. 6-month lockup after TGE, with a 20% release followed by a 24-month linear vesting for the remaining 80%. Seed Backers 16.3 Early backers from late 2021. 6-month lockup after TGE, with a 20% release followed by a 24-month linear vesting for the remaining 80%. Strategic I Backers 4.6 Strategic backers from early 2022. 12-month linear vesting after TGE. Strategic II Backers 2 Strategic backers from 2022. 6-month lockup after TGE, with a 25% release followed by an 18-month linear vesting for the remaining 75%. Team 15 6-month lockup after TGE, with a 20% release followed by a 24-month linear vesting for the remaining 80%. Reserved Treasury 28.1 Allocated for network development, grant programs, bounties, and ecosystem activities. 36-month linear vesting after TGE. Liquid Treasury 5 Liquid tokens for market making, liquidity provision on CEX/DEX, other ecosystem activities and events. Worker Rewards 10 Allocated to reward network node operators. 84-month vesting after TGE. Community Sale 5 20% release at TGE, followed by a 6-month linear vesting. Testnet Participants 1 Allocated to reward early adopters of the network who participated in the testnet on CoinList. 6-month linear vesting after TGE. Testnet Worker 1 Allocated to reward early node operators in the network who participated in the testnet on CoinList. 12-months lockup after TGE, followed by a 24-month linear vesting. *Please note that all figures are approximate based on date estimates, and may be impacted by alternate dates and other events. Edit this page Previous FAQ Next Participate Token Utility
EvmBatchProcessor reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Processor architecture EVM Block data for EVM General settings Event logs Transactions Storage state diffs Traces Field selection Substrate Data sinks Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Processors EVM EvmBatchProcessor reference documentation 📄️ Block data for EVM Block data for EVM 📄️ General settings Data sourcing and metrics 📄️ Event logs Subscribe to event data with addLog() 📄️ Transactions Subscribe to txn data with addTransaction() 📄️ Storage state diffs Track storage changes with addStateDiff() 📄️ Traces Retrieve execution traces with addTrace() 📄️ Field selection Fine-tuning data requests with setFields() Previous Processor architecture Next Block data for EVM
External tools | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions External tools On this page External tools @belopash/typeorm-store ​ @belopash/typeorm-store is a fork of @subsquid/typeorm-store that automates collecting read and write database requests into batches and caches the available entity records in RAM. Unlike the standard typeorm-store , @belopash's store is intended to be used with declarative code: it makes it easy to write mapping functions (e.g. event handlers) that explicitly define what data you're going to need from the database what code has to be executed once the data is available how to save the results Data dependencies due to entity relations are handled automatically, along with the caching of intermediate resultsg and in-memory batching of database requests. See this repository for a minimal example. DipDup ​ DipDup is a Python indexing framework that can use SQD Network as a data source. It offers SQLite, PostgreSQL and TimescaleDB data sinks GraphQL APIs based on Hasura Development workflow uses the dipdup tool to generate a stub project. Once done with that, all you have to do is to define the data schema and the handlers. Take a look at their quickstart for more details. With its handler-based architecture and the choice of Python as the transform logic language, DipDup is easier to develop for than Squid SDK , but has higher requirements on database IO bandwith and CPU. The IO bandwidth issue is partially solved by DipDup's caching layer used for database access. Edit this page Previous sqd whoami Next Glossary @belopash/typeorm-store DipDup
Simple Substrate squid | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials Simple Substrate squid On this page Simple Substrate squid Objective ​ The goal of this tutorial is to guide you through creating a simple blockchain indexer ("squid") using Squid SDK. In this example we will query the Crust storage network . Our objective will be to observe which files have been added and deleted from the network. Additionally, our squid will be able to tell us the groups joined and the storage orders placed by a given account. We will start with the substrate squid template, then go on to run the project, define a schema, and generate TypeScript interfaces. From there, we will be able to interact directly with SQD Network, using the and the metadata service to get a Crust types bundle. We expect that experienced software developers should be able to complete this tutorial in around 10-15 minutes . Pre-requisites ​ Familiarity with Git A properly set up development environment consisting of Node.js, Git and Docker Squid CLI Scaffold with sqd init ​ Use sqd init and come up with some unique name for your squid. This tutorial will index data on Crust, a Substrate-based network, so use the substrate template: sqd init substrate-crust-tutorial --template substrate cd substrate-crust-tutorial Run the project ​ Now you can follow the quickstart guide to get the project up and running. Here is a summary: npm i npm run build docker compose up -d npx squid-typeorm-migration apply node -r dotenv/config lib/main.js # will begin ingesting blocks # open a separate terminal for this next command npx squid-graphql-server # should begin listening on port 4350 After this test, shut down both processes with Ctrl-C and proceed. Define the schema and generate entity classes ​ Next, we make changes to the data schema of the squid and define entities that we would like to track. We are interested in: Files added to and deleted from the chain; Active accounts; Groups joined by accounts; Storage orders placed by accounts. For this, we use the following schema.graphql : schema.graphql type Account @entity { id : ID ! #Account address workReports : [ WorkReport ] @derivedFrom ( field : "account" ) joinGroups : [ JoinGroup ] @derivedFrom ( field : "member" ) storageOrders : [ StorageOrder ] @derivedFrom ( field : "account" ) } type WorkReport @entity { id : ID ! #event id account : Account ! addedFiles : [ [ String ] ] deletedFiles : [ [ String ] ] extrinsicHash : String createdAt : DateTime blockNum : Int ! } type JoinGroup @entity { id : ID ! member : Account ! owner : String ! extrinsicHash : String createdAt : DateTime blockNum : Int ! } type StorageOrder @entity { id : ID ! account : Account ! fileCid : String ! extrinsicHash : String createdAt : DateTime blockNum : Int ! } Notice that the Account entity is almost completely derived . It is there to tie the other three entities together. To finalize this step, run the codegen tool: npx squid-typeorm-codegen This will automatically generate TypeScript entity classes for our schema. They can be found in the src/model/generated folder of the project. Generate TypeScript wrappers for events ​ We generate these using the squid-substrate-typegen tool. Its configuration file is typegen.json ; there, we need to Set the "specVersions" field to a valid source of Crust chain runtime metadata. We'll use an URL of SQD-maintained metadata service: "specVersions" : "https://v2.archive.subsquid.io/metadata/crust" , List all Substrate pallets we will need the data from. For each pallet we list all events, calls, storage items and constants needed. info Refer to this note if you are unsure what Substrate data to use in your project. Our final typegen.json looks like this: typegen.json { "outDir" : "src/types" , "specVersions" : "https://v2.archive.subsquid.io/metadata/crust" , "pallets" : { "Swork" : { "events" : [ "WorksReportSuccess" , "JoinGroupSuccess" ] } , "Market" : { "events" : [ "FileSuccess" ] } } } Once done with the configuration, we run the tool with npx squid-substrate-typegen ./typegen.json The generated Typescript wrappers are at src/types . Set up the processor object ​ The next step is to create a SubstrateBatchProcessor object which subscribes to all the events we need. We do it at src/processor.ts : src/processor.ts import { SubstrateBatchProcessor , SubstrateBatchProcessorFields , DataHandlerContext } from '@subsquid/substrate-processor' import { events } from './types' // the wrappers generated in previous section const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/crust' ) . setRpcEndpoint ( { url : 'https://crust.api.onfinality.io/public' , rateLimit : 10 } ) . setBlockRange ( { from : 583000 } ) . addEvent ( { name : [ events . market . fileSuccess . name , events . swork . joinGroupSuccess . name , events . swork . worksReportSuccess . name ] , call : true , extrinsic : true } ) . setFields ( { extrinsic : { hash : true } , block : { timestamp : true } } ) type Fields = SubstrateBatchProcessorFields < typeof processor > export type ProcessorContext < Store > = DataHandlerContext < Store , Fields > This creates a processor that Uses SQD Network as its main data source and a chain RPC for real-time updates . URLs of the SQD Network gateways are available on this page and via sqd gateways . See this page for the reference on data sources configuration; Subscribes to Market.FileSuccess , Swork.JoinGroupSuccess and Swork.WorksReportSuccess events emitted at heights starting at 583000; Additionally subscribes to calls that emitted the events and the corresponding extrinsics; Requests the hash data field for all retrieved extrinsics and the timestamp field for all block headers. We also export the ProcessorContext type to be able to pass the sole argument of the batch handler function around safely. Define the batch handler ​ Squids batch process chain data from multiple blocks. Compared to the handlers approach this results in a much lower database load. Batch processing is fully defined by processor's batch handler , the callback supplied to the processor.run() call at the entry point of each processor ( src/main.ts by convention). We begin defining our batch handler by importing the entity model classes and Crust event types that we generated in previous sections. We also import the processor and its types: src/main.ts import { Account , WorkReport , JoinGroup , StorageOrder } from './model' import { processor , ProcessorContext } from './processor' Let's skip for now the process.run() call - we are going to come back to it in a second - and scroll down to the getTransferEvents function. In the template repository this function loops through the items contained in the context, extracts the events data and stores it in a list of objects. For this project we are still going to extract events data from the context, but this time we have more than one event type so we have to sort them. We also need to handle the account information. Let's start with deleting the TransferEvent interface and defining this instead: type Tuple < T , K > = [ T , K ] interface EventsInfo { joinGroups : Tuple < JoinGroup , string > [ ] marketFiles : Tuple < StorageOrder , string > [ ] workReports : Tuple < WorkReport , string > [ ] accountIds : Set < string > } Now, let's replace the getTransferEvents function with the below snippet that extracts event information in a manner specific to its name (known from e.name ); stores event information in an object (we are going to use entity classes for that) and extracts accountId s from it; store all accountId s in a set. import { toHex } from '@subsquid/substrate-processor' import * as ss58 from '@subsquid/ss58' function getEventsInfo ( ctx : ProcessorContext < Store > ) : EventInfo { let eventsInfo : EventsInfo = { joinGroups : [ ] , marketFiles : [ ] , workReports : [ ] , accountIds : new Set < string > ( ) } for ( let block of ctx . blocks ) { const blockTimestamp = block . header . timestamp ? new Date ( block . header . timestamp ) : undefined for ( let e of block . events ) { if ( e . name === events . swork . joinGroupSuccess . name ) { const decoded = events . swork . joinGroupSuccess . v1 . decode ( e ) const memberId = ss58 . codec ( 'crust' ) . encode ( decoded [ 0 ] ) eventsInfo . joinGroups . push ( [ new JoinGroup ( { id : e . id , owner : ss58 . codec ( 'crust' ) . encode ( decoded [ 1 ] ) , blockNum : block . header . height , createdAt : blockTimestamp , extrinsicHash : e . extrinsic ?. hash } ) , memberId ] ) // add encountered account ID to the set of unique accountIDs eventsInfo . accountIds . add ( memberId ) } if ( e . name === events . market . fileSuccess . name ) { const decoded = events . market . fileSuccess . v1 . decode ( e ) const accountId = ss58 . codec ( 'crust' ) . encode ( decoded [ 0 ] ) eventsInfo . marketFiles . push ( [ new StorageOrder ( { id : e . id , fileCid : toHex ( decoded [ 1 ] ) , blockNum : block . header . height , createdAt : blockTimestamp , extrinsicHash : e . extrinsic ?. hash } ) , accountId ] ) eventsInfo . accountIds . add ( accountId ) } if ( e . name === events . swork . worksReportSuccess . name ) { const decoded = events . swork . worksReportSucces . v1 . decode ( e ) const accountId = ss58 . codec ( 'crust' ) . encode ( decoded [ 0 ] ) const addedExtr = e . call ?. args . addedFiles const deletedExtr = e . call ?. args . deletedFiles const addedFiles = addedExtr . map ( v => v . map ( ve => String ( ve ) ) ) const deletedFiles = deletedExtr . map ( v => v . map ( ve => String ( ve ) ) ) if ( addedFiles . length > 0 || deletedFiles . length > 0 ) { eventsInfo . workReports . push ( [ new WorkReport ( { id : e . id , addedFiles : addedFiles , deletedFiles : deletedFiles , blockNum : block . header . height , createdAt : blockTimestamp , extrinsicHash : e . extrinsic ?. hash , } ) , accountId ] ) eventsInfo . accountIds . add ( accountId ) } } } } return eventsInfo } Next, we want to create an entity ( Account ) object for every accountId in the set, then add the Account information to every event entity object. Finally, we save all the created and modified entity models into the database. Take the code inside processor.run() and change it so that it looks like this: processor . run ( new TypeormDatabase ( ) , async ( ctx ) => { const eventsInfo = getEventsInfo ( ctx ) let accounts = await ctx . store . findBy ( Account , { id : In ( [ ... eventsInfo . accountIds ] ) } ) . then ( accounts => new Map ( accounts . map ( a => [ a . id , a ] ) ) for ( let aid of eventsInfo . accountIds ) { if ( ! accounts . has ( aid ) ) { accounts . set ( aid , new Account ( { id : aid } ) ) } } for ( const jg of eventsInfo . joinGroups ) { // necessary to add this field to the previously created model // because now we have the Account created. jg [ 0 ] . member = accounts . get ( jg [ 1 ] ) } for ( const mf of eventsInfo . marketFiles ) { mf [ 0 ] . account = accounts . get ( mf [ 1 ] ) } for ( const wr of eventsInfo . workReports ) { wr [ 0 ] . account = accounts . get ( wr [ 1 ] ) } await ctx . store . save ( [ ... accounts . values ( ) ] ) ; await ctx . store . insert ( eventsInfo . joinGroups . map ( el => el [ 0 ] ) ) ; await ctx . store . insert ( eventsInfo . marketFiles . map ( el => el [ 0 ] ) ) ; await ctx . store . insert ( eventsInfo . workReports . map ( el => el [ 0 ] ) ) ; } ) Apply changes to the database ​ Squid projects automatically manage the database connection and schema via an ORM abstraction provided by TypeORM . Previously we changed the data schema at schema.graphql and reflected these changes in our Typescript code using npx squid-typeorm-codegen . Here, we apply the corresponding changes to the database itself . First, we'll need to compile our updated project code. Do this with: npm run build Next we ensure that the database is at blank state: docker compose down docker compose up -d Then we replace any old migrations with the new one with rm -r db/migrations npx squid-typeorm-migration generate The new migration will be generated from the TypeORM entity classes we previously made out of schema.graphql . Apply it with: npx squid-typeorm-migration apply Launch the project ​ It's finally time to run the project! Run node -r dotenv/config lib/main.js in one terminal, then open another one and run npx squid-graphql-server Now you can see the results of our hard work by visiting localhost:4350/graphql in a browser and accessing the GraphiQL console. From this window we can perform queries. This one displays info on ten latest work reports, including all involved files and the account id: query MyQuery { workReports ( limit : 10 , orderBy : blockNum_DESC ) { account { id } addedFiles deletedFiles } } Credits ​ This sample project is adapted from a real integration, developed by our very own Mikhail Shulgin . Credit for building it and helping with the guide goes to him. Edit this page Previous Use with Ganache or Hardhat Next ink! contract indexing Objective Pre-requisites Scaffold with sqd init Run the project Define the schema and generate entity classes Generate TypeScript wrappers for events Set up the processor object Define the batch handler Apply changes to the database Launch the project Credits
SQD Cloud reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Reference Public gateways EVM API Substrate API Starknet API Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Reference SQD Cloud reference documentation üìÑÔ∏è Public gateways Datasets for various blockchains üìÑÔ∏è EVM API Access the data of EVM blockchains üìÑÔ∏è Substrate API Access the data of Substrate blockchains üìÑÔ∏è Starknet API Access the Starknet data Previous Self-host a portal Next Public gateways
Inputs | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource Inputs On this page Inputs addInput(options) ​ Get some or all inputs on the network. options has the following structure: { // data requests type ? : InputType [ ] coinOwner ? : string [ ] coinAssetId ? : string [ ] contractContract ? : string [ ] messageSender ? : string [ ] messageRecipient ? : string [ ] // related data retrieval transaction ? : boolean range ? : { from : number to ? : number } } Data requests: type sets the type of the input. You can request one or more of 'InputCoin' | 'InputContract' | 'InputMessage' . Leave it undefined to subscribe to all inputs. Enabling the transaction flag will cause the processor to retrieve transactions where the selected inputs have occurred. The data will be added to the appropriate iterables within the block data . You can also call augmentBlock() from @subsquid/fuel-objects on the block data to populate the convenience reference fields like input.transaction . Note that inputs can also be requested by the other FuelDataSource methods as related data. Selection of the exact fields to be retrieved for each transaction and the optional related data items is done with the setFields() method documented on the Field selection page. Examples ​ Request all inputs with InputCoin type and include transactions: processor . addInput ( { type : [ "InputCoin" ] , transaction : true , } ) . build ( ) ; Edit this page Previous General settings Next Outputs Examples
Run a gateway | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Overview Whitepaper FAQ Tokenomics Participate Procuring SQD Delegate Run a worker Run a gateway Self-host a portal Reference Portal beta info Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Network Participate Run a gateway On this page Run a gateway warning Public SQD Network gateways are now deprecated. The new (also decentralized) way to access the data will be announced soon. Running a gateway enables you to access SQD Network data without relying on any centralized services. You can run a private gateway for your own needs, or make a high-throughput public gateway. In either scenario you will need a working Docker installation some SQD tokens (in your wallet or in a special vesting contract) some Arbitrum ETH (for gas) Hardware requirements depend on how the gateway will be used. A private gateway with a single user can run on a laptop. Staking requirements and compute units ​ All gateways have to be registered on-chain and have a SQD stake associated with them to begin serving requests. Size and duration of the stake determine the rate at which the gateway can serve requests. The exact rate limiting mechanism is based on partitioning time into epochs . Workers periodically checks if a new epoch has started (once every 30 seconds by default). When a worker receives its first (on the current epoch) request from a gateway, it makes an on-chain query for how many compute units (CUs) are allocated to the worker due to any stakes associated with the gateway. The worker keeps the number and decrements it every time it receives more requests from the same gateway. When the CUs are exhausted it begins replying to any new requests from the gateway with HTTP 429 errors. The cycle repeats once the worker detects that a new epoch has begun. At present, any single data request to worker spends 1 CU. See e.g. EVM worker API to get an idea as to what the requests may look like. By default, the contract allocates the same amount of CUs for each worker, namely (SQD_AMOUNT * EPOCH_LENGTH * BOOST_FACTOR) / (NUM_WORKERS * NUM_GATEWAYS) per epoch. Here, EPOCH_LENGTH is 100 and BOOST_FACTOR depends on the duration of staking, varying from 1 (stakes under 60 days) to 3 (720 or more days). See Data consumers and Boosters sections of the whitepaper. NUM_WORKERS is the number of workers active on the network . NUM_GATEWAYS is the number of gateways associated with the wallet that made the stake. info It is possible to allocate CUs to workers selectively, e.g. to get more bandwidth on some datasets that are served by a subset workers. Currently there's only a low-level interface for this feature. If you're interested, please let us know in SquidDevs Telegram chat . For example, if you expect your gateway to make up to 36k requests to any worker at any epoch, and the network currently has 1000 workers, you will need to stake 30000 SQD if you choose the shortest staking duration and 10000 SQD if you stake for two years or more. tip By default, your gateway will go down at the end of the staking period. To prevent that, enable the "Auto-extension" option when staking. This will cause your SQD to be immediately restaked once the staking period ends. In this setup you have to unstake then wait for the end of the current staking period to withdraw your tokens. Running a gateway ​ Generate your key file by running docker run --rm subsquid/rpc-node:0.2.5 keygen > < KEY_PATH > The command will display your peer ID: Your peer ID: <THIS IS WHAT YOU NEED TO COPY> Please copy this ID, as it will be needed for further steps. ⚠️ Note: Please make sure that the generated file is safe and secure at <KEY_PATH> (i.e. it will not be deleted accidentally and cannot be accessed by unauthorized parties). Or else . Go to https://network.subsquid.io . Connect your EVM wallet (we recommend using Metamask). Use the wallet that holds the tokens or is the beneficiary of your vesting contract. Go to the "Gateways" tab and click the "Add gateway" button. Fill the gateway registration form. If you plan to make your gateway public, click the "Publicly available" switch and populate the additional fields. Once done, click "Register" and confirm the transaction. Go to the "Gateways" tab and click the Add lock button. Make a SQD stake of size and duration appropriate for the planned bandwidth of your gateway (see Staking requirements and compute units ). Wait for your stake to become active. This will happen at the beginning of the next epoch . Clone the gateway repo and enter the folder. git clone https://github.com/subsquid/query-gateway cd query-gateway Prepare the environment. Begin with cp config.yml.mainnet config.yml cp mainnet.env .env then set the path to your key file: echo KEY_PATH = < KEY_PATH > >> .env ⚠️ Warning: Be careful when supplying the path to the key you created at step 4. If you make a mistake here, a new random key will be automatically created there and your node will attempt to operate with a new (unregistered) peer ID - unsuccessfully. Run your gateway. You can either utilize a pre-built Docker image: docker compose up -d or build it from source: cargo run --release If you're running the Docker image you can watch the logs with docker compose logs -f High throughput gateways ​ The recommended way to make a gateway that can serve a large number of requests is deploy multiple gateways associated with a single wallet: Use a single wallet to register as many peer IDs as you need gateway instances. Make one SQD stake for all your future gateway instances. See Staking requirements and compute-units to get an idea of how large your stake should be. Run gateways in parallel, balancing traffic between the instances. If you plan to automate running your gateway instances, you may find this helm chart useful. Troubleshooting ​ What are the consequences of losing my key file / getting it stolen? ​ If you lose your key file you won't be able to run your gateway until you get a new one and register it. If your key file get stolen the perpetrator will be able to cause connectivity issues for your gateway, effectively causing a downtime. If any of that happens, unregister your gateway (on the "Gateways" tab of network.subsquid.io ), then generate a new key file and register the new gateway peer ID. Edit this page Previous Run a worker Next Self-host a portal Staking requirements and compute units Running a gateway High throughput gateways Troubleshooting
Overview | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Overview On this page Overview A squid is an indexing project built with Squid SDK to retrieve and process blockchain data from the SQD Network (either permissioned or decentralized instance). The Squid SDK is a set of open source Typescript libraries that retrieve, decode, transform and persist the data. It can also make the transformed data available via an API. All stages of the indexing pipeline, from the data extraction to transformation to persistence are performed on batches of blocks to maximize the indexing speed. Modular architecture of the SDK makes it possible to extend indexing projects (squids) with custom plugins and data targets. Required squid components ​ Processor ​ Processor is the word used for The main NodeJS process of the squid. The main object ( processor ) of this process: its method call processor.run() is the entry point. processor objects handle data retrieval and transformation; data persistence is handled by a separate object called Store . Squid SDK offers two processor classes: EvmBatchProcessor via the @subsquid/evm-processor NPM package - for Ethereum-compatible networks SubstrateBatchProcessor via @subsquid/substrate-processor - for networks based on Substrate such as Polkadot Store ​ A store is an object that processors use to persist their data. SQD offers three store classes: TypeormStore for saving data to PostgreSQL, via @subsquid/typeorm-store @subsquid/typeorm-codegen (a code generator, install with --save-dev ) @subsquid/typeorm-migration Install all three packages to use this store. file-store via @subsquid/file-store - for saving data to filesystems. It is a modular system with a variety of extensions for various formats and destinations. bigquery-store via @subsquid/bigquery-store - for saving data to Google BigQuery . You can mix and match any store class with any processor class. Optional squid components ​ Typegen ​ A typegen is a tool for generating utility code for technology-specific operations such as decoding. Here are the typegens available: On EVM On Substrate squid-evm-typegen via @subsquid/evm-typegen : decodes smart contract data handles direct calls to contract methods exposes useful constants such as event topics and function signature hashes
The generated code depends on @subsquid/evm-abi , SQD's own high performance, open source EVM codec. squid-substrate-typegen via @subsquid/substrate-typegen : general purpose pallet data decoding (aware of runtime versions) handles direct storage queries squid-ink-typegen via @subsquid/ink-typegen for decoding the data of ink! contracts Install these with --save-dev . GraphQL server ​ Squids that store their data in PostgreSQL can subsequently make it available as a GraphQL API via a variety of supported servers. See Serving GraphQL . Among other alternatives, SQD provides its own server called OpenReader via the @subsquid/graphql-server package. The server runs as a separate process. Core API is automatically derived from the schema file; it is possible to extend it with custom queries and basic access control . Misc utilities ​ On EVM On Substrate Squid CLI is a utility for retrieving squid templates , managing chains of commands commonly used in development and running all squid processes at once . It can also be used for deploying to SQD Cloud . Squid CLI is a utility for retrieving squid templates , managing chains of commands commonly used in development and running all squid processes at once . It can also be used for deploying to SQD Cloud . @subsquid/ss58 handles encoding and decoding of SS58 addresses @subsquid/frontier decodes events and calls of the Frontier EVM pallet to make them decodable with squid-evm-typegen Edit this page Previous Quickstart Next Getting started Required squid components Processor Store Optional squid components Typegen GraphQL server Misc utilities
Under construction | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Under construction Under construction If you're here you clicked on a link that we haven't filled yet. Those do not last long - check back in a day or two! Edit this page
An open source GraphQL server built by SQD | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks Logger Schema file OpenReader Overview Configuration Core API The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference OpenReader An open source GraphQL server built by SQD üìÑÔ∏è Overview An open source GraphQL server built by SQD üóÉÔ∏è Configuration 5 items üóÉÔ∏è Core API 9 items Previous Interfaces Next Overview
Substrate data sourcing | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific ink! contracts support Frontier EVM support Gear support Substrate data sourcing Substrate types bundles Tools Migration guides Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Substrate-specific Substrate data sourcing On this page Substrate data sourcing How do I know which events and calls I need on Substrate? â€‹ This part depends on the runtime business logic of the chain. The primary and the most reliable source of information is thus the Rust sources for the pallets used by the chain. For a quick lookup of the documentation and the data format, it is often useful to check Runtime section of Subscan (e.g. Statemine ). One can see the deployed pallets and drill down to events and extrinsics from there. One can also choose the spec version on the drop down. Edit this page Previous Gear support Next Substrate types bundles How do I know which events and calls I need on Substrate?
SQD Firehose | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions SQD Firehose On this page Run subgraphs without full nodes warning This tutorial uses alpha-quality software. If you encouter any issues while using it please let us know at the SquidDevs Telegram chat . Dependencies : Docker, Git, NodeJS, Yarn. Developing and running subgraphs is hard, as one has to run a full archival node, and for many networks it is not feasible to run a full archival node to at all. SQD Firehose is an open-source lightweight adapter run as a side-car to a graph indexer node, ingesting and filtering the data directly from SQD Network instead of an RPC endpoint. However, since the network does not provide the real-time blocks, the most recent and unfinalized blocks are (optionally) ingested from a complementary RPC endpoint in a seamless way. Currently it is only possible to run subgraphs against a production-ready permissioned SQD Network instance (formerly known as Subsquid Archives). Running against the decentralized SQD Network is scheduled to be supported in Q2 2024 (see SQD Network Overview to learn more about the difference). The easiest way to run a subgraph with SQD Firehose to use our graph-node-setup repo. Here's how: Clone the repo and install the dependencies: git clone https://github.com/subsquid-labs/graph-node-setup cd graph-node-setup npm ci Interactively configure the environment with npm run configure You will be asked to select a network. You can pick any network from our supported EVM networks ; networks that are not currently supported by TheGraph will be available their under SQD names. Optionally you can also provide an RPC endpoint. If you do, it will be used to sync a few thousands of blocks at the chain end, so it does not have to be a paid one. However, firehose-grpc does not limit its request rate yet, so using a public RPC might result in a cooldown. If you do not provide an RPC endpoint, your subgraph deployments will be a few thousands of blocks behind the chain head. warning Running against the decentralized testnet is not currently supported. This feature will be re-enabled in Q2 2024. Download and deploy your subgraph of choice! For example, if you configured the environment to use Ethereum mainnet ( eth-mainnet ), you can deploy the well known Gravatar subgraph: git clone https://github.com/graphprotocol/example-subgraph cd example-subgraph # the repo is a bit outdated, giving it a deps update rm yarn.lock npx --yes npm-check-updates --upgrade yarn install # generate classes for the smart contract # and events used in the subgraph npm run codegen # create and deploy the subgraph npm run create-local npm run deploy-local GraphiQL playground will be available at http://127.0.0.1:8000/subgraphs/name/example/graphql . Troubleshooting â€‹ Do not hesitate to let us know about any issues (whether listed here or not) at the SquidDevs Telegram chat . If your subgraph is not syncing and you're getting thread 'tokio-runtime-worker' panicked at 'called `Option::unwrap()` on a `None` value', src/ds_rpc.rs:556:80 errors in the graph-node-setup-firehose container logs, that likely means that the chain RPC is not fully Ethereum-compatible and a workaround is not yet implemented in firehose-grpc . You can still sync your subgraph with RPC ingestion disabled. Edit this page Previous Tron API Next ApeWorx plugin Troubleshooting
ink! contract indexing | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Indexing BAYC Index to local CSV files Index to Parquet files Use with Ganache or Hardhat Simple Substrate squid ink! contract indexing Frontier EVM-indexing squid Processor in action Case studies Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Tutorials ink! contract indexing On this page ink! contract indexing Objective ​ This tutorial starts with the substrate squid template and goes through all the necessary changes to index the events of a WASM contract developed with ink! . This approach is taken to illustrate the development process. If you want to start indexing ASAP, consider starting with the ink template that contains the final code of this tutorial: sqd init < your squid name here > --template ink Here we will use a simple test ERC20-type token contract deployed to Shibuya at XnrLUQucQvzp5kaaWLG9Q3LbZw5DPwpGn69B5YcywSWVr5w . Our squid will track all the token holders and account balances, together with the historical token transfers. info Squid SDK only supports WASM contracts executed by the Contracts pallet natively. The pallet is enabled by the following network runtimes: Astar (a Polkadot parachain) Shibuya ( Astar testnet) Shiden ( Kusama -cousin of Astar ) AlephZero (a standalone Substrate-based chain) Pre-requisites ​ Familiarity with Git A properly set up development environment consisting of Node.js, Git and Docker Squid CLI Run the template ​ Retrieve the substrate template with sqd init : sqd init ink-tutorial --template substrate cd ink-tutorial and run it: npm i npm run build docker compose up -d npx squid-typeorm-migration apply node -r dotenv/config lib/main.js # should begin to ingest blocks # open a separate terminal for this next command npx squid-graphql-server # should begin listening on port 4350 After this test, shut down both processes with Ctrl-C and proceed. Define the data schema ​ The next step is to define the target data entities and their relations at schema.graphql , then use that file to autogenerate TypeORM entity classes. We track: Wallet balances Token transfers Our schema definition for modelling this data is straightforward: # schema.graphql type Owner @entity { id : ID ! balance : BigInt ! @index } type Transfer @entity { id : ID ! from : Owner to : Owner amount : BigInt ! @index timestamp : DateTime ! @index block : Int ! } Note: a one-to-many relation between Owner and Transfer ; @index decorators for properties that we want to be able to filter the data by. Next, we generate TypeORM entity classes from the schema with the squid-typeorm-codegen tool: npx squid-typeorm-codegen The generated entity classes can be found under src/model/generated . Finally, we create database migrations to match the changed schema. We restore the database to a clean state, then replace any existing migrations with the new one: docker compose down docker compose up -d rm -r db/migrations npx squid-typeorm-migration generate Apply the migration with npx squid-typeorm-migration apply WASM ABI Tools ​ The Contracts pallet stores the contract execution logs (calls and events) in a binary format. The decoding of this data is contract-specific and is done with the help of an ABI file typically published by the contract developer. ABI for ERC20 contracts like the one we're indexing can be found here . Download that file to the abi folder and install the following two tools from Squid SDK: @subsquid/ink-abi -- A performant library for decoding binary ink! contract data. @subsquid/ink-typegen -- A tool for making TypeScript modules for handling contract event and call data based on ABIs of contracts. npm i @subsquid/ink-abi && npm i @subsquid/ink-typegen --save-dev Since @subsquid/ink-typegen is only used to generate source files, we install it as a dev dependency. Generate the contract data handling module by running npx squid-ink-typegen --abi abi/erc20.json --output src/abi/erc20.ts The generated src/abi/erc20.ts module defines interfaces to represent WASM data defined in the ABI, as well as functions necessary to decode this data (e.g. the decodeEvent function). Define the processor object ​ Squid SDK provides users with the SubstrateBatchProcessor class . Its instances connect to SQD Network gateways at chain-specific URLs to get chain data and apply custom transformations. The indexing begins at the starting block and keeps up with new blocks after reaching the tip. SubstrateBatchProcessor s exposes methods to "subscribe" them to specific data such as Substrate events, extrinsics, storage items etc. The Contracts pallet emits ContractEmitted events wrapping the logs emitted by the WASM contracts. Processor allows one to subscribe to such events emitted by a specific contract. The processor is instantiated and configured at the src/processor.ts . Here are the changes we need to make there: Change the gateway used to https://v2.archive.subsquid.io/network/shibuya-substrate . Remove the addEvent function call, and add addContractsContractEmitted instead, specifying the address of the contract we are interested in. The address should be represented as a hex string, so we need to decode our ss58 address of interest, XnrLUQucQvzp5kaaWLG9Q3LbZw5DPwpGn69B5YcywSWVr5w . Here is the end result: src/processor.ts import { assertNotNull } from '@subsquid/util-internal' import { toHex } from '@subsquid/util-internal-hex' import * as ss58 from '@subsquid/ss58' import { BlockHeader , DataHandlerContext , SubstrateBatchProcessor , SubstrateBatchProcessorFields , Event as _Event , Call as _Call , Extrinsic as _Extrinsic } from '@subsquid/substrate-processor' export const SS58_NETWORK = 'astar' // used for the ss58 prefix, astar shares it with shibuya const CONTRACT_ADDRESS_SS58 = 'XnrLUQucQvzp5kaaWLG9Q3LbZw5DPwpGn69B5YcywSWVr5w' export const CONTRACT_ADDRESS = ss58 . codec ( SS58_NETWORK ) . decode ( CONTRACT_ADDRESS_SS58 ) export const processor = new SubstrateBatchProcessor ( ) . setGateway ( 'https://v2.archive.subsquid.io/network/shibuya-substrate' ) . setRpcEndpoint ( { url : assertNotNull ( process . env . RPC_ENDPOINT ) , rateLimit : 10 } ) . addContractsContractEmitted ( { contractAddress : [ CONTRACT_ADDRESS ] , extrinsic : true } ) . setFields ( { block : { timestamp : true } , extrinsic : { hash : true } } ) . setBlockRange ( { // genesis block happens to not have a timestamp, so it's easier // to start from 1 in cases when the deployment height is unknown from : 1 } ) export type Fields = SubstrateBatchProcessorFields < typeof processor > export type Block = BlockHeader < Fields > export type Event = _Event < Fields > export type Call = _Call < Fields > export type Extrinsic = _Extrinsic < Fields > export type ProcessorContext < Store > = DataHandlerContext < Store , Fields > Define the batch handler ​ Once requested, the events can be processed by calling the .run() function that starts generating requests to SQD Network for batches of data. Every time a batch is returned by the Network, it will trigger the callback function, or batch handler (passed to .run() as second argument). It is in this callback function that all the mapping logic is expressed. This is where chain data decoding should be implemented, and where the code to save processed data on the database should be defined. Batch handler is typically defined at the squid processor entry point, src/main.ts . Here is one that works for our task: src/main.ts import { In } from 'typeorm' import assert from 'assert' import * as ss58 from '@subsquid/ss58' import { Store , TypeormDatabase } from '@subsquid/typeorm-store' import * as erc20 from './abi/erc20' import { Owner , Transfer } from "./model" import { processor , SS58_NETWORK , CONTRACT_ADDRESS , ProcessorContext } from './processor' processor . run ( new TypeormDatabase ( { supportHotBlocks : true } ) , async ctx => { const txs : TransferRecord [ ] = getTransferRecords ( ctx ) const owners : Map < string , Owner > = await createOwners ( ctx , txs ) const transfers : Transfer [ ] = createTransfers ( txs , owners ) await ctx . store . upsert ( [ ... owners . values ( ) ] ) await ctx . store . insert ( transfers ) } ) interface TransferRecord { id : string from ? : string to ? : string amount : bigint block : number timestamp : Date extrinsicHash : string } function getTransferRecords ( ctx : ProcessorContext < Store > ) : TransferRecord [ ] { const records : TransferRecord [ ] = [ ] for ( const block of ctx . blocks ) { assert ( block . header . timestamp , ` Block ${ block . header . height } had no timestamp ` ) for ( const event of block . events ) { if ( event . name === 'Contracts.ContractEmitted' && event . args . contract === CONTRACT_ADDRESS ) { assert ( event . extrinsic , ` Event ${ event } arrived without a parent extrinsic ` ) const decodedEvent = erc20 . decodeEvent ( event . args . data ) if ( decodedEvent . __kind === 'Transfer' ) { records . push ( { id : event . id , from : decodedEvent . from && ss58 . codec ( SS58_NETWORK ) . encode ( decodedEvent . from ) , to : decodedEvent . to && ss58 . codec ( SS58_NETWORK ) . encode ( decodedEvent . to ) , amount : decodedEvent . value , block : block . header . height , timestamp : new Date ( block . header . timestamp ) , extrinsicHash : event . extrinsic . hash } ) } } } } return records } async function createOwners ( ctx : ProcessorContext < Store > , txs : TransferRecord [ ] ) : Promise < Map < string , Owner >> { const ownerIds = new Set < string > ( ) txs . forEach ( tx => { if ( tx . from ) { ownerIds . add ( tx . from ) } if ( tx . to ) { ownerIds . add ( tx . to ) } } ) const ownersMap = await ctx . store . findBy ( Owner , { id : In ( [ ... ownerIds ] ) } ) . then ( owners => { return new Map ( owners . map ( owner => [ owner . id , owner ] ) ) } ) return ownersMap } function createTransfers ( txs : TransferRecord [ ] , owners : Map < string , Owner > ) : Transfer [ ] { return txs . map ( tx => { const transfer = new Transfer ( { id : tx . id , amount : tx . amount , block : tx . block , timestamp : tx . timestamp , extrinsicHash : tx . extrinsicHash } ) if ( tx . from ) { transfer . from = owners . get ( tx . from ) if ( transfer . from == null ) { transfer . from = new Owner ( { id : tx . from , balance : 0n } ) owners . set ( tx . from , transfer . from ) } transfer . from . balance -= tx . amount } if ( tx . to ) { transfer . to = owners . get ( tx . to ) if ( transfer . to == null ) { transfer . to = new Owner ( { id : tx . to , balance : 0n } ) owners . set ( tx . to , transfer . to ) } transfer . to . balance += tx . amount } return transfer } ) } The getTransferRecords function generates a list of TransferRecord objects that contain the data we need to fill the models we have defined with our schema. This data is extracted from the events found in the batch context, ctx . We use the in the main body of the batch handler, the arrow function used as the second argument of the .run() function call, to fetch or create Owner instance and create a Transfer instance for every event found in the context. Finally, these TypeORM entity instances are saved to the database, all in one go. This is done to reduce the number of database queries. info In the getTransferRecords function we loop over the blocks and over the events contained in them, then filter the events with an if . The filtering is redundant when there's only one event type to process but will be needed when the processor is subscribed to multiple ones. Launch the Project ​ Build, then launch the processor with npm run build node -r dotenv/config lib/main.js This will block the current terminal. In a separate terminal window, launch the GraphQL server: npx squid-graphql-server Visit localhost:4350/graphql to access the GraphiQl console. There you can perform queries such as this one, to find out the account owners with the biggest balances: query MyQuery { owners ( limit : 10 , where : { } , orderBy : balance_DESC ) { balance id } } Or this other one, looking up the largest transfers: query MyQuery { transfers ( limit : 10 , orderBy : amount_DESC ) { amount block id timestamp to { balance id } from { balance id } } } Have fun playing around with queries, after all, it's a playground ! Edit this page Previous Simple Substrate squid Next Frontier EVM-indexing squid Objective Pre-requisites Run the template Define the data schema WASM ABI Tools Define the processor object Define the batch handler Launch the Project
You've been rate limited | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions You've been rate limited You've been rate limited If you are seeing this page, chances are that your squids used SQD's public portal and got rate limited. This happens when you make more than 50 requests in any 10 seconds long interval. Here are your options: Set up a private SQD Network portal with or without the support for real time data (see below). Migrate to SQD Cloud where we'll manage the portal for you. Do nothing and accept the reduced sync rate. Edit this page
FuelDataSource reference documentation | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Indexing Fuel Network data FuelDataSource Block data for Fuel Network General settings Inputs Outputs Receipts Transactions Field selection Cheatsheet Network API Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Fuel indexing FuelDataSource FuelDataSource reference documentation 📄️ Block data for Fuel Network Block data for Fuel 📄️ General settings Data sourcing and metrics 📄️ Inputs Subscribe to Input data with addInput() 📄️ Outputs Subscribe to outputs data with addOutput() 📄️ Receipts Subscribe to txn data with addTransaction() 📄️ Transactions Subscribe to txn data with addTransaction() 📄️ Field selection Fine-tuning data requests with setFields() Previous Indexing Fuel Network data Next Block data for Fuel Network
Tools for saving squid data to file-based datasets locally or on S3 | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Tutorials Reference Processors Data sinks typeorm-store file-store extensions CSV support Parquet support JSON support S3 support bigquery-store Logger Schema file OpenReader The frontier package Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Reference Data sinks file-store extensions Tools for saving squid data to file-based datasets locally or on S3 üìÑÔ∏è CSV support Table class for writing CSV files üìÑÔ∏è Parquet support Table class for writing Apache Parquet files üìÑÔ∏è JSON support Table class for writing JSON and JSONL files üìÑÔ∏è S3 support A Dest class for uploading data to buckets Previous typeorm-store Next CSV support
sqd init | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI Installation commands.json sqd auth sqd autocomplete sqd deploy sqd explorer sqd gateways sqd init sqd list sqd logs prod sqd remove sqd restart sqd run sqd secrets sqd tags sqd whoami External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Squid CLI sqd init On this page sqd init Setup a new squid project from a template or github repo sqd init NAME sqd init NAME â€‹ Setup a new squid project from a template or github repo USAGE $ sqd init NAME [--interactive] [-t <value>] [-d <value>] [-r] ARGUMENTS NAME  The squid name. It must contain only alphanumeric or dash ("-") symbols and must not start with "-". FLAGS -d, --dir=<value> The target location for the squid. If omitted, a new folder NAME is created. -r, --remove Clean up the target directory if it exists -t, --template=<value> A template for the squid. Accepts: - a github repository URL containing a valid squid.yaml manifest in the root folder or one of the pre-defined aliases: - evm  A minimal squid template for indexing EVM data. - abi  A template to auto-generate a squid indexing events and txs from a contract ABI - multichain  A template for indexing data from multiple chains - gravatar  A sample EVM squid indexing the Gravatar smart contract on Ethereum. - substrate  A template squid for indexing Substrate-based chains. - ink  A template for indexing Ink! smart contracts - ink-abi  A template to auto-generate a squid from an ink! contract ABI - frontier-evm  A template for indexing Frontier EVM chains, like Moonbeam and Astar. --[no-]interactive Disable interactive mode See code: src/commands/init.ts Edit this page Previous sqd gateways Next sqd list sqd init NAME
Migrate from The Graph | SQD Skip to main content Current Current All versions Search Home Source data from a portal Overview SQD Network Indexing SDK Quickstart Overview Getting started Features & Guides Batch processing RPC ingestion and reorgs External APIs and IPFS Multichain Serving GraphQL Self-hosting Persisting data EVM-specific Substrate-specific Tools Migration guides Migrate from The Graph ArrowSquid for EVM ArrowSquid for Substrate hasura-configuration tool v2 Tutorials Reference Troubleshooting Examples FAQ SQD vs The Graph SQD Cloud Solana indexing Fuel indexing Tron indexing SQD Firehose ApeWorx plugin Squid CLI External tools Glossary Migrate to the Cloud portal Conduit integration Under construction SQD Cloud deployments, now upgraded! On the FireSquid release Indexing Hyperliquid with SQD Rules SQD Portal Closed Beta Instructions You've been rate limited SQD Portal, now in closed beta. SQD Boost Program Versions Indexing SDK Features & Guides Migration guides Migrate from The Graph On this page Migrate from The Graph This guide walks through the steps to migrate a subgraph to SQD. In what follows we will convert the Gravatar subgraph into a squid and run it locally. Impatient readers may clone the squid from the repo and run it by following the instructions in README: git clone https://github.com/subsquid-labs/gravatar-squid.git EvmBatchProcessor provided by the Squid SDK defines a single handler that indexes EVM logs and transaction data in batches. It differs from the programming model of subgraph mappings that defines a separate data handler for each EVM log topic to be indexed. Due to significantly less frequent database hits (once per batch compared to once per log) the batch-based handling model shows up to a 10x increase in the indexing speed. At the same time, concepts of the schema file , code generation from the schema file and auto-generated GraphQL API should be familiar to subgraph developers. In most cases the schema file of a subgraph can be imported into a squid as is. There are some known limitations: Many-to-Many entity relations should be modeled explicitly as two many-to-one relations On top of the features provided by subgraphs, Squid SDK and SQD Cloud offer extra flexibility in developing tailored APIs and ETLs for on-chain data: Full control over the target database (Postgres), including custom migrations and ad-hoc queries in the handler Custom target databases and data formats (e.g. CSV) Arbitrary code execution in the data handler Extension of the GraphQL API with arbitrary SQL Secret environment variables , allowing to seamlessly use private third-party JSON-RPC endpoints and integrate with external APIs Deployment slots and tags API caching For a full feature set comparison, see SQD vs The Graph . Squid setup ​ 1. Install Squid CLI ​ Instructions here . 2. Fetch the template ​ Create a squid from the minimalistic evm template : sqd init my-new-squid -t evm cd squid-evm-template 3. Copy the schema file and generate entities ​ The minimal template already contains a dummy schema.graphql file. We replace it with the subgraph schema as is: type Gravatar @entity { id: ID! owner: Bytes! displayName: String! imageUrl: String! } Next, we generate the entities from the schema using the squid-typeorm-codegen tool of the Squid SDK, then build the squid: npx squid-typeorm-codegen npm run build This command is equivalent to running yarn codegen in subgraph. After that, start the local database and regenerate migrations based on the generated entities using the squid-typeorm-migration tool: docker compose up -d rm -r db/migrations npx squid-typeorm-migration generate A database migration file for creating a table for Gravatar will appear in db/migrations . Apply it with npx squid-typeorm-migration apply 4. Generate typings from ABI ​ Copy ./abis/Gravity.json from the subgraph project and paste it to ./abi folder in the squid project.
To generate the typings, run: npx squid-evm-typegen ./src/abi ./abi/*.json --multicall Alternatively, similar to graph add <address> [<subgraph-manifest default: "./subgraph.yaml">] command, to generate typings, run: npx squid-evm-typegen src/abi 0x2E645469f354BB4F5c8a05B3b30A929361cf77eC #Gravity --clean This command runs the evm-typegen tool that fetches the contract ABI by the address and generates type-safe access classes in src/abi/Gravity.ts . The generated boilerplate code will be used to decode EVM logs and directly query the contract. It also contains topic definitions used in the next step. 5. Subscribe to EVM logs ​ While in The Graph data source is defined in the manifest file subgraph.yaml , in SQD subscriptions to EVM data, including logs, are performed at the processor object definition customarily located at src/processor.ts . The processor is configured directly by the code, unlike subgraphs which require handlers and events to be defined in the manifest file. import { EvmBatchProcessor } from '@subsquid/evm-processor' // generated by the evm-typegen tool // the events object contains typings for all events defined in the ABI import { events } from './abi/Gravity' export const GRAVATAR_CONTRACT = '0x2E645469f354BB4F5c8a05B3b30A929361cf77eC' . toLowerCase ( ) export const processor = new EvmBatchProcessor ( ) // change the gateway URL to run against other EVM networks, e.g. // 'https://v2.archive.subsquid.io/network/polygon-mainnet' // 'https://v2.archive.subsquid.io/network/binance-mainnet' . setGateway ( 'https://v2.archive.subsquid.io/network/ethereum-mainnet' ) . setRpcEndpoint ( '<my_eth_rpc_url>' ) . setBlockRange ( { from : 6175243 } ) . setFinalityConfirmation ( 75 ) . addLog ( { address : [ GRAVATAR_CONTRACT ] , topic0 : [ events . NewGravatar . topic , events . UpdatedGravatar . topic , ] , } ) In the snippet above we tell the squid processor to fetch logs emitted by the contract 0x2E645469f354BB4F5c8a05B3b30A929361cf77eC with topic0 within a specified list. The configuration also states that indexing should start from block 6175243 , the height at which the contract was deployed. Check out the EVM indexing section for the list of supported networks and configuration details. The above snippet is eqivalent to the following subgraph.yaml : specVersion : 0.0.4 description : Gravatar for Ethereum repository : https : //github.com/graphprotocol/example - subgraph schema : file : ./schema.graphql dataSources : - kind : ethereum/contract name : Gravity network : mainnet source : address : '0x2E645469f354BB4F5c8a05B3b30A929361cf77eC' abi : Gravity mapping : kind : ethereum/events apiVersion : 0.0.5 language : wasm/assemblyscript entities : - Gravatar abis : - name : Gravity file : ./abis/Gravity.json eventHandlers : - event : NewGravatar(uint256 , address , string , string) handler : handleNewGravatar - event : UpdatedGravatar(uint256 , address , string , string) handler : handleUpdatedGravatar file : ./src/mapping.ts 6. Transform and save the data ​ In Subgraph data is saved in the mapping.ts file. The mapping function will receive an ethereum.Block as its only argument. In SQD, we set up the processor in processor.ts and save the data in the main.ts . We migrate the subgraph handlers that transform the event data into Gravatar objects. Instead of saving or updating gravatars one by one, EvmBatchProcessor receives an ordered batch of event items it is subscribed to. In our case we have only two kinds of logs -- emitted on gravatar creations and updates. The entry point for transform code is src/main.ts . We start by appending an auxiliary data normalization function to the end of that file: src/main.ts function extractData ( evmLog : any ) : { id : bigint , owner : string , displayName : string , imageUrl : string } { if ( evmLog . topics [ 0 ] === events . NewGravatar . topic ) { return events . NewGravatar . decode ( evmLog ) } if ( evmLog . topics [ 0 ] === events . UpdatedGravatar . topic ) { return events . UpdatedGravatar . decode ( evmLog ) } throw new Error ( 'Unsupported topic' ) } Next, we make a batch handler collecting the updates from a single batch of EVM logs. To convert a 0x... string into a byte array we use the decodeHex utility from Squid SDK. src/main.ts import { TypeormDatabase } from '@subsquid/typeorm-store' import { decodeHex } from '@subsquid/evm-processor' import { events } from './abi/Gravity' import { Gravatar } from './model' import { processor , GRAVATAR_CONTRACT } from './processor' processor . run ( new TypeormDatabase ( { supportHotBlocks : true } ) , async ( ctx ) => { const gravatars : Map < string , Gravatar > = new Map ( ) for ( const c of ctx . blocks ) { for ( const e of c . logs ) { if ( ! ( e . address === GRAVATAR_CONTRACT && ( e . topics [ 0 ] === events . NewGravatar . topic || e . topics [ 0 ] === events . UpdatedGravatar . topic ) ) ) continue const { id , owner , displayName , imageUrl } = extractData ( e ) let idString = '0x' + id . toString ( 16 ) gravatars . set ( idString , new Gravatar ( { id : idString , owner : decodeHex ( owner ) , displayName , imageUrl } ) ) } } await ctx . store . upsert ( [ ... gravatars . values ( ) ] ) } ) The implementation is straightforward -- the newly created and/or updated gravatars are tracked by an in-memory map. The values are persisted in a single batch upsert once all items are processed. 7. Run the processor and GraphQL API ​ To start the indexing, (re)build the project and run the processor: npm run build node -r dotenv/config lib/main.js The processor will output the sync progress and the ETA to reach the chain head. After it reaches the head it will continue indexing new blocks until stopped. To start an API server (at port 4350 by default) with a GraphQL schema auto-generated from the schema file, run in a new terminal window npx squid-graphql-server and inspect the auto-generated GraphQL API using an interactive playground at http://localhost:4350/graphql . What's Next? ​ Compare your API to that of subgraph using the compareGraphQL script Have a closer look at EvmBatchProcessor Learn how to deploy a squid to SQD Cloud for free Learn how to index and query the contract state Inspect a more complex Uniswap V3 squid which tracks Uniswap V3 trading data. It was migrated from the Uniswap V3 subgraph . It takes only a few hours to sync from scratch on a local machine. Edit this page Previous Migration guides Next ArrowSquid for EVM Squid setup 1. Install Squid CLI 2. Fetch the template 3. Copy the schema file and generate entities 4. Generate typings from ABI 5. Subscribe to EVM logs 6. Transform and save the data 7. Run the processor and GraphQL API What's Next?